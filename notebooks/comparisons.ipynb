{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "# os.environ['XLA_PYTHON_CLIENT_ALLOCATOR']= 'platform'\n",
    "# os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import blackjax\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import jaxopt\n",
    "# sns.set(font_scale=2, style='whitegrid')\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats(\"pdf\", \"png\")\n",
    "plt.rcParams[\"savefig.dpi\"] = 150\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams[\"figure.figsize\"] = 6, 4\n",
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"lines.linewidth\"] = 2.0\n",
    "plt.rcParams[\"lines.markersize\"] = 8\n",
    "plt.rcParams[\"legend.fontsize\"] = 14\n",
    "plt.rcParams[\"grid.linestyle\"] = \"-\"\n",
    "plt.rcParams[\"grid.linewidth\"] = 1.0\n",
    "plt.rcParams[\"legend.facecolor\"] = \"white\"\n",
    "# plt.rcParams['grid.color'] = \"grey\"\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "# plt.rcParams['font.family'] = \"normal\"\n",
    "# plt.rcParams['font.family'] = \"sans-serif\"\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "plt.rcParams[\n",
    "    \"text.latex.preamble\"\n",
    "] = \"\\\\usepackage{subdepth} \\\\usepackage{amsfonts} \\\\usepackage{type1cm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snelson(x_train, y_train, n_test=500, x_test_lim=6, standardize_x=False, standardize_y=False, scale_x=1, holdout=False):\n",
    "    if holdout:\n",
    "        mask = ((x_train < 1.5) | (x_train > 3)).flatten()\n",
    "        x_train = x_train[mask]\n",
    "        y_train = y_train[mask]\n",
    "\n",
    "    idx = np.argsort(x_train)\n",
    "    x_train = x_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "\n",
    "    if standardize_x:\n",
    "        x_train = (x_train - x_train.mean(0)) / x_train.std(0) * scale_x\n",
    "    if standardize_y:\n",
    "        y_train = (y_train - y_train.mean(0)) / y_train.std(0)\n",
    "\n",
    "    x_test = np.linspace(-x_test_lim, x_test_lim, n_test)[:, None]\n",
    "\n",
    "    return x_train[:, None], y_train[:, None], x_test\n",
    "\n",
    "def get_data(N=50, D_X=3, sigma_obs=0.05, N_test=500):\n",
    "    D_Y = 1  # create 1d outputs\n",
    "    np.random.seed(0)\n",
    "    X = np.linspace(-1, 1, N)\n",
    "    X = np.power(X[:, np.newaxis], np.arange(D_X))\n",
    "    W = 0.5 * np.random.randn(D_X)\n",
    "    Y = np.dot(X, W) + 0.5 * np.power(0.5 + X[:, 1], 2.0) * np.sin(4.0 * X[:, 1])\n",
    "    Y += sigma_obs * np.random.randn(N)\n",
    "    Y = Y[:, np.newaxis]\n",
    "    mu = np.mean(Y)\n",
    "    std = np.std(Y)\n",
    "    Y -= mu\n",
    "    Y /= std\n",
    "\n",
    "    assert X.shape == (N, D_X)\n",
    "    assert Y.shape == (N, D_Y)\n",
    "\n",
    "    X_test = np.linspace(-1.3, 1.3, N_test)\n",
    "    X_test = np.power(X_test[:, np.newaxis], np.arange(D_X))\n",
    "\n",
    "    Y_test = np.dot(X_test, W) + 0.5 * np.power(0.5 + X_test[:, 1], 2.0) * np.sin(4.0 * X_test[:, 1])\n",
    "    Y_test = Y_test[:, np.newaxis]\n",
    "    Y_test -= mu\n",
    "    Y_test /= std\n",
    "\n",
    "    return X[:, 1][:, None], Y, X_test[:, 1][:, None], Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_data = pd.read_csv('snelson.csv')\n",
    "x_train, y_train = np.array(_raw_data.x.values), np.array(_raw_data.y.values)\n",
    "x_train, y_train, x_test = snelson(x_train, y_train, standardize_x=True, standardize_y=True, scale_x=1, x_test_lim=6)\n",
    "y_test = np.zeros_like(x_test)\n",
    "\n",
    "# x_train, y_train, x_test, y_test = get_data(sigma_obs=0.1)\n",
    "# # plot\n",
    "# plt.scatter(x_train, y_train, label='trian', s=10)\n",
    "# plt.scatter(x_test, y_test, label='test', s=10)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from typing import Callable\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 64\n",
    "    hidden_layers: int = 1\n",
    "    act: Callable = nn.relu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        layers = [nn.Dense(self.H)]\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers += [self.act, nn.Dense(self.H)]\n",
    "        layers += [self.act, nn.Dense(self.out_size)]\n",
    "        return nn.Sequential(layers)(x)\n",
    "\n",
    "def reparam_initializer(initializer, f):\n",
    "    def init(key, shape, dtype=jnp.float32):\n",
    "        # sample original parameters and then invert the reparametrization\n",
    "        return f(initializer(key, shape, dtype))\n",
    "    return init\n",
    "\n",
    "class ReparamDense(nn.Module):\n",
    "    # same as nn.Dense but with reparam weights\n",
    "    # reparam: Callable = lambda x: x # w = reparam(x)\n",
    "    # reparam_inv: Callable = lambda x: x # x = reparam_inv(w)\n",
    "    # bias_init: Callable = reparam_initializer(nn.initializers.normal(stddev=1e-6))\n",
    "    # kernel_init: Callable = reparam_initializer(nn.initializers.lecun_normal())\n",
    "\n",
    "    def __init__(self, features, reparam, reparam_inv, init_scale=None):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.reparam = reparam\n",
    "        self.reparam_inv = reparam_inv\n",
    "        # zero init for bias\n",
    "        if init_scale is None:\n",
    "            self.bias_init = reparam_initializer(nn.initializers.normal(stddev=1e-4), f=reparam_inv)\n",
    "            self.kernel_init = reparam_initializer(nn.initializers.lecun_normal(), f=reparam_inv)\n",
    "        else:\n",
    "            self.bias_init = reparam_initializer(nn.initializers.normal(stddev=init_scale), f=reparam_inv)\n",
    "            self.kernel_init = reparam_initializer(nn.initializers.normal(stddev=init_scale), f=reparam_inv)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        reparam_kernel = self.param('reparam_kernel', self.kernel_init, (inputs.shape[-1], self.features))\n",
    "        reparam_bias = self.param('reparam_bias', self.bias_init, (1, self.features)) # not using bias_init to avoid dividing by zero\n",
    "        # invert weights\n",
    "        kernel = jax.tree_util.tree_map(self.reparam, reparam_kernel)\n",
    "        bias = jax.tree_util.tree_map(self.reparam, reparam_bias)\n",
    "        # clamp to avoid numerical issues\n",
    "        kernel = jnp.clip(kernel, a_min=-1e6, a_max=1e6)\n",
    "        bias = jnp.clip(bias, a_min=-1e6, a_max=1e6)\n",
    "        return jnp.dot(inputs, kernel) + bias\n",
    "\n",
    "class ReparamMLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 64\n",
    "    hidden_layers: int = 1\n",
    "    reparam: Callable = lambda x: x # w = reparam(x)\n",
    "    reparam_inv: Callable = lambda x: x # x = reparam_inv(w)\n",
    "    act: Callable = nn.tanh\n",
    "    init_scale: float = None\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        layers = [ReparamDense(features=self.H, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers += [self.act, ReparamDense(features=self.H, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        layers += [self.act, ReparamDense(features=self.out_size, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        out = nn.Sequential(layers)(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_sigular_values(model, p, x):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('b...->...b', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    J = J.reshape(-1, x.shape[0]).T # (N, P)\n",
    "    # sigular values of J\n",
    "    _, S, _ = jnp.linalg.svd(J, full_matrices=False)\n",
    "    return S\n",
    "\n",
    "def effdim(eigs, cutoff):\n",
    "    eigs = eigs[eigs > 0]\n",
    "    return jnp.sum(eigs / (eigs + cutoff))\n",
    "\n",
    "def fspace_effdim(model, p, x, cutoff, jitter=0):\n",
    "    # K = fspace_hessian(model, p, x)\n",
    "    # K = K + jitter * jnp.eye(K.shape[0])\n",
    "    # eigenvals = jnp.linalg.eigvalsh(K)\n",
    "    # assert jnp.all(eigenvals > 0), 'Hessian is not positive definite'\n",
    "    # return jnp.sum(eigenvals / (eigenvals + cutoff))\n",
    "    eigs = fspace_hessian_eigenvalues(model, p, x)\n",
    "    return jnp.sum(eigs / (eigs + cutoff))\n",
    "\n",
    "def fspace_hessian_eigenvalues(model, p, x):\n",
    "    S = jacobian_sigular_values(model, p, x)\n",
    "    return (S ** 2) / x.shape[0]\n",
    "\n",
    "def fspace_hessian(model, p, x):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('b...->...b', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    J = J.reshape(-1, x.shape[0]).T # (N, P)\n",
    "    N = x.shape[0]\n",
    "    K = J.T @ J / N # (P, P)\n",
    "    return K\n",
    "\n",
    "def log_det_K_svd(model, p, x, jitter=1e-6, scale=1.0):\n",
    "    # log det J^T J = sum log s^2 (careful, check this for more general cases when J is not injective)\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    # s = s + jitter\n",
    "    logdet_svd = 2 * jnp.sum(jnp.log(s))\n",
    "    return logdet_svd\n",
    "\n",
    "def log_det_K(model, p, x, jitter=1e-6, scale=1.0):\n",
    "    K = fspace_hessian(model, p, x)\n",
    "    # add jitter\n",
    "    K = K + jitter * jnp.eye(K.shape[0])\n",
    "    s, log_det = jnp.linalg.slogdet(K)\n",
    "    # assert s > 0, 'K is not positive definite'\n",
    "    # # compute cholesky\n",
    "    # # L = jnp.linalg.cholesky(K)\n",
    "    # # # compute log det\n",
    "    # # log_det = 2 * jnp.sum(jnp.log(jnp.diag(L)))\n",
    "    return log_det\n",
    "\n",
    "    # log det J^T J = sum log s^2 (careful, check this for more general cases when J is not injective)\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    # s = s + jitter\n",
    "    logdet_svd = 2 * jnp.sum(jnp.log(s))\n",
    "    return logdet_svd\n",
    "\n",
    "\n",
    "def log_det_diagonal_approx(model, p, x, jitter=1e-6):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('b...->...b', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    J = J.reshape(-1, x.shape[0]).T # (N, P)\n",
    "    avg_j_sq = jnp.mean(J ** 2, axis=0) # (P,)\n",
    "    logdet_diag = jnp.sum(jnp.log(avg_j_sq + jitter))\n",
    "    return logdet_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(lr, weight_decay, n_step, rng_key, loss_fn, model, x_train, y_train, x_eval_generator, optimizer, fsmap, jitter, diag=False):\n",
    "    rng_key, init_params_key = jax.random.split(rng_key)\n",
    "    init_params = jax.jit(model.init)(init_params_key, jnp.ones((1, x_train.shape[1])))\n",
    "    if optimizer == 'adam':\n",
    "        tx = optax.adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd':\n",
    "        tx = optax.sgd(learning_rate=lr, momentum=0.9)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    ts = train_state.TrainState.create(apply_fn=model.apply, params=init_params, tx=tx)\n",
    "    \n",
    "    def augmented_loss_fn(p, x_eval):\n",
    "        # loss = likelihood / N = 1 / (2 * sigma^2) * ||y - f(x)||^2 / N\n",
    "        # it contains a factor 1 / N, where N = x_train.shape[0]\n",
    "        # all other terms should be divided by N as well\n",
    "        loss = loss_fn(p) \n",
    "        if fsmap:\n",
    "            if diag:\n",
    "                fs_loss = 1 / 2 * log_det_diagonal_approx(model, p, x_eval, jitter) / x_train.shape[0]\n",
    "            else:\n",
    "                fs_loss = 1 / 2 * log_det_K(model, p, x_eval, jitter) / x_train.shape[0]\n",
    "        else:\n",
    "            fs_loss = 0\n",
    "        params_flat, unravel = jax.flatten_util.ravel_pytree(p)\n",
    "        wd_loss = weight_decay * jnp.sum(params_flat ** 2) / x_train.shape[0]\n",
    "        return loss + wd_loss + fs_loss\n",
    "\n",
    "    grad_fn = jax.jit(jax.value_and_grad(augmented_loss_fn))\n",
    "    losses = []\n",
    "    @jax.jit\n",
    "    def train_step(ts, rng_key):\n",
    "        rng_key, x_eval_key = jax.random.split(rng_key)\n",
    "        x_eval = x_eval_generator(x_eval_key)\n",
    "        loss, grads = grad_fn(ts.params, x_eval)\n",
    "        ts = ts.apply_gradients(grads=grads)\n",
    "        return ts, loss, rng_key\n",
    "    for e in tqdm(range(n_step)):\n",
    "        ts, loss, rng_key = train_step(ts, rng_key)\n",
    "        losses.append(loss.item())\n",
    "    losses = np.array(losses)\n",
    "    return ts.params, losses\n",
    "\n",
    "def make_flat_function(f, unravel):\n",
    "    def f_flat(p_flat, *args, **kwargs):\n",
    "        p = unravel(p_flat)\n",
    "        return f(p, *args, **kwargs)\n",
    "    return f_flat\n",
    "\n",
    "def optimize_func_decay(lr, func_decay, n_step, rng_key, loss_fn, model, x_train, y_train, x_eval_generator, optimizer):\n",
    "    rng_key, init_params_key = jax.random.split(rng_key)\n",
    "    init_params = jax.jit(model.init)(init_params_key, jnp.ones((1, x_train.shape[1])))\n",
    "    if optimizer == 'adam':\n",
    "        tx = optax.adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd':\n",
    "        tx = optax.sgd(learning_rate=lr, momentum=0.9)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    ts = train_state.TrainState.create(apply_fn=model.apply, params=init_params, tx=tx)\n",
    "    \n",
    "    def augmented_loss_fn(p, x_eval):\n",
    "        # loss = likelihood / N = 1 / (2 * sigma^2) * ||y - f(x)||^2 / N\n",
    "        # it contains a factor 1 / N, where N = x_train.shape[0]\n",
    "        # all other terms should be divided by N as well\n",
    "        loss = loss_fn(p)\n",
    "        fs_loss = func_decay * (model.apply(p, x_eval) ** 2).sum() / x_train.shape[0]\n",
    "        return loss + fs_loss\n",
    "\n",
    "    grad_fn = jax.jit(jax.value_and_grad(augmented_loss_fn))\n",
    "    losses = []\n",
    "    @jax.jit\n",
    "    def train_step(ts, rng_key):\n",
    "        rng_key, x_eval_key = jax.random.split(rng_key)\n",
    "        x_eval = x_eval_generator(x_eval_key)\n",
    "        loss, grads = grad_fn(ts.params, x_eval)\n",
    "        ts = ts.apply_gradients(grads=grads)\n",
    "        return ts, loss, rng_key\n",
    "    for e in tqdm(range(n_step)):\n",
    "        ts, loss, rng_key = train_step(ts, rng_key)\n",
    "        losses.append(loss.item())\n",
    "    losses = np.array(losses)\n",
    "    return ts.params, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(arch, noise_scale, x_train, y_train, x_test, y_test, x_eval_generator, n_step, lr, weight_decay, optimizer, output_dir, seed, task='regression', fsmap=False, jitter=1e-6):\n",
    "    # model\n",
    "    model = arch(out_size=1)\n",
    "    x_eval_sample = x_eval_generator(jax.random.PRNGKey(0))\n",
    "    # count parameters\n",
    "    init_params = model.init(jax.random.PRNGKey(0), jnp.ones((1, x_train.shape[1])))\n",
    "    leaves, _ = jax.tree_util.tree_flatten(init_params)\n",
    "    n_params = sum([np.prod(p.shape) for p in leaves])\n",
    "    print(f\"Number of parameters: {n_params}\")\n",
    "\n",
    "\n",
    "    if task == 'regression':\n",
    "        train_loss_fn = lambda p: 0.5 * jnp.mean((model.apply(p, x_train) - y_train) ** 2) / noise_scale ** 2\n",
    "        test_loss_fn = lambda p: jnp.mean((model.apply(p, x_test) - y_test) ** 2)\n",
    "    elif task == 'classification':\n",
    "        train_loss_fn = lambda p: jnp.mean(jax.nn.sigmoid(model.apply(p, x_train)) * (1 - y_train) + (1 - jax.nn.sigmoid(model.apply(p, x_train))) * y_train)\n",
    "        test_loss_fn = lambda p: jnp.mean(jnp.round(jax.nn.sigmoid(model.apply(p, x_test))) != y_test)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    params, losses = optimize(lr, weight_decay, n_step, rng_key, train_loss_fn, model, x_train, y_train, x_eval_generator, optimizer, fsmap, jitter)\n",
    "    torch.save(params, f'{output_dir}/{task}_wd{weight_decay}_{optimizer}_{seed}_params.pt')\n",
    "    print('Saved ps parameters at ', f'{output_dir}/{task}_wd{weight_decay}_{optimizer}_{seed}_params.pt')\n",
    "    # plot and save losses\n",
    "    plt.figure()\n",
    "    # plot without 90% quantiles as y limits\n",
    "    q1, q2 = np.quantile(losses, [0., 0.95])\n",
    "    plt.plot(losses)\n",
    "    plt.ylim(q1, q2)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6), dpi=120)\n",
    "    prediction = model.apply(params, x_test)\n",
    "    ax.plot(x_test[..., 0], prediction, c='green', label='Prediction')\n",
    "    # plot X_eval on the x-axis\n",
    "    ax.scatter(x_eval_sample[..., 0], np.zeros(x_eval_sample.shape[0]), c='black', label='Eval Points Sample', linestyle='None', s=3)\n",
    "    ax.set(xlabel='$x$', ylabel='$y$', ylim=[-3, 3], xlim=[2 * x_train.min(), 2 * x_train.max()])\n",
    "    ax.scatter(x_train[..., 0], y_train, c='r', label='Train Data', s=10)\n",
    "    ax.plot(x_test[..., 0], y_test, c='b', label='Test Data')\n",
    "    ax.grid(True)\n",
    "\n",
    "    # show noise_scale, prior_scale on plot\n",
    "    ax.text(0.05, 0.95, f'wd: {weight_decay}', transform=ax.transAxes, fontsize=16, verticalalignment='top')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "    train_loss = train_loss_fn(params)\n",
    "    test_loss = test_loss_fn(params)\n",
    "    result = {'train_loss': train_loss, 'test_loss': test_loss, 'n_params': n_params, 'weight_decay': weight_decay}\n",
    "    if task == 'regression':\n",
    "        result['test_loss'] = jnp.mean((model.apply(params, x_test) - y_test) ** 2)\n",
    "    elif task == 'classification':\n",
    "        result['test_loss'] = jnp.mean(jax.nn.sigmoid(model.apply(params, x_test)) * (1 - y_test) + (1 - jax.nn.sigmoid(model.apply(params, x_test))) * y_test)\n",
    "        result['test_error'] = jnp.mean(jnp.round(jax.nn.sigmoid(model.apply(params, x_test))) != y_test)\n",
    "    # Hessian of training loss\n",
    "    params_flat, unravel = jax.flatten_util.ravel_pytree(params)\n",
    "    model_apply_flat = make_flat_function(model.apply, unravel)\n",
    "    if task == 'regression':\n",
    "        loss_fn_flat = lambda p: jnp.mean((model_apply_flat(p, x_train) - y_train) ** 2)\n",
    "    elif task == 'classification':\n",
    "        loss_fn_flat = lambda p: jnp.mean(jax.nn.sigmoid(model_apply_flat(p, x_train)) * (1 - y_train) + (1 - jax.nn.sigmoid(model_apply_flat(p, x_train))) * y_train)\n",
    "    # h_train_loss = jax.hessian(loss_fn_flat)(params_flat)\n",
    "    # h_train_loss_eigenvals = jnp.linalg.eigvalsh(h_train_loss)\n",
    "    # print('Fraction of eigenvalues smaller than 0: ', np.sum(h_train_loss_eigenvals < 0) / h_train_loss_eigenvals.shape[0])\n",
    "    # print('Fraction of eigenvalues smaller than 0.01: ', np.sum(h_train_loss_eigenvals < 0.01) / h_train_loss_eigenvals.shape[0])\n",
    "    # print('Fraction of eigenvalues smaller than 0.1: ', np.sum(h_train_loss_eigenvals < 0.1) / h_train_loss_eigenvals.shape[0])\n",
    "    # result['h_train_loss_eigenvals'] = h_train_loss_eigenvals\n",
    "    # # Hessian function space\n",
    "    # h_fspace_eigenvals = fspace_hessian_eigenvalues(model, params, x_eval_sample)\n",
    "    # result['h_fspace_eigenvals'] = h_fspace_eigenvals\n",
    "    result['p_norm'] = jnp.linalg.norm(params_flat)\n",
    "    torch.save(result, f'{output_dir}/{task}_wd{weight_decay}_{optimizer}_{seed}_result.pt')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_compare(arch, noise_scale, x_train, y_train, x_test, y_test, x_eval_generator, n_step, lr, weight_decay, func_decay, optimizer, output_dir, seed, task='regression', jitter=1e-6, load=True):\n",
    "    if not isinstance(n_step , list):\n",
    "        n_step = [n_step] * 3\n",
    "    # model\n",
    "    model = arch(out_size=1)\n",
    "    x_eval_sample = x_eval_generator(jax.random.PRNGKey(0))\n",
    "    # count parameters\n",
    "    init_params = model.init(jax.random.PRNGKey(0), jnp.ones((1, x_train.shape[1])))\n",
    "    leaves, _ = jax.tree_util.tree_flatten(init_params)\n",
    "    n_params = sum([np.prod(p.shape) for p in leaves])\n",
    "    print(f\"Number of parameters: {n_params}\")\n",
    "\n",
    "\n",
    "    if task == 'regression':\n",
    "        train_loss_fn = lambda p: 0.5 * jnp.mean((model.apply(p, x_train) - y_train) ** 2) / noise_scale ** 2\n",
    "        test_loss_fn = lambda p: jnp.mean((model.apply(p, x_test) - y_test) ** 2)\n",
    "    elif task == 'classification':\n",
    "        train_loss_fn = lambda p: jnp.mean(jax.nn.sigmoid(model.apply(p, x_train)) * (1 - y_train) + (1 - jax.nn.sigmoid(model.apply(p, x_train))) * y_train)\n",
    "        test_loss_fn = lambda p: jnp.mean(jnp.round(jax.nn.sigmoid(model.apply(p, x_test))) != y_test)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # pmap\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    param_path = f'{output_dir}/{task}_wd{weight_decay}_{optimizer}_{seed}_pmap_params.pt'\n",
    "    if load and os.path.exists(param_path):\n",
    "        print('Loading parameters from ', param_path)\n",
    "        pmap_params = torch.load(param_path)\n",
    "    else:\n",
    "        pmap_params, losses = optimize(lr, weight_decay, n_step[0], rng_key, train_loss_fn, model, x_train, y_train, x_eval_generator, optimizer, False, jitter)\n",
    "        torch.save(pmap_params, param_path)\n",
    "        print('Saved parameters at ', param_path)\n",
    "        # plot and save losses\n",
    "        plt.figure()\n",
    "        # plot without 90% quantiles as y limits\n",
    "        q1, q2 = np.quantile(losses, [0., 0.95])\n",
    "        plt.plot(losses)\n",
    "        plt.ylim(q1, q2)\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # fmap\n",
    "    param_path = f'{output_dir}/{task}_wd{weight_decay}_{optimizer}_{seed}_fmap_params.pt'\n",
    "    if load and os.path.exists(param_path):\n",
    "        print('Loading parameters from ', param_path)\n",
    "        fmap_params = torch.load(param_path)\n",
    "    else:\n",
    "        fmap_params, losses = optimize(lr, weight_decay, n_step[1], rng_key, train_loss_fn, model, x_train, y_train, x_eval_generator, optimizer, True, jitter)\n",
    "        torch.save(fmap_params, param_path)\n",
    "        print('Saved parameters at ', param_path)\n",
    "        # plot and save losses\n",
    "        plt.figure()\n",
    "        # plot without 90% quantiles as y limits\n",
    "        q1, q2 = np.quantile(losses, [0., 0.95])\n",
    "        plt.plot(losses)\n",
    "        plt.ylim(q1, q2)\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # fmap diagonal approx\n",
    "    param_path = f'{output_dir}/{task}_wd{weight_decay}_{optimizer}_{seed}_diag_fmap_params.pt'\n",
    "    if load and os.path.exists(param_path):\n",
    "        print('Loading parameters from ', param_path)\n",
    "        diag_fmap_params = torch.load(param_path)\n",
    "    else:\n",
    "        diag_fmap_params, losses = optimize(lr, weight_decay, n_step[1], rng_key, train_loss_fn, model, x_train, y_train, x_eval_generator, optimizer, True, jitter, diag=True)\n",
    "        torch.save(diag_fmap_params, param_path)\n",
    "        print('Saved parameters at ', param_path)\n",
    "        # plot and save losses\n",
    "        plt.figure()\n",
    "        # plot without 90% quantiles as y limits\n",
    "        q1, q2 = np.quantile(losses, [0., 0.95])\n",
    "        plt.plot(losses)\n",
    "        plt.ylim(q1, q2)\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # fmap white noise\n",
    "    param_path = f'{output_dir}/{task}_fd{func_decay}_{optimizer}_{seed}_fdecay_params.pt'\n",
    "    if load and os.path.exists(param_path):\n",
    "        print('Loading parameters from ', param_path)\n",
    "        fdecay_params = torch.load(param_path)\n",
    "    else:\n",
    "        fdecay_params, losses = optimize_func_decay(lr, func_decay, n_step[2], rng_key, train_loss_fn, model, x_train, y_train, x_eval_generator, optimizer)\n",
    "        torch.save(fdecay_params, param_path)\n",
    "        print('Saved parameters at ', param_path)\n",
    "        # plot and save losses\n",
    "        plt.figure()\n",
    "        # plot without 90% quantiles as y limits\n",
    "        q1, q2 = np.quantile(losses, [0., 0.95])\n",
    "        plt.plot(losses)\n",
    "        plt.ylim(q1, q2)\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6), dpi=120)\n",
    "    ax.plot(x_test[..., 0], model.apply(fmap_params, x_test), c='purple', label=r'Exact F-MAP')\n",
    "    ax.plot(x_test[..., 0], model.apply(diag_fmap_params, x_test), c='Magenta', label=r'Diag F-MAP', linestyle='--')\n",
    "    ax.plot(x_test[..., 0], model.apply(fdecay_params, x_test), c='green', label=r'Linearized F-MAP')\n",
    "    ax.plot(x_test[..., 0], model.apply(pmap_params, x_test), c='blue', label='P-MAP', linestyle='--')\n",
    "    # plot X_eval on the x-axis\n",
    "    ax.scatter(x_eval_sample[..., 0], np.zeros(x_eval_sample.shape[0]), c='black', label='Eval Points Sample', linestyle='None', s=3)\n",
    "    # ax.set(xlabel='$x$', ylabel='$y$', ylim=[-3, 3], xlim=[2 * x_train.min(), 2 * x_train.max()])\n",
    "    ax.set(xlabel='$x$', ylabel='$y$', ylim=[-3, 3], xlim=[-3, 3])\n",
    "    ax.scatter(x_train[..., 0], y_train, c='r', label='Train Data', s=10)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # show noise_scale, prior_scale on plot\n",
    "    ax.text(0.05, 0.95, rf'$\\lambda = {weight_decay:.2g}, \\tau = {func_decay:2g}$', transform=ax.transAxes, fontsize=16, verticalalignment='top')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig(os.path.join(output_dir, f'noise{noise_scale}_wd{weight_decay}_fd{func_decay}.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'tanh': nn.tanh,\n",
    "    'elu': nn.elu,\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss definitions ###\n",
    "\n",
    "$\\mathcal{L}_\\mathrm{P-MAP}(\\sigma, \\lambda) = (2\\sigma^2)^{-1} ||Y - f(X)||^2 + \\lambda ||w||^2$\n",
    "\n",
    "$\\mathcal{L}_\\mathrm{Exact.F-MAP}(\\sigma, \\lambda) = (2\\sigma^2)^{-1} ||Y - f(X)||^2 + \\lambda ||w||^2 + 1/2 \\log\\det(E_{x' \\in X'}[\\nabla f(x') \\nabla f(x')^T])$\n",
    "\n",
    "$\\mathcal{L}_\\mathrm{Diagonal.F-MAP}(\\sigma, \\lambda) = (2\\sigma^2)^{-1} ||Y - f(X)||^2 + \\lambda ||w||^2 + 1/2 \\sum_{i=1}^{p}\\log E_{x' \\in X'}[(\\partial f(x') / \\partial w_i)^2]$\n",
    "\n",
    "$\\mathcal{L}_\\mathrm{Linearized.F-MAP}(\\sigma, \\tau) = (2\\sigma^2)^{-1} ||Y - f(X)||^2 + \\tau ||f(X')||^2$\n",
    "\n",
    "\n",
    "$f$ has 1153 parameters (2 hidden layers, 32 units), $X'$ is 10,000 either uniformly random or linearly spaced samples in $[-3, 3]$, i.e., the function is defined to have domain $[-3, 3]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random uniform evaluation points ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'regression'\n",
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [32]\n",
    "depths = [2]\n",
    "acts = ['tanh']\n",
    "\n",
    "# SGD hyper\n",
    "optimizer = 'adam'\n",
    "lr = 1e-2 #3e-3\n",
    "n_step = [int(1e4), int(1e4), int(1e4)]\n",
    "weight_decays = [0.1, 1, 10, 100]\n",
    "func_decays =  [0.001, 0.01, 0.1]\n",
    "noise_scale = 0.1\n",
    "\n",
    "n_eval = 1e4\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = jax.random.uniform(rng_key, (int(n_eval), 1), minval=-3, maxval=3)\n",
    "    return X_eval\n",
    "\n",
    "load = True\n",
    "results = []\n",
    "for width in widths:\n",
    "    for depth in depths:\n",
    "        for act in acts:\n",
    "            output_dir = f'comparison_rand_uniform_3/{width}_{depth}_{act}_noise{noise_scale}_lr{lr}_{optimizer}'\n",
    "            # mkdir if needed\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            for weight_decay in weight_decays:\n",
    "                for func_decay in func_decays + [weight_decay]:\n",
    "                    for seed in range(0, 1):\n",
    "                        arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                        results.append(run_compare(arch, noise_scale, x_train, y_train, x_test, y_test, x_eval_generator, n_step, lr, weight_decay, func_decay, optimizer, output_dir, seed, task=task, load=load))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linspace evaluation points ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'regression'\n",
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [32]\n",
    "depths = [2]\n",
    "acts = ['tanh']\n",
    "\n",
    "# SGD hyper\n",
    "optimizer = 'adam'\n",
    "lr = 1e-2 #3e-3\n",
    "n_step = [int(1e4), int(1e4), int(1e4)]\n",
    "weight_decays = [0.1, 1, 10, 100]\n",
    "func_decays =  [0.001, 0.01, 0.1]\n",
    "noise_scale = 0.1\n",
    "\n",
    "n_eval = 1e4\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = jnp.linspace(-3, 3, int(n_eval)).reshape(-1, 1)\n",
    "    return X_eval\n",
    "\n",
    "load = True\n",
    "results = []\n",
    "for width in widths:\n",
    "    for depth in depths:\n",
    "        for act in acts:\n",
    "            output_dir = f'comparison_linspace_3/{width}_{depth}_{act}_noise{noise_scale}_lr{lr}_{optimizer}'\n",
    "            # mkdir if needed\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            for weight_decay in weight_decays:\n",
    "                for func_decay in func_decays + [weight_decay]:\n",
    "                    for seed in range(0, 1):\n",
    "                        arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                        results.append(run_compare(arch, noise_scale, x_train, y_train, x_test, y_test, x_eval_generator, n_step, lr, weight_decay, func_decay, optimizer, output_dir, seed, task=task, load=load))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Adam to SGD leaves the result qualitatively unchanged ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'regression'\n",
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [32]\n",
    "depths = [2]\n",
    "acts = ['tanh']\n",
    "\n",
    "# SGD hyper\n",
    "optimizer = 'sgd'\n",
    "lr = 1e-3\n",
    "n_step = [int(1e6), int(1e6), int(1e6)]\n",
    "weight_decays = [0.1]\n",
    "func_decays =  [0.001, 0.01, 0.1]\n",
    "noise_scale = 0.1\n",
    "\n",
    "n_eval = 1e4\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = jax.random.uniform(rng_key, (int(n_eval), 1), minval=-3, maxval=3)\n",
    "    return X_eval\n",
    "\n",
    "load = True\n",
    "results = []\n",
    "for width in widths:\n",
    "    for depth in depths:\n",
    "        for act in acts:\n",
    "            output_dir = f'comparison_rand_uniform_3_sgd_1e-3_1e6/{width}_{depth}_{act}_noise{noise_scale}_lr{lr}_{optimizer}'\n",
    "            # mkdir if needed\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            for weight_decay in weight_decays:\n",
    "                for func_decay in func_decays + [weight_decay]:\n",
    "                    for seed in range(0, 1):\n",
    "                        arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                        results.append(run_compare(arch, noise_scale, x_train, y_train, x_test, y_test, x_eval_generator, n_step, lr, weight_decay, func_decay, optimizer, output_dir, seed, task=task, load=load))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fspace': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9436057e92285046d415c34e216bd357b01decd87fa7e06f42744a4b160880c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
