{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "from bayesian_benchmarks.data import get_regression_data, get_classification_data\n",
    "import bayesian_benchmarks.data as bd\n",
    "import torch\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=3, rc={\"lines.linewidth\": 2})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import blackjax\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import jaxopt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"pdf\", \"png\")\n",
    "# plt.rcParams[\"savefig.dpi\"] = 75\n",
    "# plt.rcParams[\"figure.autolayout\"] = True\n",
    "# plt.rcParams[\"figure.figsize\"] = 6, 4\n",
    "# plt.rcParams[\"axes.labelsize\"] = 18\n",
    "# plt.rcParams[\"axes.titlesize\"] = 20\n",
    "# plt.rcParams[\"font.size\"] = 16\n",
    "# plt.rcParams[\"lines.linewidth\"] = 2.0\n",
    "# plt.rcParams[\"lines.markersize\"] = 8\n",
    "# plt.rcParams[\"legend.fontsize\"] = 14\n",
    "# plt.rcParams[\"grid.linestyle\"] = \"-\"\n",
    "# plt.rcParams[\"grid.linewidth\"] = 1.0\n",
    "# plt.rcParams[\"legend.facecolor\"] = \"white\"\n",
    "# # plt.rcParams['grid.color'] = \"grey\"\n",
    "# plt.rcParams[\"text.usetex\"] = True\n",
    "# # plt.rcParams['font.family'] = \"normal\"\n",
    "# # plt.rcParams['font.family'] = \"sans-serif\"\n",
    "# plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "# plt.rcParams[\n",
    "#     \"text.latex.preamble\"\n",
    "# ] = \"\\\\usepackage{subdepth} \\\\usepackage{amsfonts} \\\\usepackage{type1cm}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from typing import Callable, List\n",
    "\n",
    "def count_params(model, x):\n",
    "    params = model.init(jax.random.PRNGKey(0), x)\n",
    "    n = sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "    print(f'Number of parameters: {n/1e3} k')\n",
    "    \n",
    "def param_size(params):\n",
    "    return sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "\n",
    "class NonLinearMixture(nn.Module):\n",
    "    feature_fn: Callable\n",
    "    act_fn: Callable\n",
    "    prior_stds: List[float]\n",
    "    is_orthogonal: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # evaluate basis functions\n",
    "        features = self.feature_fn(x) # (n, d)\n",
    "        # sample from N(0, prior_stds ** 2)\n",
    "        mixture_coeffs = self.param('mixture_coeffs', lambda key: jax.random.normal(key, (features.shape[-1],)) * self.prior_stds)\n",
    "        mixture_coeffs = self.act_fn(mixture_coeffs) / jnp.sqrt(mixture_coeffs.shape[-1]) # (d, )\n",
    "        return (features @ mixture_coeffs).reshape(-1, 1) # (n, 1)\n",
    "    \n",
    "class LogDet:\n",
    "    def __init__(self, model: NonLinearMixture, x_eval: jnp.ndarray, svd=False):\n",
    "        features = model.feature_fn(x_eval) # (n, d)\n",
    "        self.Phi = features.T @ features / features.shape[0] # (d, d)\n",
    "        self.act_deriv = jax.grad(model.act_fn)\n",
    "        self.act_deriv = jax.vmap(self.act_deriv)\n",
    "        self.diag = model.is_orthogonal\n",
    "        self.svd = svd\n",
    "        \n",
    "    \n",
    "    def compute_log_det(self, model_params, jitter=0, return_J=False):\n",
    "        # J[i,j] = s(w_i) * s(w_j) * Phi[i,j], where s is the derivative of model.act_fn\n",
    "        s = self.act_deriv(model_params['params']['mixture_coeffs'])\n",
    "        if self.diag:\n",
    "            print('DIAG!')\n",
    "            Phi_diag = jnp.diag(self.Phi)\n",
    "            return jnp.sum(jnp.log(Phi_diag * (s ** 2) + jitter))\n",
    "        J = s[:, None] * s[None, :] * self.Phi # (d, d)\n",
    "        # compute with svd\n",
    "        if self.svd:\n",
    "            singular_values = jnp.linalg.svd(J, compute_uv=False)\n",
    "            eigs = singular_values ** 2 + jitter\n",
    "            logdet = jnp.sum(jnp.log(eigs))\n",
    "        else:\n",
    "            logdet = jnp.linalg.slogdet(J + jitter * jnp.eye(J.shape[0]))[1]\n",
    "        if return_J:\n",
    "            return logdet, J\n",
    "        return logdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_norm(tree):\n",
    "    return jnp.sqrt(sum([jnp.sum(x**2) for x in jax.tree_leaves(tree)]))\n",
    "\n",
    "def random_split_like_tree(rng_key, target=None, treedef=None):\n",
    "    if treedef is None:\n",
    "        treedef = jax.tree_structure(target)\n",
    "    keys = jax.random.split(rng_key, treedef.num_leaves)\n",
    "    return jax.tree_unflatten(treedef, keys)\n",
    "\n",
    "@jax.jit\n",
    "def tree_random_normal_like(rng_key, target):\n",
    "    keys_tree = random_split_like_tree(rng_key, target)\n",
    "    return jax.tree_map(\n",
    "        lambda l, k: jax.random.normal(k, l.shape, l.dtype),\n",
    "        target,\n",
    "        keys_tree,\n",
    "    )\n",
    "\n",
    "def jacobian_sigular_values(model, p, x):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x) # (b, o, p)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('bo...->...bo', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    P = param_size(p)\n",
    "    J = J.reshape(P, -1).T # (B*O, P)\n",
    "    # sigular values of J\n",
    "    _, S, _ = jnp.linalg.svd(J, full_matrices=False)\n",
    "    return S\n",
    "\n",
    "def log_det_g_svd(model, p, x, jitter=1e-6, return_eig=False, shift_by_jitter=True):\n",
    "    P = param_size(p)\n",
    "    zeros = jnp.zeros(P)\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    # P eigenvalues, correctly handling the case where P > N\n",
    "    s = zeros.at[:s.shape[0]].set(s)\n",
    "    logdet_svd = jnp.sum(jnp.log(s ** 2 + jitter))\n",
    "    if shift_by_jitter:\n",
    "        logdet_svd = logdet_svd - P * jnp.log(jitter)\n",
    "    if return_eig:\n",
    "        return logdet_svd, s ** 2\n",
    "    return logdet_svd\n",
    "\n",
    "def log_det_g_svd_first_order(model, p, x, jitter=1e-6, return_eig=False):\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    logdet_svd = jnp.sum(jnp.log(jitter) + (s ** 2) / jitter)\n",
    "    P = param_size(p)\n",
    "    logdet_svd = logdet_svd - P * jnp.log(jitter)\n",
    "    if return_eig:\n",
    "        return logdet_svd, s ** 2\n",
    "    return logdet_svd\n",
    "\n",
    "def trace_estimator(model, p, x, dp, sigma=0.1):\n",
    "    # dp: N(0, I)\n",
    "    # p_perturbed = p + dp, both pytrees\n",
    "    # scale dp by sigma\n",
    "    dp = jax.tree_util.tree_map(lambda x: x * sigma, dp)\n",
    "    p_perturbed = jax.tree_util.tree_map(lambda x, y: x + y, p, dp)\n",
    "    # compute the difference between the outputs\n",
    "    y = model.apply(p, x)\n",
    "    y_perturbed = model.apply(p_perturbed, x)\n",
    "    diff = y - y_perturbed\n",
    "    # compute the avg sq norm of the difference\n",
    "    avg_dff_sq_norm = jnp.mean((diff ** 2).sum(axis=-1))\n",
    "    return avg_dff_sq_norm / (sigma ** 2)\n",
    "\n",
    "def log_det_trace_estimator(model, p, x, jitter, sigma=0.01, n_samples=1, normalize=False):\n",
    "    # sample n_samples dp from N(0, I) and compute the trace estimator\n",
    "    traces = []\n",
    "    P = param_size(p)\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    for _ in range(n_samples):\n",
    "        rng_key, rng_key_sample = jax.random.split(rng_key)\n",
    "        dp = tree_random_normal_like(rng_key_sample, p)\n",
    "        if normalize:\n",
    "            scale = (P ** 0.5) / tree_norm(dp)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        # scale dp to have squared norm P * sigma^2\n",
    "        dp = jax.tree_util.tree_map(lambda x: x * scale, dp)\n",
    "        trace = trace_estimator(model, p, x, dp, sigma=sigma)\n",
    "        traces.append(trace)\n",
    "    trace = jnp.array(traces).mean()\n",
    "    return trace / jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(train_loss_fn, test_loss_fn, neg_log_p_w_fn, n_train, init_params, lr, n_step, rng_key, model, x_eval_generator, logdet_calculator, optimizer, jitter, method, temp=1.0):    \n",
    "    def get_train_state(optimizer, lr, init_params, warmup_steps=100):\n",
    "        lr_sched = optax.linear_schedule(0, lr, warmup_steps, transition_begin=0)\n",
    "        if optimizer == 'adam':\n",
    "            tx = optax.chain(\n",
    "                    optax.adam(learning_rate=lr_sched),\n",
    "                )\n",
    "        elif optimizer == 'sgd':\n",
    "            tx = optax.sgd(learning_rate=lr_sched, momentum=0.9)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return train_state.TrainState.create(apply_fn=model.apply, params=init_params, tx=tx)\n",
    "    \n",
    "    ts = get_train_state(optimizer, lr, init_params)\n",
    "    \n",
    "    def augmented_loss_fn(p, x_eval, dp):\n",
    "        # loss = likelihood / n_train = 1 / (2 * sigma^2) * ||y - f(x)||^2 / n_train\n",
    "        # it contains a factor 1 / n_train\n",
    "        # all other terms should be divided by n_train as well\n",
    "        avg_nll = train_loss_fn(p) \n",
    "        test_loss = test_loss_fn(p)\n",
    "        # number of params\n",
    "        P = jax.tree_util.tree_leaves(p)[0].shape[0]\n",
    "        if method == 'fsmap':\n",
    "            # logdet = 1 / 2 * log_det_g_svd(model, p, x_eval, jitter) / n_train\n",
    "            logdet = 1 / 2 * logdet_calculator.compute_log_det(p, jitter) / n_train\n",
    "        elif method == 'psmap':\n",
    "            # logdet = 1 / 2 * log_det_g_svd(model, p, x_eval, jitter) / n_train\n",
    "            logdet = 1 / 2 * logdet_calculator.compute_log_det(p, jitter) / n_train\n",
    "            logdet = jax.lax.stop_gradient(logdet)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        neg_log_p_w = neg_log_p_w_fn(p) / n_train #jnp.sum((params_flat ** 2) / (2 * (prior_stds ** 2))) / n_train\n",
    "        logdet =  temp * logdet + (1 - temp) * jax.lax.stop_gradient(logdet)\n",
    "        # return avg_nll + neg_log_p_w + logdet, (avg_nll, logdet, test_loss)\n",
    "        return avg_nll + neg_log_p_w + logdet, (avg_nll, logdet, test_loss)\n",
    "\n",
    "    grad_fn = jax.jit(jax.value_and_grad(lambda p, x_eval, dp: augmented_loss_fn(p, x_eval, dp), has_aux=True))\n",
    "    losses = []\n",
    "    logdets = []\n",
    "    avg_nlls = []\n",
    "    test_losss = []\n",
    "    def sample_dp(p, rng_key):\n",
    "        # gaussian tree\n",
    "        dp = tree_random_normal_like(rng_key, p)\n",
    "        return dp\n",
    "    @jax.jit\n",
    "    def train_step(ts, rng_key):\n",
    "        rng_key, x_eval_key = jax.random.split(rng_key)\n",
    "        rng_key, dp_key = jax.random.split(rng_key)\n",
    "        x_eval = x_eval_generator(x_eval_key)\n",
    "        dp = sample_dp(ts.params, dp_key)\n",
    "        (loss, aux), grads = grad_fn(ts.params, x_eval, dp)\n",
    "        ts = ts.apply_gradients(grads=grads)\n",
    "        return ts, loss, aux, rng_key\n",
    "    for _ in tqdm(range(int(n_step))):\n",
    "        ts, loss, aux, rng_key = train_step(ts, rng_key)\n",
    "        avg_nll, logdet, test_loss = aux\n",
    "        losses.append(loss.item())\n",
    "        logdets.append(logdet.item())\n",
    "        avg_nlls.append(avg_nll.item())\n",
    "        test_losss.append(test_loss.item())\n",
    "    losses = np.array(losses)\n",
    "    logdets = np.array(logdets)\n",
    "    avg_nlls = np.array(avg_nlls)\n",
    "    test_losss = np.array(test_losss)\n",
    "    return ts.params, losses, avg_nlls, logdets, test_losss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(fn, p, noise_std=0.1, samples=10):\n",
    "    # fn: parameters pytree -> number\n",
    "    # p: parameters\n",
    "    # noise: add noise N(0, noise_std^2) to the parameters\n",
    "    # return average absolute change in fn if we perturb parameters\n",
    "    changes = []\n",
    "    fn0 = fn(p)\n",
    "    for i in range(samples):\n",
    "        rng_key = jax.random.PRNGKey(i)\n",
    "        rng_key, rng_key_sample = jax.random.split(rng_key)\n",
    "        dp = tree_random_normal_like(rng_key_sample, p)\n",
    "        new_p = jax.tree_util.tree_map(lambda x, y: x + y * noise_std, p, dp)\n",
    "        change = jnp.abs(fn(new_p) - fn0)\n",
    "        changes.append(change)\n",
    "    return jnp.array(changes).mean().item()\n",
    "\n",
    "\n",
    "def avg_hess_eig(loss_fn, p):\n",
    "    # fn: parameters pytree -> number\n",
    "    # p: parameters\n",
    "    # return average eigenvalue of hessian at p\n",
    "    p_flat, unflatten = jax.flatten_util.ravel_pytree(p)\n",
    "    def loss_fn_flat(p_flat):\n",
    "        p = unflatten(p_flat)\n",
    "        return loss_fn(p)\n",
    "    hess_fn = jax.jit(jax.hessian(loss_fn_flat), device=jax.devices('cpu')[0])\n",
    "    hess = hess_fn(p_flat)\n",
    "    eigvals = jnp.linalg.eigvalsh(hess)\n",
    "    mean_eig = eigvals.mean().item()\n",
    "    assert mean_eig > 0, 'mean eigenvalue of hessian is non-positive'\n",
    "    return mean_eig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset(model, in_dim, n_train, x_eval_generator, n_step, lr, optimizer, seed, method, noise_std, jitter, temp=1, result_path=None, plot=False, prior_misspecification=1, noise_misspecification=1):\n",
    "    # define log det calculator\n",
    "    x_eval = x_eval_generator(jax.random.PRNGKey(0))\n",
    "    logdet_calculator = LogDet(model, x_eval)\n",
    "    # count parameters\n",
    "    dummy_param = model.init(jax.random.PRNGKey(0), jnp.ones((1, in_dim)))\n",
    "    P = param_size(dummy_param)\n",
    "    print(f\"Number of parameters: {P}\")\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    \n",
    "    prior_stds = model.prior_stds # (p,)\n",
    "    rng_key, init_params_key = jax.random.split(rng_key)\n",
    "    w_true = model.init(init_params_key, jnp.ones((1, in_dim))) # (p,)\n",
    "    rng_key, x_train_key = jax.random.split(rng_key)\n",
    "    x_train = jax.random.uniform(x_train_key, (n_train, in_dim), minval=-1, maxval=1)\n",
    "    y_train = model.apply(w_true, x_train)\n",
    "    rng_key, noise_key = jax.random.split(rng_key)\n",
    "    noise = jax.random.normal(noise_key, y_train.shape) * noise_std\n",
    "    print('Y train mean:', jnp.mean(y_train))\n",
    "    print('Y train std:', jnp.std(y_train))\n",
    "    y_train = y_train + noise\n",
    "    \n",
    "    # misspecification\n",
    "    noise_std = noise_std / noise_misspecification\n",
    "    prior_stds = prior_stds / prior_misspecification\n",
    "    \n",
    "    # x_test = jax.random.uniform(jax.random.PRNGKey(99), (1000, in_dim), minval=-1, maxval=1)\n",
    "    x_test = jnp.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "    y_test = model.apply(w_true, x_test)\n",
    "    # zero initialization\n",
    "    init_params = jax.tree_util.tree_map(lambda x: jnp.zeros_like(x), w_true)\n",
    "    train_loss_fn = lambda p: jnp.mean((model.apply(p, x_train) - y_train) ** 2) / (2 * noise_std ** 2)\n",
    "    test_loss_fn = lambda p: jnp.mean((model.apply(p, x_test) - y_test) ** 2) / (2 * noise_std ** 2)\n",
    "    neg_log_p_w_fn = lambda p: jnp.sum((jax.flatten_util.ravel_pytree(p)[0] ** 2) / (2 * (prior_stds ** 2)))\n",
    "    train_rmse = lambda p: jnp.sqrt(jnp.mean((model.apply(p, x_train) - y_train) ** 2))\n",
    "    train_mse = lambda p: jnp.mean((model.apply(p, x_train) - y_train) ** 2)\n",
    "    test_rmse = lambda p: jnp.sqrt(jnp.mean((model.apply(p, x_test) - y_test) ** 2))\n",
    "    \n",
    "    # log training time\n",
    "    start_time = time.time()\n",
    "    params, losses, avg_nlls, logdets, test_losss = optimize(train_loss_fn, test_loss_fn, neg_log_p_w_fn, n_train, init_params, lr, n_step, rng_key, model, x_eval_generator, logdet_calculator, optimizer, jitter, method, temp)\n",
    "    end_time = time.time()\n",
    "    # train time in seconds\n",
    "    train_time = end_time - start_time\n",
    "    # plot and save losses\n",
    "    plt.figure()\n",
    "    \n",
    "    def make_subplots(arrays, labels):\n",
    "        n_plots = len(arrays)\n",
    "        fig, axs = plt.subplots(n_plots, 1, figsize=(6, 6))\n",
    "        for i, (a, label) in enumerate(zip(arrays, labels)):\n",
    "            q1, q2 = np.quantile(a, [0., 0.98])\n",
    "            axs[i].plot(a)\n",
    "            axs[i].set_ylim(q1, q2)\n",
    "            axs[i].set_xlabel('Step')\n",
    "            axs[i].set_ylabel(label)\n",
    "        plt.show()\n",
    "    \n",
    "    make_subplots([losses, logdets, avg_nlls, test_losss], ['Loss', 'Logdet', 'Train NLL', 'Test NLL'])\n",
    "    \n",
    "    # plot data and predictions\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 8), dpi=100)\n",
    "        plt.scatter(x_train.squeeze(-1), y_train.squeeze(-1), label='Train', s=20, color='black')\n",
    "        plt.plot(x_test.squeeze(-1), y_test.squeeze(-1), label='Test', color='red')\n",
    "        plt.plot(x_test.squeeze(-1), model.apply(params, x_test).squeeze(-1), label='Predictions', color='blue')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def measure(params, logdet_calculator):\n",
    "        neg_log_param_prior = neg_log_p_w_fn(params).item()\n",
    "        log_det = logdet_calculator.compute_log_det(params, jitter=1e-32).item()\n",
    "        train_loss = train_loss_fn(params).item()\n",
    "        measurements = {\n",
    "            'log_det': log_det,\n",
    "            'log_fs_posterior': -n_train * train_loss - neg_log_param_prior - 1 / 2 * log_det,\n",
    "            'train_rmse': train_rmse(params).item(),\n",
    "            'test_rmse': test_rmse(params).item(),\n",
    "            'hess_train_mse': avg_hess_eig(train_mse, params),\n",
    "            'log_ps_posterior': -n_train * train_loss - neg_log_param_prior,\n",
    "            'neg_log_param_prior': neg_log_param_prior,\n",
    "            'train_time': train_time,\n",
    "            'jitter': jitter,\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss_fn(params).item(),\n",
    "        }\n",
    "        print(measurements)\n",
    "        # return log_likelihood + neg_log_param_prior - 1 / 2 * log_det\n",
    "        return measurements\n",
    "    \n",
    "    # compute function space posterior \n",
    "    measurements = measure(params, logdet_calculator)\n",
    "    if result_path is not None:\n",
    "        torch.save(measurements, result_path)\n",
    "\n",
    "    return measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_compare(model, in_dim, x_train, y_train, x_eval_generator, n_step, lr, optimizer, noise_std, jitter, temp=1, result_path=None, plot=False):\n",
    "    n_train = x_train.shape[0]\n",
    "    # define log det calculator\n",
    "    x_eval = x_eval_generator(jax.random.PRNGKey(0))\n",
    "    logdet_calculator = LogDet(model, x_eval, svd=True)\n",
    "    # count parameters\n",
    "    dummy_param = model.init(jax.random.PRNGKey(0), jnp.ones((1, in_dim)))\n",
    "    P = param_size(dummy_param)\n",
    "    print(f\"Number of parameters: {P}\")\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    \n",
    "    prior_stds = model.prior_stds # (p,)\n",
    "\n",
    "    print('Y train mean:', jnp.mean(y_train))\n",
    "    print('Y train std:', jnp.std(y_train))\n",
    "    \n",
    "    x_test = jnp.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "    # zero initialization\n",
    "    init_params = jax.tree_util.tree_map(lambda x: jnp.zeros_like(x), dummy_param)\n",
    "    train_loss_fn = lambda p: jnp.mean((model.apply(p, x_train) - y_train) ** 2) / (2 * noise_std ** 2)\n",
    "    test_loss_fn = lambda p: 0\n",
    "    neg_log_p_w_fn = lambda p: jnp.sum((jax.flatten_util.ravel_pytree(p)[0] ** 2) / (2 * (prior_stds ** 2)))\n",
    "    train_rmse = lambda p: jnp.sqrt(jnp.mean((model.apply(p, x_train) - y_train) ** 2))\n",
    "    test_rmse = lambda p: 0\n",
    "    \n",
    "    # log training time\n",
    "    start_time = time.time()\n",
    "    ps_params, *_ = optimize(train_loss_fn, test_loss_fn, neg_log_p_w_fn, n_train, init_params, lr, n_step, rng_key, model, x_eval_generator, logdet_calculator, optimizer, jitter, 'psmap', temp)\n",
    "    fs_params, *_ = optimize(train_loss_fn, test_loss_fn, neg_log_p_w_fn, n_train, init_params, lr, n_step, rng_key, model, x_eval_generator, logdet_calculator, optimizer, jitter, 'fsmap', temp)\n",
    "    w_ps, w_fs = ps_params['params']['mixture_coeffs'], fs_params['params']['mixture_coeffs']\n",
    "    end_time = time.time()\n",
    "\n",
    "    @jax.jit\n",
    "    def log_posteriors(params):\n",
    "        neg_log_param_prior = neg_log_p_w_fn(params)\n",
    "        log_det = logdet_calculator.compute_log_det(params)\n",
    "        train_loss = train_loss_fn(params)\n",
    "        log_fs_posterior = -n_train * train_loss - neg_log_param_prior - 1 / 2 * log_det\n",
    "        log_ps_posterior = -n_train * train_loss - neg_log_param_prior\n",
    "        return log_fs_posterior, log_ps_posterior\n",
    "    # compute posteriors over a 2d grid [-3, 3] x [-3, 3]\n",
    "    w1 = ps_params['params']['mixture_coeffs'][0]\n",
    "    w2s = jnp.linspace(-1000, 5, 1000) # (100,)\n",
    "    # w2s = jax.random.uniform(jax.random.PRNGKey(0), shape=(1000,), minval=-10, maxval=1)\n",
    "    # sort w2s\n",
    "    w2s = jnp.sort(w2s)\n",
    "    log_fs_posteriors = []\n",
    "    log_ps_posteriors = []\n",
    "    preds = []\n",
    "    _, unflatten = jax.flatten_util.ravel_pytree(dummy_param)\n",
    "    for w2 in tqdm(w2s):\n",
    "        w = jnp.array([w1, w2])\n",
    "        params = unflatten(w)\n",
    "        log_fs_posterior, log_ps_posterior = log_posteriors(params)\n",
    "        log_fs_posteriors.append(log_fs_posterior)\n",
    "        log_ps_posteriors.append(log_ps_posterior)\n",
    "        preds.append(model.apply(params, x_test))\n",
    "    log_fs_posteriors = jnp.array(log_fs_posteriors)\n",
    "    log_ps_posteriors = jnp.array(log_ps_posteriors)\n",
    "    preds = jnp.array(preds) # (100, 1000, 1)\n",
    "    ps_posteriors = jax.nn.softmax(log_ps_posteriors, axis=0) # (100,)\n",
    "    pred_mean = jnp.sum(preds * ps_posteriors[:, None, None], axis=0) # (1000, 1)\n",
    "    # sample from preds with probability ps_posteriors\n",
    "    num_samples = 50\n",
    "    indices = jax.random.choice(jax.random.PRNGKey(0), jnp.arange(len(ps_posteriors)), shape=(num_samples,), p=ps_posteriors)\n",
    "    pred_sampleds = preds[indices] # (num_samples, 1000, 1)\n",
    "    \n",
    "    # plot data and predictions\n",
    "    plt.figure(figsize=(6.5, 5), dpi=200)\n",
    "    plt.scatter(x_train.squeeze(-1), y_train.squeeze(-1), s=20, color='black', zorder=10, label='Training Data')\n",
    "    plt.plot(x_test.squeeze(-1), model.apply(fs_params, x_test).squeeze(-1), label='FS-MAP', color='C0', alpha=1, linewidth=3)\n",
    "    plt.plot(x_test.squeeze(-1), model.apply(ps_params, x_test).squeeze(-1), label='PS-MAP', color='C1', linestyle='--', alpha=1, linewidth=2.5)\n",
    "    # plot samples with thin lines and label only the first one\n",
    "    # plt.plot(x_test.squeeze(-1), pred_sampleds.squeeze(-1).T, color='C2', alpha=0.3, linewidth=1)\n",
    "    # plt.plot(x_test.squeeze(-1), pred_sampleds[0].squeeze(-1), color='C2', alpha=0.3, linewidth=1, label='Posterior Samples')\n",
    "    \n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.plot(x_test.squeeze(-1), pred_mean.squeeze(-1), label='BMA', color='green', linewidth=2)\n",
    "    plt.ylim([-0.01, 1])\n",
    "    plt.legend(loc='upper right', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    return w2s, log_fs_posteriors, log_ps_posteriors, w_ps, w_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_features(dim, num_freqs_per_dim, min_freq, max_freq):\n",
    "    \"Return a function that computes Fourier features (sin and cos) for an input of given dimension.\"\n",
    "    assert min_freq > 0 and max_freq > 0, \"min_freq and max_freq must both be positive\"\n",
    "    # num_freqs = num_freqs_per_dim ** dim\n",
    "    # create a (num_freqs, dim) matrix of frequencies on a dim-dimensional lattice\n",
    "    k = jnp.stack(jnp.meshgrid(*[jnp.linspace(min_freq, max_freq, num_freqs_per_dim)] * dim), axis=-1).reshape(-1, dim) # (num_freqs, dim)\n",
    "    def feature_fn(x):\n",
    "        # (n, dim) -> (n, 2 * num_freqs_per_dim ** dim)\n",
    "        return jnp.concatenate([jnp.sin(x @ k.T), jnp.cos(x @ k.T)], axis=-1)\n",
    "    return feature_fn\n",
    "\n",
    "def polynomial_features(dim, num_degs_per_dim):\n",
    "    \"Return a function that computes polynomial features for an input of given dimension.\"\n",
    "    assert num_degs_per_dim > 0, \"num_degs_per_dim must be positive\"\n",
    "    # num_degs = num_degs_per_dim ** dim\n",
    "    # create a (num_degs, dim) matrix of degrees on a dim-dimensional lattice\n",
    "    k = jnp.stack(jnp.meshgrid(*[jnp.arange(num_degs_per_dim)] * dim), axis=-1).reshape(-1, dim) # (num_degs, dim)\n",
    "    def feature_fn(x):\n",
    "        # (n, 1, dim) * (1, num_degs, dim) -> (n, num_degs)\n",
    "        x = x.reshape(-1, 1, dim)\n",
    "        return jnp.prod(2 * x ** k[None, :, :], axis=-1) # (n, num_degs)\n",
    "    return feature_fn\n",
    "\n",
    "def rbf_features(cs, width, height=1):\n",
    "    # RBF function\n",
    "    def rbf(x, c, width):\n",
    "        return height * jnp.exp(-(x - c) ** 2 / width)\n",
    "    def feature_fn(x):\n",
    "        feats = [rbf(x, c, width) for c in cs]\n",
    "        feats = jnp.concatenate(feats, axis=-1)\n",
    "        return feats\n",
    "    return feature_fn\n",
    "\n",
    "def cos_features(dim, num_freqs_per_dim, min_freq, max_freq):\n",
    "    \"Return a function that computes Fourier features (sin and cos) for an input of given dimension.\"\n",
    "    assert min_freq > 0 and max_freq > 0, \"min_freq and max_freq must both be positive\"\n",
    "    # num_freqs = num_freqs_per_dim ** dim\n",
    "    # create a (num_freqs, dim) matrix of frequencies on a dim-dimensional lattice\n",
    "    k = jnp.stack(jnp.meshgrid(*[jnp.linspace(min_freq, max_freq, num_freqs_per_dim)] * dim), axis=-1).reshape(-1, dim) # (num_freqs, dim)\n",
    "    def feature_fn(x):\n",
    "        # (n, dim) -> (n, num_freqs_per_dim ** dim)\n",
    "        return jnp.cos(x @ k.T) # (n, num_freqs_per_dim ** dim)\n",
    "    return feature_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 1\n",
    "num_freqs_per_dim = 2\n",
    "min_freq = 1 * (2 * np.pi / 2) # n 2 pi / L\n",
    "max_freq = 5 * (2 * np.pi / 2)\n",
    "feature_fn = cos_features(in_dim, num_freqs_per_dim, min_freq, max_freq) # 2 * num_freqs_per_dim ** dim features\n",
    "prior_stds = 5 * np.ones(num_freqs_per_dim ** in_dim)\n",
    "act_fn = jax.nn.tanh\n",
    "model = NonLinearMixture(feature_fn, act_fn, prior_stds)\n",
    "\n",
    "def x_eval_generator(rng_key):\n",
    "    return jax.random.uniform(rng_key, shape=(1000, in_dim), minval=-1, maxval=1)\n",
    "\n",
    "x_eval = x_eval_generator(jax.random.PRNGKey(0))\n",
    "logdet_calculator = LogDet(model, x_eval)\n",
    "\n",
    "params = model.init(jax.random.PRNGKey(0), jnp.ones((1, in_dim))) # (p,)\n",
    "\n",
    "logdet_calculator.compute_log_det(params)\n",
    "# visualize logdet_calculator.Phi\n",
    "plt.figure()\n",
    "plt.imshow(logdet_calculator.Phi, cmap='gray')\n",
    "# colorbar\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: log det computed with SVD!\n",
    "in_dim = 1\n",
    "cs = [-0.5, 0.5]\n",
    "width = 0.03\n",
    "feature_fn = rbf_features(cs, width, height=1)\n",
    "prior_scale = 10\n",
    "prior_stds = prior_scale * np.ones(2)\n",
    "act_fn = jnp.exp\n",
    "model = NonLinearMixture(feature_fn, act_fn, prior_stds)\n",
    "\n",
    "# optimizer\n",
    "optimizer = 'adam'\n",
    "lr = 3e-1\n",
    "n_step = int(1000)\n",
    "\n",
    "\n",
    "jitter = 1e-32\n",
    "def x_eval_generator(rng_key):\n",
    "    return jax.random.uniform(rng_key, shape=(1000, in_dim), minval=-1, maxval=1)\n",
    "\n",
    "# ------------------------------ #\n",
    "# output_dir = f'nonlinear_mixture/weights/{feature_name}'\n",
    "skip_if_done = True\n",
    "\n",
    "n_train = 10\n",
    "x_train = jax.random.normal(jax.random.PRNGKey(0), shape=(n_train, in_dim)) * 0.2 - 0.5\n",
    "y_train = jnp.exp(-(x_train + 0.5) ** 2 / 0.03).reshape(-1, 1)\n",
    "# add noise\n",
    "y_train += jax.random.normal(jax.random.PRNGKey(2), shape=y_train.shape) * 0.1\n",
    "# add new data\n",
    "new_x = jnp.array([0.7]).reshape(-1, in_dim)\n",
    "new_y = jnp.array([0.1]).reshape(-1, 1)\n",
    "x_train = jnp.concatenate([x_train, new_x], axis=0)\n",
    "y_train = jnp.concatenate([y_train, new_y], axis=0)\n",
    "\n",
    "noise_std = 0.1 # observation noise in likelihood\n",
    "r = run_compare(model, in_dim, x_train, y_train, x_eval_generator, n_step, lr, optimizer, noise_std, jitter, temp=1, result_path=None, plot=True)\n",
    "plt.savefig(f'nonlinear_mixture/plots/GM_{prior_scale}.pdf')\n",
    "\n",
    "w2, pf, pw, w_ps, w_fs = r\n",
    "pf = jax.nn.softmax(pf)\n",
    "pw = jax.nn.softmax(pw)\n",
    "\n",
    "# x = jnp.exp(w2) # not uniform spacing\n",
    "x = w2\n",
    "dx = jnp.diff(x)\n",
    "dx = jnp.concatenate([dx, dx[-1:]]) # repeat the last element\n",
    "pf_area = jnp.sum(pf * dx)\n",
    "pw_area = jnp.sum(pw * dx)\n",
    "# normalize area \n",
    "pf = pf / pf_area\n",
    "pw = pw / pw_area\n",
    "# Plot w2 and pf\n",
    "plt.figure(figsize=(6.5, 5), dpi=200)\n",
    "plt.plot(x, pf, label=r'$p(f_\\theta|\\mathcal{D})$', color='C0')\n",
    "plt.plot(x, pw, label=r'$p(\\theta|\\mathcal{D})$', color='C1')\n",
    "# plt.xlabel(r'$\\exp(\\theta_R)$')\n",
    "plt.xlabel(r'$\\theta_R$')\n",
    "# plt.xlim([-0.1, 2.5])\n",
    "\n",
    "# Label the max of each curve on the x-axis\n",
    "pf_max_index = jnp.argmax(pf)\n",
    "pw_max_index = jnp.argmax(pw)\n",
    "\n",
    "plt.plot((w2[pf_max_index]), pf[pf_max_index], '*', color='C0', markersize=20)\n",
    "plt.plot((w2[pw_max_index]), pw[pw_max_index], '*', color='C1', markersize=20)\n",
    "# plt.yscale('log')\n",
    "plt.legend(fontsize=20)\n",
    "# add margin to each side\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'nonlinear_mixture/plots/GM_posterior_{prior_scale}.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch\n",
    "# data generation\n",
    "in_dim = 1\n",
    "num_freqs_per_dim = 20\n",
    "min_freq = 1 * 2 * np.pi / 2 # n 2 pi / L\n",
    "max_freq = 20 * 2 * np.pi / 2\n",
    "feature_name = 'fourier1d'\n",
    "feature_fn = fourier_features(in_dim, num_freqs_per_dim, min_freq, max_freq) # 2 * num_freqs_per_dim ** dim features\n",
    "prior_stds = 5 * np.ones(2 * num_freqs_per_dim ** in_dim)\n",
    "act_fn = jax.nn.tanh\n",
    "model = NonLinearMixture(feature_fn, act_fn, prior_stds, is_orthogonal=True)\n",
    "\n",
    "# optimizer\n",
    "optimizer = 'adam'\n",
    "lr = 1e-1\n",
    "n_step = int(2500)\n",
    "\n",
    "# method\n",
    "jitter = 1e-32\n",
    "def x_eval_generator(rng_key):\n",
    "    return jax.random.uniform(rng_key, shape=(1000, in_dim), minval=-1, maxval=1)\n",
    "\n",
    "# ------------------------------ #\n",
    "output_dir = f'nonlinear_mixture/weights/{feature_name}'\n",
    "skip_if_done = False\n",
    "fsmap_results = []\n",
    "\n",
    "seeds = range(3)\n",
    "n_trains = [2560]\n",
    "noise_stds = [0.1]\n",
    "\n",
    "\n",
    "for seed, n_train, noise_std in itertools.product(seeds, n_trains, noise_stds):\n",
    "    for method in ['psmap', 'fsmap']:\n",
    "        # mkdir if needed\n",
    "        r = run_dataset(model, in_dim, n_train, x_eval_generator, n_step, lr, optimizer, seed, method, noise_std, jitter, temp=1, result_path=None, plot=False)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSMAP\n",
    "# data generation\n",
    "in_dim = 1\n",
    "num_freqs_per_dim = 100\n",
    "min_freq = 1 * 2 * np.pi / 2 # n 2 pi / L\n",
    "max_freq = num_freqs_per_dim * 2 * np.pi / 2\n",
    "feature_name = 'fourier1d'\n",
    "feature_fn = fourier_features(in_dim, num_freqs_per_dim, min_freq, max_freq) # 2 * num_freqs_per_dim ** dim features\n",
    "prior_stds = 10 * np.ones(2 * num_freqs_per_dim ** in_dim)\n",
    "act_fn = jax.nn.tanh\n",
    "model = NonLinearMixture(feature_fn, act_fn, prior_stds, is_orthogonal=True)\n",
    "\n",
    "# optimizer\n",
    "optimizer = 'adam'\n",
    "lr = 1e-1\n",
    "n_step = int(2500)\n",
    "\n",
    "# method\n",
    "method = 'fsmap'\n",
    "jitter = 1e-32\n",
    "def x_eval_generator(rng_key):\n",
    "    return jax.random.uniform(rng_key, shape=(1000, in_dim), minval=-1, maxval=1)\n",
    "\n",
    "# ------------------------------ #\n",
    "output_dir = f'nonlinear_mixture/weights/{feature_name}'\n",
    "skip_if_done = True\n",
    "fsmap_results = []\n",
    "\n",
    "seeds = range(3)\n",
    "# n_trains = [5, 10, 20, 40, 80, 160, 320, 640, 1280, 2560]\n",
    "n_trains = [100, 200, 400, 800, 1600, 3200, 6400]\n",
    "noise_stds = [0.1]\n",
    "\n",
    "for seed, n_train, noise_std in itertools.product(seeds, n_trains, noise_stds):\n",
    "    # mkdir if needed\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    result_path = f'{output_dir}/{method}_seed{seed}_ntrain{n_train}_noise{noise_std}.pt'\n",
    "    if os.path.exists(result_path) and skip_if_done:\n",
    "        r = torch.load(result_path)\n",
    "        print('Loaded result from ', result_path)\n",
    "    else:\n",
    "        r = run_dataset(model, in_dim, n_train, x_eval_generator, n_step, lr, optimizer, seed, method, noise_std, jitter, temp=1, result_path=result_path)\n",
    "    r['n_train'] = n_train\n",
    "    r['noise_std'] = noise_std\n",
    "    r['seed'] = seed\n",
    "    fsmap_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSMAP\n",
    "# data generation\n",
    "in_dim = 1\n",
    "num_freqs_per_dim = 100\n",
    "min_freq = 1 * 2 * np.pi / 2 # n 2 pi / L\n",
    "max_freq = num_freqs_per_dim * 2 * np.pi / 2\n",
    "feature_name = 'fourier1d'\n",
    "feature_fn = fourier_features(in_dim, num_freqs_per_dim, min_freq, max_freq) # 2 * num_freqs_per_dim ** dim features\n",
    "prior_stds = 10 * np.ones(2 * num_freqs_per_dim ** in_dim)\n",
    "act_fn = jax.nn.tanh\n",
    "model = NonLinearMixture(feature_fn, act_fn, prior_stds, is_orthogonal=True)\n",
    "\n",
    "# optimizer\n",
    "optimizer = 'adam'\n",
    "lr = 1e-1\n",
    "n_step = int(2500)\n",
    "\n",
    "# method\n",
    "method = 'psmap'\n",
    "jitter = 1e-32\n",
    "def x_eval_generator(rng_key):\n",
    "    return jax.random.uniform(rng_key, shape=(1000, in_dim), minval=-1, maxval=1)\n",
    "\n",
    "# ------------------------------ #\n",
    "output_dir = f'nonlinear_mixture/weights/{feature_name}'\n",
    "skip_if_done = True\n",
    "psmap_results = []\n",
    "\n",
    "seeds = range(3)\n",
    "# n_trains = [5, 10, 20, 40, 80, 160, 320, 640, 1280, 2560]\n",
    "n_trains = [100, 200, 400, 800, 1600, 3200, 6400]\n",
    "noise_stds = [0.1]\n",
    "\n",
    "for seed, n_train, noise_std in itertools.product(seeds, n_trains, noise_stds):\n",
    "    # mkdir if needed\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    result_path = f'{output_dir}/{method}_seed{seed}_ntrain{n_train}_noise{noise_std}.pt'\n",
    "    if os.path.exists(result_path) and skip_if_done and 'hess_train_mse' in torch.load(result_path):\n",
    "        r = torch.load(result_path)\n",
    "        print('Loaded result from ', result_path)\n",
    "    else:\n",
    "        r = run_dataset(model, in_dim, n_train, x_eval_generator, n_step, lr, optimizer, seed, method, noise_std, jitter, temp=1, result_path=result_path)\n",
    "    r['n_train'] = n_train\n",
    "    r['noise_std'] = noise_std\n",
    "    r['seed'] = seed\n",
    "    psmap_results.append(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misspecified evaluation distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSMAP\n",
    "# data generation\n",
    "in_dim = 1\n",
    "num_freqs_per_dim = 100\n",
    "min_freq = 1 * 2 * np.pi / 2 # n 2 pi / L\n",
    "max_freq = num_freqs_per_dim * 2 * np.pi / 2\n",
    "feature_name = 'fourier1d_noise_mis0.1'\n",
    "feature_fn = fourier_features(in_dim, num_freqs_per_dim, min_freq, max_freq) # 2 * num_freqs_per_dim ** dim features\n",
    "prior_stds = 10 * np.ones(2 * num_freqs_per_dim ** in_dim)\n",
    "act_fn = jax.nn.tanh\n",
    "model = NonLinearMixture(feature_fn, act_fn, prior_stds, is_orthogonal=True)\n",
    "\n",
    "# optimizer\n",
    "optimizer = 'adam'\n",
    "lr = 1e-1\n",
    "n_step = int(2500)\n",
    "\n",
    "# method\n",
    "method = 'fsmap'\n",
    "jitter = 1e-6\n",
    "def x_eval_generator(rng_key):\n",
    "    return jax.random.uniform(rng_key, shape=(1000, in_dim), minval=-1, maxval=1)\n",
    "\n",
    "# ------------------------------ #\n",
    "output_dir = f'nonlinear_mixture/weights/{feature_name}'\n",
    "skip_if_done = True\n",
    "fsmap_results = []\n",
    "\n",
    "seeds = range(3)\n",
    "# n_trains = [5, 10, 20, 40, 80, 160, 320, 640, 1280, 2560]\n",
    "n_trains = [100, 200, 400, 800, 1600, 3200, 6400]\n",
    "noise_stds = [0.1]\n",
    "prior_misspecification=1\n",
    "noise_misspecification=0.1\n",
    "\n",
    "for seed, n_train, noise_std in itertools.product(seeds, n_trains, noise_stds):\n",
    "    # mkdir if needed\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    result_path = f'{output_dir}/{method}_seed{seed}_ntrain{n_train}_noise{noise_std}.pt'\n",
    "    if os.path.exists(result_path) and skip_if_done:\n",
    "        r = torch.load(result_path)\n",
    "        print('Loaded result from ', result_path)\n",
    "    else:\n",
    "        r = run_dataset(model, in_dim, n_train, x_eval_generator, n_step, lr, optimizer, seed, method, noise_std, jitter, temp=1, result_path=result_path, prior_misspecification=prior_misspecification, noise_misspecification=noise_misspecification)\n",
    "    r['n_train'] = n_train\n",
    "    r['noise_std'] = noise_std\n",
    "    r['seed'] = seed\n",
    "    fsmap_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSMAP\n",
    "# data generation\n",
    "in_dim = 1\n",
    "num_freqs_per_dim = 100\n",
    "min_freq = 1 * 2 * np.pi / 2 # n 2 pi / L\n",
    "max_freq = num_freqs_per_dim * 2 * np.pi / 2\n",
    "feature_name = 'fourier1d_prior_mis0.1'\n",
    "feature_fn = fourier_features(in_dim, num_freqs_per_dim, min_freq, max_freq) # 2 * num_freqs_per_dim ** dim features\n",
    "prior_stds = 10 * np.ones(2 * num_freqs_per_dim ** in_dim)\n",
    "act_fn = jax.nn.tanh\n",
    "model = NonLinearMixture(feature_fn, act_fn, prior_stds, is_orthogonal=True)\n",
    "\n",
    "# optimizer\n",
    "optimizer = 'adam'\n",
    "lr = 1e-1\n",
    "n_step = int(2500)\n",
    "\n",
    "# method\n",
    "method = 'psmap'\n",
    "jitter = 1e-32\n",
    "def x_eval_generator(rng_key):\n",
    "    return jax.random.uniform(rng_key, shape=(1000, in_dim), minval=-1, maxval=1)\n",
    "\n",
    "# ------------------------------ #\n",
    "output_dir = f'nonlinear_mixture/weights/{feature_name}'\n",
    "skip_if_done = True\n",
    "psmap_results = []\n",
    "\n",
    "seeds = range(3)\n",
    "# n_trains = [5, 10, 20, 40, 80, 160, 320, 640, 1280, 2560]\n",
    "n_trains = [100, 200, 400, 800, 1600, 3200, 6400]\n",
    "noise_stds = [0.1]\n",
    "prior_misspecification=1\n",
    "noise_misspecification=1\n",
    "\n",
    "for seed, n_train, noise_std in itertools.product(seeds, n_trains, noise_stds):\n",
    "    # mkdir if needed\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    result_path = f'{output_dir}/{method}_seed{seed}_ntrain{n_train}_noise{noise_std}.pt'\n",
    "    if os.path.exists(result_path) and skip_if_done:\n",
    "        r = torch.load(result_path)\n",
    "        print('Loaded result from ', result_path)\n",
    "    else:\n",
    "        r = run_dataset(model, in_dim, n_train, x_eval_generator, n_step, lr, optimizer, seed, method, noise_std, jitter, temp=1, result_path=result_path, prior_misspecification=prior_misspecification, noise_misspecification=noise_misspecification)\n",
    "    r['n_train'] = n_train\n",
    "    r['noise_std'] = noise_std\n",
    "    r['seed'] = seed\n",
    "    psmap_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert fsmap results to pandas dataframe\n",
    "import pandas as pd\n",
    "df_fsmap = pd.DataFrame(fsmap_results)\n",
    "df_psmap = pd.DataFrame(psmap_results)\n",
    "df_fsmap['method'] = 'FS-MAP'\n",
    "df_psmap['method'] = 'PS-MAP'\n",
    "\n",
    "df = pd.concat([df_fsmap, df_psmap])\n",
    "# sort by n_train and method\n",
    "df = df.sort_values(by=['n_train', 'method'])\n",
    "# add a column for generalization gap\n",
    "df['generalization_gap'] = df['test_rmse'] - df['train_rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker\n",
    "from matplotlib.ticker import ScalarFormatter, NullFormatter\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=3, rc={\"lines.linewidth\": 2})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "os.makedirs(f'nonlinear_mixture/plots/{feature_name}', exist_ok=True)\n",
    "\n",
    "# plt.rcParams['ytick.labelsize'] = 'x-small'\n",
    "    \n",
    "# Create a new DataFrame where each row is a trial for a specific method and 'n_train' value\n",
    "df_diff = pd.DataFrame()\n",
    "\n",
    "df_fs_map = df[df.method == 'FS-MAP']\n",
    "df_ps_map = df[df.method == 'PS-MAP']\n",
    "\n",
    "# Make sure the trials align between FS-MAP and PS-MAP\n",
    "assert all(df_fs_map.n_train.values == df_ps_map.n_train.values), 'n_train values do not align between FS-MAP and PS-MAP'\n",
    "\n",
    "df_diff['n_train'] = df_fs_map.n_train\n",
    "df_diff['difference'] = (df_fs_map.log_fs_posterior.values - df_ps_map.log_fs_posterior.values)\n",
    "# Plot the differences using sns.lineplot\n",
    "plt.figure(figsize=(6,5), dpi=200)\n",
    "plot = sns.lineplot(data=df_diff, x='n_train', y='difference', ci='sd', errorbar='sd', marker='o', markersize=5)\n",
    "plot.set_xscale('log')\n",
    "plot.set_yscale('log')\n",
    "plot.yaxis.set_minor_formatter(NullFormatter())\n",
    "plot.set_yticks([1e3, 6e3])\n",
    "plt.xlabel('# Train samples')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/log_fs_posterior.pdf')\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/log_fs_posterior.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,5), dpi=200)\n",
    "sns.lineplot(x='n_train', y='hess_train_mse', data=df[df.method == 'PS-MAP'], linestyle='--', label='PS-MAP', color='C1', errorbar='sd', marker='o', markersize=5)\n",
    "plot = sns.lineplot(x='n_train', y='hess_train_mse', data=df[df.method == 'FS-MAP'], linestyle='--', label='FS-MAP', color='C0', errorbar='sd', marker='o', markersize=5)\n",
    "plt.xlabel('# Train samples')\n",
    "plt.ylabel('')\n",
    "plot.set_xscale('log')\n",
    "plot.set_yscale('log')\n",
    "plot.yaxis.set_minor_formatter(NullFormatter())\n",
    "plot.set_yticks([1e-3, 1e-4])\n",
    "plt.legend(loc='upper right', prop={'size': 24})\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/hess_train_mse.pdf')\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/hess_train_mse.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5), dpi=200)\n",
    "sns.lineplot(x='n_train', y='test_rmse', data=df[df.method == 'PS-MAP'], linestyle='-', label='PS-MAP', color='C1', errorbar='sd', marker='o', markersize=5)\n",
    "sns.lineplot(x='n_train', y='test_rmse', data=df[df.method == 'FS-MAP'], linestyle='-', label='FS-MAP', color='C0', errorbar='sd', marker='o', markersize=5)\n",
    "plt.xlabel('# Train samples')\n",
    "plt.ylabel('')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend([],[], frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/test_rmse.pdf')\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/test_rmse.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,5), dpi=200)\n",
    "sns.lineplot(x='n_train', y='train_rmse', data=df[df.method == 'PS-MAP'], linestyle='--', label='PS-MAP', color='C1', errorbar='sd', marker='o', markersize=5)\n",
    "plot = sns.lineplot(x='n_train', y='train_rmse', data=df[df.method == 'FS-MAP'], linestyle='--', label='FS-MAP', color='C0', errorbar='sd', marker='o', markersize=5)\n",
    "plt.xlabel('# Train samples')\n",
    "plt.ylabel('')\n",
    "plot.set_xscale('log')\n",
    "plot.set_yscale('log')\n",
    "plot.yaxis.set_minor_formatter(NullFormatter())\n",
    "plot.set_yticks([1e-1, 3e-2])\n",
    "# remove legend\n",
    "plt.legend([],[], frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/train_rmse.pdf')\n",
    "plt.savefig(f'nonlinear_mixture/plots/{feature_name}/train_rmse.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\", font_scale=3, rc={\"lines.linewidth\": 2})\n",
    "\n",
    "# Effect of eval points\n",
    "fsmap_results = []\n",
    "psmap_results = []\n",
    "noise_std = 0.1\n",
    "n_train = 400\n",
    "feature_names = ['fourier1d_discrete10', 'fourier1d_discrete50', 'fourier1d_discrete200', 'fourier1d'] #, 'fourier1d_narrow_eval'\n",
    "names = [10, 50, 200, 800]\n",
    "for f, n in zip(feature_names, names):\n",
    "    output_dir = f'nonlinear_mixture/weights/{f}'\n",
    "    for seed in range(3):\n",
    "        r = torch.load(f'{output_dir}/fsmap_seed{seed}_ntrain{n_train}_noise{noise_std}.pt')\n",
    "        r['name'] = n\n",
    "        fsmap_results.append(r)\n",
    "df_fsmap = pd.DataFrame(fsmap_results)\n",
    "\n",
    "output_dir = f'nonlinear_mixture/weights/fourier1d'\n",
    "for seed in range(3):\n",
    "    r = torch.load(f'{output_dir}/psmap_seed{seed}_ntrain{n_train}_noise{noise_std}.pt')\n",
    "    psmap_results.append(r)\n",
    "df_psmap = pd.DataFrame(psmap_results)\n",
    "\n",
    "plt.figure(figsize=(6.5, 5), dpi=200)\n",
    "sns.lineplot(x='name', y='test_rmse', data=df_fsmap, errorbar='sd', label='FS-MAP', marker='o', markersize=10)\n",
    "# plot a line at the PS-MAP value between 10 and 400\n",
    "plt.axhline(df_psmap.test_rmse.mean(), color='C1', linestyle='-', label='PS-MAP')\n",
    "# shade 1 std\n",
    "plt.fill_between(names, df_psmap.test_rmse.mean() - df_psmap.test_rmse.std(), df_psmap.test_rmse.mean() + df_psmap.test_rmse.std(), color='C1', alpha=0.2)\n",
    "plt.xlim(names[0], names[-1])\n",
    "plt.legend(prop={'size': 24})\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xticks(names, ['10', '50', '200', r'$\\infty$'])\n",
    "plt.ylabel('Test RMSE')\n",
    "plt.xlabel('# Eval Points')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'nonlinear_mixture/plots/fourier1d_num_points.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsmap_results = []\n",
    "feature_names = ['fourier1d', 'fourier1d_narrow_eval']\n",
    "names = [r'$\\mathrm{U}(-1,1)$', r'$\\mathrm{U}(-0.1,0.1)$']\n",
    "for f, n in zip(feature_names, names):\n",
    "    output_dir = f'nonlinear_mixture/weights/{f}'\n",
    "    for seed in range(3):\n",
    "        r = torch.load(f'{output_dir}/fsmap_seed{seed}_ntrain{n_train}_noise{noise_std}.pt')\n",
    "        r['name'] = n\n",
    "        fsmap_results.append(r)\n",
    "\n",
    "df = pd.DataFrame(fsmap_results)\n",
    "# set name to N/A for PS-MAP\n",
    "df_psmap['name'] = 'N/A'\n",
    "df = pd.concat([df, df_psmap])\n",
    "\n",
    "plt.figure(figsize=(6.5, 5), dpi=200)\n",
    "sns.barplot(x='name', y='test_rmse', data=df, capsize=0.1)\n",
    "plt.ylabel('Test RMSE')\n",
    "plt.xlabel(r'$p_X$')\n",
    "# make xticks small\n",
    "plt.xticks(fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'nonlinear_mixture/plots/fourier1d_pX.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misspecified likelihood\n",
    "fsmap_results = []\n",
    "psmap_results = []\n",
    "noise_std = 0.1\n",
    "n_train = 400\n",
    "feature_names = ['fourier1d', 'fourier1d_noise_mis3', 'fourier1d_noise_mis0.1']\n",
    "names = [1, 1/3, 10]\n",
    "for f,n in zip(feature_names, names):\n",
    "    output_dir = f'nonlinear_mixture/weights/{f}'\n",
    "    for seed in range(3):\n",
    "        r = torch.load(f'{output_dir}/fsmap_seed{seed}_ntrain{n_train}_noise{noise_std}.pt')\n",
    "        r['feat'] = f\n",
    "        r['name'] = n\n",
    "        fsmap_results.append(r)\n",
    "        r = torch.load(f'{output_dir}/psmap_seed{seed}_ntrain{n_train}_noise{noise_std}.pt')\n",
    "        r['feat'] = f\n",
    "        r['name'] = n\n",
    "        psmap_results.append(r)\n",
    "df_fsmap = pd.DataFrame(fsmap_results)\n",
    "df_psmap = pd.DataFrame(psmap_results)\n",
    "df_fsmap['method'] = 'FS-MAP'\n",
    "df_psmap['method'] = 'PS-MAP'\n",
    "df = pd.concat([df_fsmap, df_psmap])\n",
    "# df = df.sort_values(by=['n_train', 'method'])\n",
    "\n",
    "# barplot with feat on x-axis, hue = method, y = test_rmse\n",
    "plt.figure(figsize=(6.5, 5), dpi=200)\n",
    "sns.lineplot(x='name', y='test_rmse', data=df[df.method == 'PS-MAP'], linestyle='-', label='PS-MAP', color='C1', errorbar='sd', marker='o', markersize=10)\n",
    "sns.lineplot(x='name', y='test_rmse', data=df[df.method == 'FS-MAP'], linestyle='-', label='FS-MAP', color='C0', errorbar='sd', marker='o', markersize=10)\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Test RMSE')\n",
    "plt.xlabel(r'$\\sigma/\\sigma^*$')\n",
    "plt.xticks(names, ['1', r'$1/3$', '10'])\n",
    "# TODO: change to line plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'nonlinear_mixture/plots/fourier1d_noise_misspec.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misspecified likelihood\n",
    "fsmap_results = []\n",
    "psmap_results = []\n",
    "noise_std = 0.1\n",
    "n_train = 400\n",
    "feature_names = ['fourier1d_prior_mis10', 'fourier1d', 'fourier1d_prior_mis0.1']\n",
    "names = [0.1, 1, 10]\n",
    "for f,n in zip(feature_names, names):\n",
    "    output_dir = f'nonlinear_mixture/weights/{f}'\n",
    "    for seed in range(3):\n",
    "        r = torch.load(f'{output_dir}/fsmap_seed{seed}_ntrain{n_train}_noise{noise_std}.pt')\n",
    "        r['feat'] = f\n",
    "        r['name'] = n\n",
    "        fsmap_results.append(r)\n",
    "        r = torch.load(f'{output_dir}/psmap_seed{seed}_ntrain{n_train}_noise{noise_std}.pt')\n",
    "        r['feat'] = f\n",
    "        r['name'] = n\n",
    "        psmap_results.append(r)\n",
    "df_fsmap = pd.DataFrame(fsmap_results)\n",
    "df_psmap = pd.DataFrame(psmap_results)\n",
    "df_fsmap['method'] = 'FS-MAP'\n",
    "df_psmap['method'] = 'PS-MAP'\n",
    "df = pd.concat([df_fsmap, df_psmap])\n",
    "\n",
    "# barplot with feat on x-axis, hue = method, y = test_rmse\n",
    "plt.figure(figsize=(6.5, 5), dpi=200)\n",
    "sns.lineplot(x='name', y='test_rmse', data=df[df.method == 'PS-MAP'], linestyle='-', label='PS-MAP', color='C1', errorbar='sd', marker='o', markersize=10)\n",
    "sns.lineplot(x='name', y='test_rmse', data=df[df.method == 'FS-MAP'], linestyle='-', label='FS-MAP', color='C0', errorbar='sd', marker='o', markersize=10)\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Test RMSE')\n",
    "plt.xlabel(r'$\\alpha/\\alpha^*$')\n",
    "plt.xticks(names)\n",
    "# TODO: change to line plot\n",
    "plt.savefig(f'nonlinear_mixture/plots/fourier1d_prior_misspec.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misspecified prior\n",
    "fsmap_results = []\n",
    "psmap_results = []\n",
    "noise_std = 0.1\n",
    "n_train = 400\n",
    "feature_names = ['fourier1d_prior_mis10', 'fourier1d', 'fourier1d_prior_mis1e-5']\n",
    "for f in feature_names:\n",
    "    output_dir = f'nonlinear_mixture/weights/{f}'\n",
    "    for seed in range(3):\n",
    "        r = torch.load(f'{output_dir}/fsmap_seed{seed}_ntrain{n_train}_noise{noise_std}.pt')\n",
    "        r['feat'] = f\n",
    "        fsmap_results.append(r)\n",
    "        r = torch.load(f'{output_dir}/psmap_seed{seed}_ntrain{n_train}_noise{noise_std}.pt')\n",
    "        r['feat'] = f\n",
    "        psmap_results.append(r)\n",
    "df_fsmap = pd.DataFrame(fsmap_results)\n",
    "df_psmap = pd.DataFrame(psmap_results)\n",
    "df_fsmap['method'] = 'FS-MAP'\n",
    "df_psmap['method'] = 'PS-MAP'\n",
    "df = pd.concat([df_fsmap, df_psmap])\n",
    "# df = df.sort_values(by=['n_train', 'method'])\n",
    "\n",
    "# barplot with feat on x-axis, hue = method, y = test_rmse\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='feat', y='test_rmse', hue='method', data=df)\n",
    "# TODO: change to line plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fspace': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9436057e92285046d415c34e216bd357b01decd87fa7e06f42744a4b160880c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
