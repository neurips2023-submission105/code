{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "\n",
    "from bayesian_benchmarks.data import get_regression_data, get_classification_data\n",
    "import bayesian_benchmarks.data as bd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import blackjax\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import jaxopt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"pdf\", \"png\")\n",
    "plt.rcParams[\"savefig.dpi\"] = 75\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams[\"figure.figsize\"] = 6, 4\n",
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"lines.linewidth\"] = 2.0\n",
    "plt.rcParams[\"lines.markersize\"] = 8\n",
    "plt.rcParams[\"legend.fontsize\"] = 14\n",
    "plt.rcParams[\"grid.linestyle\"] = \"-\"\n",
    "plt.rcParams[\"grid.linewidth\"] = 1.0\n",
    "plt.rcParams[\"legend.facecolor\"] = \"white\"\n",
    "# plt.rcParams['grid.color'] = \"grey\"\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "# plt.rcParams['font.family'] = \"normal\"\n",
    "# plt.rcParams['font.family'] = \"sans-serif\"\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "plt.rcParams[\n",
    "    \"text.latex.preamble\"\n",
    "] = \"\\\\usepackage{subdepth} \\\\usepackage{amsfonts} \\\\usepackage{type1cm}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from typing import Callable\n",
    "\n",
    "def count_params(model, x):\n",
    "    params = model.init(jax.random.PRNGKey(0), x)\n",
    "    n = sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "    print(f'Number of parameters: {n/1e3} k')\n",
    "    \n",
    "def param_size(params):\n",
    "    return sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 64\n",
    "    hidden_layers: int = 1\n",
    "    act: Callable = nn.relu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        layers = [nn.Dense(self.H)]\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers += [self.act, nn.Dense(self.H)]\n",
    "        layers += [self.act, nn.Dense(self.out_size)]\n",
    "        return nn.Sequential(layers)(x)\n",
    "\n",
    "def reparam_initializer(initializer, f):\n",
    "    def init(key, shape, dtype=jnp.float32):\n",
    "        # sample original parameters and then invert the reparametrization\n",
    "        return f(initializer(key, shape, dtype))\n",
    "    return init\n",
    "\n",
    "class ReparamDense(nn.Module):\n",
    "    # same as nn.Dense but with reparam weights\n",
    "    # reparam: Callable = lambda x: x # w = reparam(x)\n",
    "    # reparam_inv: Callable = lambda x: x # x = reparam_inv(w)\n",
    "    # bias_init: Callable = reparam_initializer(nn.initializers.normal(stddev=1e-6))\n",
    "    # kernel_init: Callable = reparam_initializer(nn.initializers.lecun_normal())\n",
    "\n",
    "    def __init__(self, features, reparam, reparam_inv, init_scale=None):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.reparam = reparam\n",
    "        self.reparam_inv = reparam_inv\n",
    "        # zero init for bias\n",
    "        if init_scale is None:\n",
    "            self.bias_init = reparam_initializer(nn.initializers.normal(stddev=1e-4), f=reparam_inv)\n",
    "            self.kernel_init = reparam_initializer(nn.initializers.lecun_normal(), f=reparam_inv)\n",
    "        else:\n",
    "            self.bias_init = reparam_initializer(nn.initializers.normal(stddev=init_scale), f=reparam_inv)\n",
    "            self.kernel_init = reparam_initializer(nn.initializers.normal(stddev=init_scale), f=reparam_inv)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        reparam_kernel = self.param('reparam_kernel', self.kernel_init, (inputs.shape[-1], self.features))\n",
    "        reparam_bias = self.param('reparam_bias', self.bias_init, (1, self.features)) # not using bias_init to avoid dividing by zero\n",
    "        # invert weights\n",
    "        kernel = jax.tree_util.tree_map(self.reparam, reparam_kernel)\n",
    "        bias = jax.tree_util.tree_map(self.reparam, reparam_bias)\n",
    "        # clamp to avoid numerical issues\n",
    "        kernel = jnp.clip(kernel, a_min=-1e6, a_max=1e6)\n",
    "        bias = jnp.clip(bias, a_min=-1e6, a_max=1e6)\n",
    "        return jnp.dot(inputs, kernel) + bias\n",
    "\n",
    "class ReparamMLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 64\n",
    "    hidden_layers: int = 1\n",
    "    reparam: Callable = lambda x: x # w = reparam(x)\n",
    "    reparam_inv: Callable = lambda x: x # x = reparam_inv(w)\n",
    "    act: Callable = nn.tanh\n",
    "    init_scale: float = None\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        layers = [ReparamDense(features=self.H, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers += [self.act, ReparamDense(features=self.H, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        layers += [self.act, ReparamDense(features=self.out_size, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        out = nn.Sequential(layers)(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    d_model: int\n",
    "    num_heads: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.W_q = nn.Dense(self.d_model)\n",
    "        self.W_k = nn.Dense(self.d_model)\n",
    "        self.W_v = nn.Dense(self.d_model)\n",
    "        self.W_o = nn.Dense(self.d_model)\n",
    "\n",
    "    def attention(self, q, k, v, mask=None, dropout=None):\n",
    "        # q, k: # (batch_size, num_heads, seq_len, d_model // num_heads)\n",
    "        d_k = q.shape[-1]\n",
    "        scores = jnp.einsum('...nd,...ed->...ne', q, k) / jnp.sqrt(d_k) # (batch_size, num_heads, seq_len, seq_len)\n",
    "        if mask is not None:\n",
    "            scores = scores - 1e9 * mask\n",
    "        weights = nn.softmax(scores, axis=-1) # (batch_size, num_heads, seq_len, seq_len)\n",
    "        output = jnp.matmul(weights, v) # (batch_size, num_heads, seq_len, d_model // num_heads)\n",
    "        return output\n",
    "\n",
    "    def __call__(self, x, mask=None, dropout=None):\n",
    "        batch_size = x.shape[0]\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "        q = q.reshape(batch_size, -1, self.num_heads, q.shape[-1] // self.num_heads).transpose(0, 2, 1, 3) # (batch_size, num_heads, seq_len, d_model // num_heads)\n",
    "        k = k.reshape(batch_size, -1, self.num_heads, k.shape[-1] // self.num_heads).transpose(0, 2, 1, 3)\n",
    "        v = v.reshape(batch_size, -1, self.num_heads, v.shape[-1] // self.num_heads).transpose(0, 2, 1, 3)\n",
    "        if mask is not None:\n",
    "            mask = mask[:, jnp.newaxis, :, :]\n",
    "        output = self.attention(q, k, v, mask, dropout) # (batch_size, num_heads, seq_len, d_model // num_heads)\n",
    "        output = output.transpose(0, 2, 1, 3).reshape(batch_size, -1, self.d_model)\n",
    "        return self.W_o(output)\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    d_model: int\n",
    "    d_ff: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.fc1 = nn.Dense(self.d_ff)\n",
    "        self.fc2 = nn.Dense(self.d_model)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.fc2(nn.relu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    d_model: int\n",
    "    num_heads: int\n",
    "    d_ff: int\n",
    "    dropout_rate: float\n",
    "\n",
    "    def setup(self):\n",
    "        self.mha = MultiHeadAttention(self.d_model, self.num_heads)\n",
    "        self.ffn = PositionWiseFeedForward(self.d_model, self.d_ff)\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        x = self.norm1(x + self.mha(x, mask))\n",
    "        return self.norm2(x + self.ffn(x))\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    num_layers: int\n",
    "    d_model: int\n",
    "    num_heads: int\n",
    "    d_ff: int\n",
    "    dropout_rate: float\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [EncoderLayer(self.d_model, self.num_heads, self.d_ff, self.dropout_rate)\n",
    "                       for _ in range(self.num_layers)]\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    d_model: int\n",
    "    dropout_rate: float\n",
    "\n",
    "    def __call__(self, x):\n",
    "        seq_length = x.shape[1]\n",
    "        position = jnp.arange(seq_length)[:, jnp.newaxis]\n",
    "        div_term = jnp.exp(jnp.arange(0, self.d_model, 2) * -(jnp.log(10000.0) / self.d_model))\n",
    "        pos_enc = jnp.zeros((1, seq_length, self.d_model))\n",
    "        pos_enc[:, :, 0::2] = jnp.sin(position * div_term)\n",
    "        pos_enc[:, :, 1::2] = jnp.cos(position * div_term)\n",
    "        return x + pos_enc\n",
    "    \n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    d_model: int\n",
    "    max_len: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.pos_embedding = self.param('pos_embedding', nn.initializers.normal(), (1, self.max_len, self.d_model))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        seq_length = x.shape[1]\n",
    "        assert seq_length <= self.max_len, f\"Input sequence length ({seq_length}) exceeds max_len ({self.max_len}).\"\n",
    "        return x + self.pos_embedding[:, :seq_length, :]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    out_size: int\n",
    "    d_model: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    d_ff: int\n",
    "    max_len: int\n",
    "    dropout_rate: float = 0\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Dense(self.d_model)\n",
    "        self.pos_enc = LearnedPositionalEncoding(self.d_model, self.max_len)\n",
    "        self.encoder = TransformerEncoder(self.num_layers, self.d_model, self.num_heads, self.d_ff, self.dropout_rate)\n",
    "        self.output_layer = nn.Dense(self.out_size)\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        # we assume tabular data with shape (batch_size, feature_dim)\n",
    "        # treat each feature as a token num_tokens == feature_dim\n",
    "        # reshape to (batch_size, num_tokens, 1)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.encoder(x, mask)\n",
    "        x = self.output_layer(x) # (batch_size, num_tokens, out_size)\n",
    "        x = jnp.mean(x, axis=1) # (batch_size, out_size)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_norm(tree):\n",
    "    return jnp.sqrt(sum([jnp.sum(x**2) for x in jax.tree_leaves(tree)]))\n",
    "\n",
    "def random_split_like_tree(rng_key, target=None, treedef=None):\n",
    "    if treedef is None:\n",
    "        treedef = jax.tree_structure(target)\n",
    "    keys = jax.random.split(rng_key, treedef.num_leaves)\n",
    "    return jax.tree_unflatten(treedef, keys)\n",
    "\n",
    "@jax.jit\n",
    "def tree_random_normal_like(rng_key, target):\n",
    "    keys_tree = random_split_like_tree(rng_key, target)\n",
    "    return jax.tree_map(\n",
    "        lambda l, k: jax.random.normal(k, l.shape, l.dtype),\n",
    "        target,\n",
    "        keys_tree,\n",
    "    )\n",
    "\n",
    "def jacobian_sigular_values(model, p, x):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x) # (b, o, p)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('bo...->...bo', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    P = param_size(p)\n",
    "    J = J.reshape(P, -1).T # (B*O, P)\n",
    "    # sigular values of J\n",
    "    _, S, _ = jnp.linalg.svd(J, full_matrices=False)\n",
    "    return S\n",
    "\n",
    "def get_K_matrix(model, p, x):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('b...->...b', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    J = J.reshape(-1, x.shape[0]).T # (N, P)\n",
    "    N = x.shape[0]\n",
    "    K = J.T @ J / N # (P, P)\n",
    "    return K\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,3,4))\n",
    "def log_det_K_svd(model, p, x, jitter=1e-6, return_eig=False):\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    logdet_svd = jnp.sum(jnp.log(s ** 2 + jitter))\n",
    "    P = param_size(p)\n",
    "    logdet_svd = logdet_svd - P * jnp.log(jitter)\n",
    "    if return_eig:\n",
    "        return logdet_svd, s ** 2\n",
    "    return logdet_svd\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,3,4))\n",
    "def log_det_K_svd_first_order(model, p, x, jitter=1e-6, return_eig=False):\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    logdet_svd = jnp.sum(jnp.log(jitter) + (s ** 2) / jitter)\n",
    "    P = param_size(p)\n",
    "    logdet_svd = logdet_svd - P * jnp.log(jitter)\n",
    "    if return_eig:\n",
    "        return logdet_svd, s ** 2\n",
    "    return logdet_svd\n",
    "\n",
    "def log_det_K(model, p, x, jitter=1e-6, scale=1.0):\n",
    "    K = get_K_matrix(model, p, x)\n",
    "    # add jitter\n",
    "    K = K + jitter * jnp.eye(K.shape[0])\n",
    "    s, log_det = jnp.linalg.slogdet(K)\n",
    "    return log_det\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    logdet_svd = 2 * jnp.sum(jnp.log(s))\n",
    "    return logdet_svd\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,4))\n",
    "def trace_estimator(model, p, x, dp, sigma=0.1):\n",
    "    # dp: N(0, I)\n",
    "    # p_perturbed = p + dp, both pytrees\n",
    "    # scale dp by sigma\n",
    "    dp = jax.tree_util.tree_map(lambda x: x * sigma, dp)\n",
    "    p_perturbed = jax.tree_util.tree_map(lambda x, y: x + y, p, dp)\n",
    "    # compute the difference between the outputs\n",
    "    y = model.apply(p, x)\n",
    "    y_perturbed = model.apply(p_perturbed, x)\n",
    "    diff = y - y_perturbed\n",
    "    # compute the avg sq norm of the difference\n",
    "    avg_dff_sq_norm = jnp.mean((diff ** 2).sum(axis=-1))\n",
    "    return avg_dff_sq_norm / (sigma ** 2)\n",
    "\n",
    "# @partial(jax.jit, static_argnums=(0,3,4,5,6))\n",
    "def log_det_trace_estimator(model, p, x, jitter, dp, sigma=0.01, n_samples=1, normalize=False):\n",
    "    # sample n_samples dp from N(0, I) and compute the trace estimator\n",
    "    traces = []\n",
    "    P = param_size(p)\n",
    "    if normalize:\n",
    "        scale = (P ** 0.5) / tree_norm(dp)\n",
    "    else:\n",
    "        scale = 1.0\n",
    "    # scale dp to have squared norm P * sigma^2\n",
    "    dp = jax.tree_util.tree_map(lambda x: x * scale, dp)\n",
    "    trace = trace_estimator(model, p, x, dp, sigma=sigma)\n",
    "    return trace / jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(lr, prior_scale, n_step, rng_key, loss_fn, val_loss_fn, model, x_train, y_train, x_eval_generator, optimizer, fsmap, jitter, method='exact', temp=1.0):\n",
    "    rng_key, init_params_key = jax.random.split(rng_key)\n",
    "    init_params = jax.jit(model.init)(init_params_key, jnp.ones((1, x_train.shape[1])))\n",
    "    def get_train_state(optimizer, lr, init_params, warmup_steps=100):\n",
    "        lr_sched = optax.linear_schedule(0, lr, warmup_steps, transition_begin=0)\n",
    "        if optimizer == 'adam':\n",
    "            tx = optax.chain(\n",
    "                    optax.adam(learning_rate=lr_sched),\n",
    "                )\n",
    "            \n",
    "        elif optimizer == 'sgd':\n",
    "            tx = optax.sgd(learning_rate=lr_sched, momentum=0.9)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return train_state.TrainState.create(apply_fn=model.apply, params=init_params, tx=tx)\n",
    "    \n",
    "    ts = get_train_state(optimizer, lr, init_params)\n",
    "    \n",
    "    def augmented_loss_fn(p, x_eval, dp):\n",
    "        # loss = likelihood / N = 1 / (2 * sigma^2) * ||y - f(x)||^2 / N\n",
    "        # it contains a factor 1 / N, where N = x_train.shape[0]\n",
    "        # all other terms should be divided by N as well\n",
    "        avg_nll = loss_fn(p) \n",
    "        N = x_train.shape[0]\n",
    "        # number of params\n",
    "        P = jax.tree_util.tree_leaves(p)[0].shape[0]\n",
    "        if fsmap:\n",
    "            if method == 'diag':\n",
    "                logdet = 1 / 2 * log_det_diagonal_approx(model, p, x_eval, jitter) / N\n",
    "            elif method == 'exact':\n",
    "                # svd is much more stable\n",
    "                logdet = 1 / 2 * log_det_K_svd(model, p, x_eval, jitter) / N\n",
    "            elif method == 'laplacian':\n",
    "                logdet = 1 / 2 * log_det_trace_estimator(model, p, x_eval, jitter, dp) / N\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            logdet = 0\n",
    "        params_flat, unravel = jax.flatten_util.ravel_pytree(p)\n",
    "        log_p_w = 1 / (2 * (prior_scale ** 2)) * jnp.sum(params_flat ** 2) / N\n",
    "        logdet =  temp * logdet + (1 - temp) * jax.lax.stop_gradient(logdet)\n",
    "        val_nll = val_loss_fn(p)\n",
    "        return avg_nll + log_p_w + logdet, (avg_nll, val_nll, logdet)\n",
    "\n",
    "    grad_fn = jax.jit(jax.value_and_grad(lambda p, x_eval, dp: augmented_loss_fn(p, x_eval, dp), has_aux=True))\n",
    "    losses = []\n",
    "    logdets = []\n",
    "    avg_nlls = []\n",
    "    val_nlls = []\n",
    "    def sample_dp(p, rng_key):\n",
    "        # gaussian tree\n",
    "        dp = tree_random_normal_like(rng_key, p)\n",
    "        # # norm of dp flattened\n",
    "        # dp_flat, _ = jax.flatten_util.ravel_pytree(dp)\n",
    "        # dp_norm = jnp.sqrt(jnp.sum(dp_flat ** 2))\n",
    "        # # normalize dp\n",
    "        # dp = jax.tree_util.tree_map(lambda x: x / dp_norm, dp)\n",
    "        return dp\n",
    "    @jax.jit\n",
    "    def train_step(ts, rng_key):\n",
    "        rng_key, x_eval_key = jax.random.split(rng_key)\n",
    "        rng_key, dp_key = jax.random.split(rng_key)\n",
    "        x_eval = x_eval_generator(x_eval_key)\n",
    "        dp = sample_dp(ts.params, dp_key)\n",
    "        (loss, aux), grads = grad_fn(ts.params, x_eval, dp)\n",
    "        ts = ts.apply_gradients(grads=grads)\n",
    "        return ts, loss, aux, rng_key\n",
    "    for _ in tqdm(range(int(n_step))):\n",
    "        ts, loss, aux, rng_key = train_step(ts, rng_key)\n",
    "        avg_nll, val_nll, logdet = aux\n",
    "        losses.append(loss.item())\n",
    "        logdets.append(logdet.item())\n",
    "        avg_nlls.append(avg_nll.item())\n",
    "        val_nlls.append(val_nll.item())\n",
    "    losses = np.array(losses)\n",
    "    logdets = np.array(logdets)\n",
    "    avg_nlls = np.array(avg_nlls)\n",
    "    val_nlls = np.array(val_nlls)\n",
    "    return ts.params, losses, avg_nlls, val_nlls, logdets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x_train, x_test):\n",
    "    assert x_train.ndim == 2 and x_test.ndim == 2 and x_train.shape[1] == x_test.shape[1], 'x_train and x_test should have the same number of features'\n",
    "    x_train = (x_train - x_train.mean(0)) / (x_train.std(0) + 1e-8)\n",
    "    x_test = (x_test - x_train.mean(0)) / (x_train.std(0) + 1e-8)\n",
    "    return x_train, x_test\n",
    "\n",
    "def standardize_ds(x_train, y_train, x_test, y_test, is_classification):\n",
    "    # X: (N, D)\n",
    "    # Y: (N, Dy)\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    x_train, x_test = standardize(x_train, x_test)\n",
    "    if is_classification:\n",
    "        y_train = y_train.astype(np.int32)\n",
    "        y_test = y_test.astype(np.int32)\n",
    "    else:\n",
    "        y_train = y_train.astype(np.float32)\n",
    "        y_test = y_test.astype(np.float32)\n",
    "        y_train, y_test = standardize(y_train, y_test)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def to_one_hot(y_train, y_val, y_test):\n",
    "    # y_train: (N,1)\n",
    "    # y_test: (N,1)\n",
    "    assert y_train.ndim == 2 and y_test.ndim == 2 and y_train.shape[1] == y_test.shape[1] == 1, 'y_train and y_test should have shape (N,1)'\n",
    "    n_classes = y_train.max() + 1\n",
    "    y_train = jax.nn.one_hot(y_train.reshape(-1), n_classes)\n",
    "    y_val = jax.nn.one_hot(y_val.reshape(-1), n_classes)\n",
    "    y_test = jax.nn.one_hot(y_test.reshape(-1), n_classes)\n",
    "    return y_train, y_val, y_test\n",
    "\n",
    "def run_dataset(dataset, arch, prior_scale, n_step, lr, optimizer, output_dir, seed, fsmap=False, method='exact', jitter=1e-6, temp=1, result_path=None):\n",
    "    is_classification = dataset in bd.classification_datasets\n",
    "    if is_classification:\n",
    "        print(f'Running {dataset} classification dataset')\n",
    "    else:\n",
    "        print(f'Running {dataset} regression dataset')\n",
    "    # get data\n",
    "    ds = get_regression_data(dataset) if not is_classification else get_classification_data(dataset)\n",
    "    x_train, y_train, x_test, y_test = ds.X_train, ds.Y_train, ds.X_test, ds.Y_test\n",
    "    x_train, y_train, x_test, y_test = standardize_ds(x_train, y_train, x_test, y_test, is_classification)\n",
    "    # 20% of training data as validation\n",
    "    n_val = int(x_train.shape[0] * 0.2)\n",
    "    # shuffle x_train and y_train\n",
    "    indices = np.arange(x_train.shape[0])\n",
    "    # fix seed\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    x_train = x_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "    x_train, x_val, y_train, y_val = x_train[n_val:], x_train[:n_val], y_train[n_val:], y_train[:n_val]\n",
    "    if is_classification:\n",
    "        y_train, y_val, y_test = to_one_hot(y_train, y_val, y_test)\n",
    "    print(f\"X train: {x_train.shape}, X val: {x_val.shape}, X test: {x_test.shape}\")\n",
    "    print(f\"Y train: {y_train.shape}, Y val: {y_val.shape}, Y test: {y_test.shape}\")\n",
    "    # model\n",
    "    model = arch(out_size=y_train.shape[1])\n",
    "    def x_eval_generator(rng_key):\n",
    "        # sample 200 points from normal distribution\n",
    "        return jax.random.normal(rng_key, (1000, x_train.shape[1]))\n",
    "    # count parameters\n",
    "    init_params = model.init(jax.random.PRNGKey(0), jnp.ones((1, x_train.shape[1])))\n",
    "    leaves, _ = jax.tree_util.tree_flatten(init_params)\n",
    "    n_params = sum([np.prod(p.shape) for p in leaves])\n",
    "    print(f\"Number of parameters: {n_params}\")\n",
    "\n",
    "    # softmax multi-task cross entropy loss (not binary)\n",
    "    if is_classification:\n",
    "        loss_fn = lambda p, x, y: jnp.mean(jnp.sum(-jax.nn.log_softmax(model.apply(p, x), axis=1) * y, axis=1))\n",
    "    else:\n",
    "        loss_fn = lambda p, x, y: jnp.mean((model.apply(p, x) - y)**2)\n",
    "    y_model = model.apply(init_params, x_test)\n",
    "    assert y_model.shape == y_test.shape, f'model output shape: {y_model.shape}, y_test shape: {y_test.shape}'\n",
    "    train_loss_fn = lambda p: loss_fn(p, x_train, y_train)\n",
    "    test_loss_fn = lambda p: loss_fn(p, x_test, y_test)\n",
    "    val_loss_fn = lambda p: loss_fn(p, x_val, y_val)\n",
    "    \n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    # log training time\n",
    "    start_time = time.time()\n",
    "    params, losses, avg_nlls, val_nlls, logdets = optimize(lr, prior_scale, n_step, rng_key, train_loss_fn, val_loss_fn, model, x_train, y_train, x_eval_generator, optimizer, fsmap, jitter, method, temp)\n",
    "    end_time = time.time()\n",
    "    # train time in seconds\n",
    "    train_time = end_time - start_time\n",
    "    # plot and save losses\n",
    "    plt.figure()\n",
    "    \n",
    "    def make_subplots(arrays, labels):\n",
    "        n_plots = len(arrays)\n",
    "        fig, axs = plt.subplots(n_plots, 1, figsize=(10, 10))\n",
    "        for i, (a, label) in enumerate(zip(arrays, labels)):\n",
    "            q1, q2 = np.quantile(a, [0., 0.95])\n",
    "            axs[i].plot(a)\n",
    "            axs[i].set_ylim(q1, q2)\n",
    "            axs[i].set_xlabel('Step')\n",
    "            axs[i].set_ylabel(label)\n",
    "            axs[i].legend()\n",
    "        plt.show()\n",
    "    \n",
    "    make_subplots([losses, avg_nlls, val_nlls, logdets], ['Loss', 'Train NLL', 'Val NLL', 'Logdet'])\n",
    "    \n",
    "    def log_posterior(model, params, x):\n",
    "        log_param_prior = -0.5 * jnp.sum(jnp.array([jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(params)])) / (prior_scale ** 2)\n",
    "        # log_det, eigs = log_det_K_svd(model, params, x, jitter=1e-32, return_eig=True)\n",
    "        # laplacian = log_det_K_svd_first_order(model, params, x, jitter=1)\n",
    "        measurements = {\n",
    "            'log_param_prior': log_param_prior.item(),\n",
    "            # 'log_det': log_det.item(),\n",
    "            # 'laplacian': laplacian.item(),\n",
    "            'train_time': train_time,\n",
    "            'jitter': jitter,\n",
    "            'train_loss': train_loss_fn(params).item(),\n",
    "            'test_loss': test_loss_fn(params).item(),\n",
    "            'val_loss': val_loss_fn(params).item(),\n",
    "            'dataset': dataset,\n",
    "        }\n",
    "        print(measurements)\n",
    "        # return log_likelihood + log_param_prior - 1 / 2 * log_det\n",
    "        return measurements\n",
    "    \n",
    "    # compute function space posterior \n",
    "    measurements = log_posterior(model, params, x_test)\n",
    "    torch.save(measurements, result_path)\n",
    "\n",
    "    return measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'tanh': nn.tanh,\n",
    "    'elu': nn.elu,\n",
    "    'cos': jnp.cos,\n",
    "    'sin': jnp.sin,\n",
    "    'softplus': nn.softplus,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSMAP\n",
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [128]\n",
    "depths = [2]\n",
    "acts = ['elu']\n",
    "\n",
    "\n",
    "optimizer = 'adam'\n",
    "lr = 1e-3\n",
    "n_step = int(1e4)\n",
    "prior_scales = [100]\n",
    "\n",
    "\n",
    "skip_if_done = True\n",
    "fsmap = False\n",
    "temps = [1]\n",
    "method = 'exact'\n",
    "jitter = 1e-32\n",
    "\n",
    "datasets = ['boston', 'concrete', 'energy', 'power', 'protein', 'winered']\n",
    "\n",
    "pmap_results = []\n",
    "for dataset in datasets:\n",
    "    for width in widths:\n",
    "        for depth in depths:\n",
    "            for act in acts:\n",
    "                for prior_scale in prior_scales:\n",
    "                    for temp in temps:\n",
    "                        output_dir = 'uci_psmap'\n",
    "                        # mkdir if needed\n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "                        for seed in range(1):\n",
    "                            arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                            result_path = f'{output_dir}/{dataset}_mlp_{act}_l{depth}h{width}j{jitter}s{prior_scale}seed{seed}.pt'\n",
    "                            if os.path.exists(result_path) and skip_if_done:\n",
    "                                pmap_results.append(torch.load(result_path))\n",
    "                                print('Loaded result from ', result_path)\n",
    "                            else:\n",
    "                                gt_params = None\n",
    "                                pmap_results.append(run_dataset(dataset, arch, prior_scale, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=temp, jitter=jitter, result_path=result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [128]\n",
    "depths = [2]\n",
    "acts = ['elu']\n",
    "\n",
    "\n",
    "optimizer = 'adam'\n",
    "lr = 1e-3\n",
    "n_step = int(1e4)\n",
    "prior_scales = [100]\n",
    "\n",
    "\n",
    "skip_if_done = True\n",
    "fsmap = True\n",
    "temps = [1]\n",
    "method = 'laplacian'\n",
    "\n",
    "datasets = ['boston', 'concrete', 'energy', 'power', 'protein', 'winered']\n",
    "jitters = np.logspace(-1, 1, 3)\n",
    "\n",
    "laplace_results = []\n",
    "for dataset in datasets:\n",
    "    for jitter in jitters:\n",
    "        for width in widths:\n",
    "            for depth in depths:\n",
    "                for act in acts:\n",
    "                    for prior_scale in prior_scales:\n",
    "                        for temp in temps:\n",
    "                            output_dir = 'uci_laplacian'\n",
    "                            # mkdir if needed\n",
    "                            if not os.path.exists(output_dir):\n",
    "                                os.makedirs(output_dir)\n",
    "                            for seed in range(1):\n",
    "                                arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                                result_path = f'{output_dir}/{dataset}_mlp_{act}_l{depth}h{width}j{jitter}s{prior_scale}seed{seed}.pt'\n",
    "                                if os.path.exists(result_path) and skip_if_done:\n",
    "                                    laplace_results.append(torch.load(result_path))\n",
    "                                    print('Loaded result from ', result_path)\n",
    "                                else:\n",
    "                                    gt_params = None\n",
    "                                    laplace_results.append(run_dataset(dataset, arch, prior_scale, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=temp, jitter=jitter, result_path=result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-MAP transformer\n",
    "\n",
    "\n",
    "widths = [64]\n",
    "depths = [4]\n",
    "heads = 4\n",
    "\n",
    "optimizer = 'adam'\n",
    "lr = 1e-3\n",
    "n_step = int(1e3)\n",
    "prior_scales = [100]\n",
    "\n",
    "\n",
    "skip_if_done = False\n",
    "fsmap = True\n",
    "temps = [1]\n",
    "method = 'laplacian'\n",
    "jitter = 1e-32\n",
    "\n",
    "datasets = ['boston', 'concrete', 'energy', 'power', 'protein', 'winered']\n",
    "jitters = np.logspace(-2, 4, 7)\n",
    "\n",
    "pmap_results = []\n",
    "for dataset in datasets:\n",
    "    for jitter in jitters:\n",
    "        for width in widths:\n",
    "            for depth in depths:\n",
    "                for act in acts:\n",
    "                    for prior_scale in prior_scales:\n",
    "                        for temp in temps:\n",
    "                            output_dir = 'uci_laplacian'\n",
    "                            # mkdir if needed\n",
    "                            if not os.path.exists(output_dir):\n",
    "                                os.makedirs(output_dir)\n",
    "                            for seed in range(1):\n",
    "                                arch = partial(Transformer, d_model=width, num_heads=heads, num_layers=depth, max_len=100, d_ff=width, dropout_rate=0)\n",
    "                                result_path = f'{output_dir}/{dataset}_transformer_{act}_l{depth}h{width}j{jitter}s{prior_scale}seed{seed}.pt'\n",
    "                                if os.path.exists(result_path) and skip_if_done:\n",
    "                                    pmap_results.append(torch.load(result_path))\n",
    "                                    print('Loaded result from ', result_path)\n",
    "                                else:\n",
    "                                    gt_params = None\n",
    "                                    pmap_results.append(run_dataset(dataset, arch, prior_scale, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=temp, jitter=jitter, result_path=result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert laplace results to pandas dataframe\n",
    "import pandas as pd\n",
    "df_laplace = pd.DataFrame(laplace_results)\n",
    "df_psmap = pd.DataFrame(pmap_results)\n",
    "df_laplace['method'] = 'L-MAP'\n",
    "df_psmap['method'] = 'PS-MAP'\n",
    "\n",
    "# require jitter to be > 1 for L-MAP\n",
    "# df_laplace = df_laplace[df_laplace['jitter'] > 1]\n",
    "\n",
    "# take RMSE\n",
    "df_psmap['test_loss'] = df_psmap['test_loss'] ** 0.5\n",
    "df_laplace['test_loss'] = df_laplace['test_loss'] ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", font_scale=2.0, rc={\"lines.linewidth\": 3.0})\n",
    "sns.set_palette(\"Set1\")\n",
    "# for each dataset\n",
    "plt.figure(figsize=(10, 6), dpi=200)\n",
    "# ratios = []\n",
    "psmap_test_losses = []\n",
    "worst_laplace_test_losses = []\n",
    "for dataset in datasets:\n",
    "    psmap_test_loss = float(df_psmap['test_loss'][df_psmap['dataset'] == dataset])\n",
    "    laplace_test_loss = np.array(df_laplace['test_loss'][df_laplace['dataset'] == dataset])\n",
    "    worst_laplace_test_losses.append(np.max(laplace_test_loss))\n",
    "    psmap_test_losses.append(psmap_test_loss)\n",
    "    jitters = np.array(df_laplace['jitter'][df_laplace['dataset'] == dataset])\n",
    "    # ratios.append((laplace_test_loss/psmap_test_loss).item())\n",
    "    plt.plot(jitters, laplace_test_loss/psmap_test_loss, label=dataset, marker='o', linewidth=5.0)\n",
    "\n",
    "# a line at 1\n",
    "min_ = np.min(jitters)\n",
    "max_ = np.max(jitters)\n",
    "plt.plot([min_, max_], [1, 1], 'k--')\n",
    "# max y at 2\n",
    "plt.ylim(0.5, 1.5)\n",
    "plt.xlabel(r'$\\epsilon$')\n",
    "plt.ylabel('L-MAP / PS-MAP Test RMSE')\n",
    "plt.xscale('log')\n",
    "# legend outside\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.savefig('abbi/uci_scan.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of best test loss \n",
    "plt.figure(figsize=(10, 6), dpi=200)\n",
    "df_best = pd.DataFrame({'dataset': datasets, 'PS-MAP': psmap_test_losses, 'L-MAP worst': worst_laplace_test_losses})\n",
    "df_best = df_best.melt('dataset', var_name='method', value_name='test_loss')\n",
    "sns.barplot(x='dataset', y='test_loss', hue='method', data=df_best)\n",
    "plt.ylabel('Test RMSE')\n",
    "plt.savefig('abbi/uci_worst.pdf', bbox_inches='tight')\n",
    "# plt.yscale('log')\n",
    "# plot ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", font_scale=2.0, rc={\"lines.linewidth\": 3.0})\n",
    "sns.set_palette(\"Set1\")\n",
    "# for each dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "for dataset in datasets:\n",
    "    psmap_train_loss = float(df_psmap['train_loss'][df_psmap['dataset'] == dataset])\n",
    "    laplace_train_loss = np.array(df_laplace['train_loss'][df_laplace['dataset'] == dataset])\n",
    "    jitters = np.array(df_laplace['jitter'][df_laplace['dataset'] == dataset])\n",
    "    plt.plot(jitters, laplace_train_loss/psmap_train_loss, label=dataset, marker='o')\n",
    "# a line at 1\n",
    "plt.plot(np.logspace(-1, 1, 100), np.ones(100), linestyle='--', color='black')\n",
    "# max y at 2\n",
    "plt.ylim(0, 2)\n",
    "plt.xlabel(r'$\\epsilon$')\n",
    "plt.ylabel('train Loss / PS-MAP train Loss')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [16]\n",
    "depths = [2]\n",
    "acts = ['tanh']\n",
    "\n",
    "\n",
    "optimizer = 'adam'\n",
    "lr = 1e-2\n",
    "n_step = int(1e4)\n",
    "prior_scales = [1]\n",
    "noise_scale = 0\n",
    "# n_eval = 400\n",
    "# x_lims = [-5, 5]\n",
    "\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = x_test\n",
    "    return X_eval\n",
    "\n",
    "skip_if_done = True\n",
    "fsmap = True\n",
    "temps = [1]\n",
    "method = 'exact'\n",
    "# jitter = 1e-32\n",
    "\n",
    "exact_results = []\n",
    "for jitter in [1e-32] + [j for j in np.logspace(-6, 3, 10)]:\n",
    "    for width in widths:\n",
    "        for depth in depths:\n",
    "            for act in acts:\n",
    "                for prior_scale in prior_scales:\n",
    "                    for temp in temps:\n",
    "                        output_dir = f'pathology'\n",
    "                        # mkdir if needed\n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "                        for seed in range(1):\n",
    "                            arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                            result_path = f'{output_dir}/result_{seed}.pt'\n",
    "                            if os.path.exists(result_path) and skip_if_done:\n",
    "                                exact_results.append(torch.load(result_path))\n",
    "                                print('Loaded result from ', result_path)\n",
    "                            else:\n",
    "                                gt_params = None\n",
    "                                exact_results.append(run_two_moons(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=temp, jitter=jitter, result_path=result_path))\n",
    "                                plt.show()\n",
    "                                plt.savefig(f'two_moons_exact/{act}_l{depth}h{width}j{jitter}_pred.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(style=\"whitegrid\", font_scale=2.0, rc={\"lines.linewidth\": 3.0})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "def subtract_min(x):\n",
    "    return x\n",
    "    # return x - np.min(x)\n",
    "\n",
    "def normalize(v):\n",
    "    v_norm2 = jnp.sum(jnp.array([jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(v)]))\n",
    "    v = jax.tree_util.tree_map(lambda x: x / jnp.sqrt(v_norm2), v)    \n",
    "    return v\n",
    "\n",
    "def log_posterior(model, params, x_train, y_train, x_context, jitter, log_det_fn):\n",
    "    y_train_one_hot = jax.nn.one_hot(y_train.reshape(-1), 2)\n",
    "    log_likelihood = jnp.sum(jnp.sum(jax.nn.log_softmax(model.apply(params, x_train), axis=1) * y_train_one_hot, axis=1))\n",
    "    log_param_prior = -0.5 * jnp.sum(jnp.array([jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(params)])) / (prior_scale ** 2)\n",
    "    log_det = log_det_fn(model, params, x_context, jitter)\n",
    "    # print(f'log_likelihood: {log_likelihood}, log_param_prior: {log_param_prior}, log_det: {log_det}')\n",
    "    return log_likelihood, log_param_prior, log_det\n",
    "\n",
    "def scan_posterior(model, x_context, jitter, steps, max_alpha=1, log_det_fn=log_det_K_svd):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    init_params = model.init(rng_key, x_train)\n",
    "    # split key\n",
    "    rng_key, rng_key_sample_1, rng_key_sample_2, rng_key_sample_3 = jax.random.split(rng_key, 4)\n",
    "    # gaussian random param\n",
    "    v1 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_1, x.shape), init_params))\n",
    "    v2 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_2, x.shape), init_params))\n",
    "    v3 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_3, x.shape), init_params))\n",
    "    # plot log posterior on the ray spanned by \n",
    "    alpha = jnp.concatenate([-jnp.logspace(jnp.log10(max_alpha), -4, steps), jnp.logspace(-4, jnp.log10(max_alpha), steps)])\n",
    "    alpha = alpha[alpha != 0]\n",
    "\n",
    "    log_likelihoods, log_param_priors, log_dets = [], [], []\n",
    "    log_posterior_fn = jax.jit(lambda p: log_posterior(model, p, x_train, y_train, x_context, jitter, log_det_fn))\n",
    "    for a in tqdm(alpha):\n",
    "        for v in [v1, v2, v3]:\n",
    "            params = jax.tree_util.tree_map(lambda x: x * a, v)\n",
    "            log_likelihood, log_param_prior, log_det = log_posterior_fn(params)\n",
    "            log_likelihoods.append(log_likelihood)\n",
    "            log_param_priors.append(log_param_prior)\n",
    "            log_dets.append(log_det)\n",
    "\n",
    "    log_likelihoods = jnp.array(log_likelihoods).reshape(-1, 3)\n",
    "    log_param_priors = jnp.array(log_param_priors).reshape(-1, 3)\n",
    "    log_dets = jnp.array(log_dets).reshape(-1, 3)\n",
    "    return alpha, log_likelihoods, log_param_priors, log_dets\n",
    "\n",
    "def plot_scan(alpha, log_likelihoods, log_param_priors, log_dets):\n",
    "    # plot three quantities, as dots not lines\n",
    "    plt.figure(dpi=100, figsize=(10, 6))\n",
    "    for i in range(3):\n",
    "        plt.plot(alpha, -log_likelihoods[:, i], label=r'NLL', color=f'C{i}', linestyle='--')\n",
    "        plt.plot(alpha, subtract_min(-log_param_priors[:, i]), label=r'$-\\Delta\\log p_w$', color=f'C{i}', linestyle=':')\n",
    "        plt.plot(alpha[alpha<0], 1/2 * subtract_min(log_dets[alpha<0, i]), label=rf'$\\Delta1/2\\log\\det$', color=f'C{i}', linestyle='-')\n",
    "        plt.plot(alpha[alpha>0], 1/2 * subtract_min(log_dets[alpha>0, i]), color=f'C{i}', linestyle='-')\n",
    "    # put legend outside\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('divergence.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_posterior_2d(model, x_context, jitter, steps, max_alpha=1, log_det_fn=log_det_K_svd):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    init_params = model.init(rng_key, x_train)\n",
    "    # split key\n",
    "    rng_key, rng_key_sample_1, rng_key_sample_2, rng_key_sample_3 = jax.random.split(rng_key, 4)\n",
    "    # gaussian random param\n",
    "    v1 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_1, x.shape), init_params))\n",
    "    v2 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_2, x.shape), init_params))\n",
    "    # plot log posterior on the ray spanned by v1 and v2\n",
    "    alpha = jnp.concatenate([-jnp.logspace(jnp.log10(max_alpha), -4, steps), jnp.logspace(-4, jnp.log10(max_alpha), steps)])\n",
    "    alpha = alpha[alpha != 0]\n",
    "    # 2d coordinates\n",
    "    u, v = jnp.meshgrid(alpha, alpha)\n",
    "    log_likelihoods, log_param_priors, log_dets = [], [], []\n",
    "    log_posterior_fn = jax.jit(lambda p: log_posterior(model, p, x_train, y_train, x_context, jitter, log_det_fn))\n",
    "    for a, b in tqdm(zip(u.flatten(), v.flatten())):\n",
    "        av1 = jax.tree_util.tree_map(lambda x: x * a, v1)\n",
    "        bv2 = jax.tree_util.tree_map(lambda x: x * b, v2)\n",
    "        params = jax.tree_util.tree_map(lambda x, y: x + y, av1, bv2)\n",
    "        log_likelihood, log_param_prior, log_det = log_posterior_fn(params)\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "        log_param_priors.append(log_param_prior)\n",
    "        log_dets.append(log_det)\n",
    "    log_likelihoods = jnp.array(log_likelihoods)\n",
    "    log_param_priors = jnp.array(log_param_priors)\n",
    "    log_dets = jnp.array(log_dets)\n",
    "    return u, v, log_likelihoods, log_param_priors, log_dets\n",
    "    \n",
    "def plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets):\n",
    "    # three subplots\n",
    "    fig, axs = plt.subplots(1, 3, dpi=100, figsize=(30, 10))\n",
    "    # plot delta log det\n",
    "    axs[0].contourf(u, v, subtract_min(log_dets).reshape(u.shape), levels=20)\n",
    "    axs[0].set_title(r'$\\Delta\\log\\det$')\n",
    "    axs[0].set_xlabel(r'$\\alpha$')\n",
    "    axs[0].set_ylabel(r'$\\beta$')\n",
    "    # make image square\n",
    "    axs[0].set_aspect('equal')\n",
    "    # colorbar\n",
    "    cbar = fig.colorbar(axs[0].contourf(u, v, subtract_min(log_dets).reshape(u.shape), levels=20), ax=axs[0])\n",
    "    # plot NLL\n",
    "    axs[1].contourf(u, v, -log_likelihoods.reshape(u.shape), levels=20)\n",
    "    axs[1].set_title(r'NLL')\n",
    "    axs[1].set_xlabel(r'$\\alpha$')\n",
    "    axs[1].set_ylabel(r'$\\beta$')\n",
    "    axs[1].set_aspect('equal')\n",
    "    # colorbar\n",
    "    cbar = fig.colorbar(axs[1].contourf(u, v, -log_likelihoods.reshape(u.shape), levels=20), ax=axs[1])\n",
    "    # plot delta -log p_w\n",
    "    axs[2].contourf(u, v, subtract_min(-log_param_priors).reshape(u.shape), levels=20)\n",
    "    axs[2].set_title(r'$-\\Delta\\log p_w$')\n",
    "    axs[2].set_xlabel(r'$\\alpha$')\n",
    "    axs[2].set_ylabel(r'$\\beta$')\n",
    "    axs[2].set_aspect('equal')\n",
    "    # colorbar\n",
    "    cbar = fig.colorbar(axs[2].contourf(u, v, subtract_min(-log_param_priors).reshape(u.shape), levels=20), ax=axs[2])\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets):\n",
    "    # same but plot in 3d\n",
    "    fig = plt.figure(figsize=(30, 10))\n",
    "    ax = fig.add_subplot(131, projection='3d')\n",
    "    ax.plot_surface(u, v, subtract_min(log_dets).reshape(u.shape), cmap='viridis', edgecolor='none')\n",
    "    ax.set_title(r'$\\Delta\\log\\det$')\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel(r'$\\beta$')\n",
    "    ax.set_zlabel(r'$\\Delta\\log\\det$')\n",
    "    ax = fig.add_subplot(132, projection='3d')\n",
    "    ax.plot_surface(u, v, -log_likelihoods.reshape(u.shape), cmap='viridis', edgecolor='none')\n",
    "    ax.set_title(r'NLL')\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel(r'$\\beta$')\n",
    "    ax.set_zlabel(r'NLL')\n",
    "    ax = fig.add_subplot(133, projection='3d')\n",
    "    ax.plot_surface(u, v, subtract_min(-log_param_priors).reshape(u.shape), cmap='viridis', edgecolor='none')\n",
    "    ax.set_title(r'$-\\Delta\\log p_w$')\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel(r'$\\beta$')\n",
    "    ax.set_zlabel(r'$-\\Delta\\log p_w$')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'tanh'\n",
    "prior_scale = 1\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "count_params(model, x_train)\n",
    "x_context = x_test\n",
    "\n",
    "for jitter in [1000]:\n",
    "    alpha, log_likelihoods, log_param_priors, log_dets = scan_posterior(model, x_context, jitter, steps=10, max_alpha=1)\n",
    "    plot_scan(alpha, 0*log_likelihoods, 0*log_param_priors, log_dets)\n",
    "    plt.show()\n",
    "    # plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}.png')\n",
    "    # plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'tanh'\n",
    "prior_scale = 1\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "count_params(model, x_train)\n",
    "x_context = x_test\n",
    "\n",
    "log_det_fn = lambda model, p, x, jitter: log_det_trace_estimator(model, p, x, jitter, sigma=0.01, n_samples=10)\n",
    "for jitter in [1000]:\n",
    "    alpha, log_likelihoods, log_param_priors, log_dets = scan_posterior(model, x_context, jitter, steps=10, max_alpha=1, log_det_fn=log_det_fn)\n",
    "    plot_scan(alpha, 0*log_likelihoods, 0*log_param_priors, log_dets)\n",
    "    plt.show()\n",
    "    # plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}.png')\n",
    "    # plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'tanh'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "for jitter in [1e-1]:\n",
    "    u, v, log_likelihoods, log_param_priors, log_dets = scan_posterior_2d(model, x_context, jitter, steps=30, max_alpha=100)\n",
    "    torch.save((u, v, log_likelihoods, log_param_priors, log_dets), f'pathology/{act}_l{depth}h{width}j{jitter}.pt')\n",
    "    plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_contour.png')\n",
    "    plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_surface.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'tanh'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "log_det_fn = lambda model, p, x, jitter: log_det_trace_estimator(model, p, x, jitter, sigma=0.1, n_samples=10)\n",
    "for jitter in [1e-1]:\n",
    "    u, v, log_likelihoods, log_param_priors, log_dets = scan_posterior_2d(model, x_context, jitter, steps=30, max_alpha=100, log_det_fn=log_det_fn)\n",
    "    torch.save((u, v, log_likelihoods, log_param_priors, log_dets), f'pathology/{act}_l{depth}h{width}j{jitter}.pt')\n",
    "    plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_contour_tr.png')\n",
    "    plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_surface_tr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 4\n",
    "act = 'elu'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "for jitter in [1e-32, 1e-6, 1e-3]:\n",
    "    u, v, log_likelihoods, log_param_priors, log_dets = scan_posterior_2d(model, x_context, jitter, steps=30, max_alpha=100)\n",
    "    torch.save((u, v, log_likelihoods, log_param_priors, log_dets), f'pathology/{act}_l{depth}h{width}j{jitter}.pt')\n",
    "    plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_contour.png')\n",
    "    plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_surface.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'elu'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "jitter = 1e-32\n",
    "u, v, log_likelihoods, log_param_priors, log_dets = scan_posterior_2d(model, x_context, jitter, steps=30, max_alpha=1000)\n",
    "torch.save((u, v, log_likelihoods, log_param_priors, log_dets), f'pathology/{act}_l{depth}h{width}j{jitter}_wide.pt')\n",
    "plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_contour_wide.png')\n",
    "plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_surface_wide.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 4\n",
    "act = 'elu'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "jitter = 1e-32\n",
    "u, v, log_likelihoods, log_param_priors, log_dets = scan_posterior_2d(model, x_context, jitter, steps=30, max_alpha=1000)\n",
    "torch.save((u, v, log_likelihoods, log_param_priors, log_dets), f'pathology/{act}_l{depth}h{width}j{jitter}_wide.pt')\n",
    "plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_contour_wide.png')\n",
    "plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_surface_wide.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing objectives ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'tanh'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "init_params = model.init(rng_key, x_train)\n",
    "# log_det_K_svd_fn = jax.jit(lambda params, x_context, jitter: log_det_K_svd(model, params, x_context, jitter, return_eig=True), static_argnums=(3,))\n",
    "# log_det_trace_estimator_fn = jax.jit(lambda params, x_context, jitter, sigma, n_samples: log_det_trace_estimator(model, params, x_context, jitter, sigma, n_samples), static_argnums=(3,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = 50\n",
    "x_context = x_test\n",
    "jitter = 1000\n",
    "# trace estimator\n",
    "n_samples = 100\n",
    "sigma = 0.001 # noise scale\n",
    "\n",
    "logdet_svds = []\n",
    "logdet_traces = []\n",
    "max_eigs = []\n",
    "for _ in tqdm(range(points)):\n",
    "    rng_key, rng_key_sample = jax.random.split(rng_key)\n",
    "    params = model.init(rng_key_sample, x_train[0][None, :])\n",
    "    logdet_svd, eig = log_det_K_svd(model, params, x_context, jitter, return_eig=True)\n",
    "    logdet_svds.append(logdet_svd)\n",
    "    max_eigs.append(eig.max().item())\n",
    "    logdet_traces.append(log_det_trace_estimator(model, params, x_context, jitter, sigma, n_samples).item())\n",
    "\n",
    "logdet_traces = np.array(logdet_traces)\n",
    "logdet_svds = np.array(logdet_svds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = (logdet_traces / logdet_svds)\n",
    "_ = plt.hist(ratio, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(style=\"whitegrid\", font_scale=2.0, rc={\"lines.linewidth\": 3.0})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "# a nice square plot to compare logdet_svds, logdet_traces\n",
    "# color the points by min_eig and plot a colorbar\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(logdet_svds, logdet_traces, c=max_eigs, cmap='viridis')\n",
    "\n",
    "plt.colorbar()\n",
    "# # a diagonal line spanning the range of the data\n",
    "min_logdet = min(min(logdet_svds), min(logdet_traces))\n",
    "max_logdet = max(max(logdet_svds), max(logdet_traces))\n",
    "plt.plot([0, max_logdet], [0, max_logdet], 'k--')\n",
    "\n",
    "# label the axes\n",
    "plt.xlabel('logdet svd')\n",
    "plt.ylabel('logdet trace')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fspace': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9436057e92285046d415c34e216bd357b01decd87fa7e06f42744a4b160880c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
