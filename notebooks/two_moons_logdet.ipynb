{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# os.environ['XLA_PYTHON_CLIENT_ALLOCATOR']= 'platform'\n",
    "# os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"pdf\", \"png\")\n",
    "plt.rcParams[\"savefig.dpi\"] = 75\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams[\"figure.figsize\"] = 6, 4\n",
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"lines.linewidth\"] = 2.0\n",
    "plt.rcParams[\"lines.markersize\"] = 8\n",
    "plt.rcParams[\"legend.fontsize\"] = 14\n",
    "plt.rcParams[\"grid.linestyle\"] = \"-\"\n",
    "plt.rcParams[\"grid.linewidth\"] = 1.0\n",
    "plt.rcParams[\"legend.facecolor\"] = \"white\"\n",
    "# plt.rcParams['grid.color'] = \"grey\"\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "# plt.rcParams['font.family'] = \"normal\"\n",
    "# plt.rcParams['font.family'] = \"sans-serif\"\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "plt.rcParams[\n",
    "    \"text.latex.preamble\"\n",
    "] = \"\\\\usepackage{subdepth} \\\\usepackage{amsfonts} \\\\usepackage{type1cm}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import blackjax\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import jaxopt\n",
    "# sns.set(font_scale=2, style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "seed = 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "### LOAD DATA\n",
    "def _one_hot(x, k):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k))\n",
    "\n",
    "_x_train, _y_train = make_moons(\n",
    "    n_samples=200, shuffle=True, noise=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "h = 0.25\n",
    "test_lim = 3\n",
    "# x_min, x_max = x_train[:, 0].min() - test_lim, x_train[:, 0].max() + test_lim\n",
    "# y_min, y_max = x_train[:, 1].min() - test_lim, x_train[:, 1].max() + test_lim\n",
    "x_min, x_max = _x_train[:, 0].min() - test_lim, _x_train[:, 0].max() + test_lim\n",
    "y_min, y_max = _x_train[:, 1].min() - test_lim, _x_train[:, 1].max() + test_lim\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "_x_test = np.vstack((xx.reshape(-1), yy.reshape(-1))).T\n",
    "\n",
    "h = 0.25\n",
    "test_lim = 7\n",
    "x_wide_min, x_wide_max = (\n",
    "    _x_train[:, 0].min() - test_lim,\n",
    "    _x_train[:, 0].max() + test_lim,\n",
    ")\n",
    "y_wide_min, y_wide_max = (\n",
    "    _x_train[:, 1].min() - test_lim,\n",
    "    _x_train[:, 1].max() + test_lim,\n",
    ")\n",
    "xx_wide, yy_wide = np.meshgrid(\n",
    "    np.arange(x_wide_min, x_wide_max, h), np.arange(y_wide_min, y_wide_max, h)\n",
    ")\n",
    "_x_test_wide = np.vstack((xx_wide.reshape(-1), yy_wide.reshape(-1))).T\n",
    "\n",
    "_y_test = np.ones(_x_test.shape[0])\n",
    "_y_test_wide = np.ones(_x_test_wide.shape[0])\n",
    "\n",
    "\n",
    "x_train = _x_train\n",
    "y_train = _y_train.reshape(-1, 1)\n",
    "\n",
    "x_test = _x_test\n",
    "y_test = _y_test.reshape(-1, 1)\n",
    "\n",
    "x_test_wide = _x_test_wide\n",
    "y_test_wide = _y_test_wide.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from typing import Callable\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 64\n",
    "    hidden_layers: int = 1\n",
    "    act: Callable = nn.relu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        layers = [nn.Dense(self.H)]\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers += [self.act, nn.Dense(self.H)]\n",
    "        layers += [self.act, nn.Dense(self.out_size)]\n",
    "        return nn.Sequential(layers)(x)\n",
    "\n",
    "def reparam_initializer(initializer, f):\n",
    "    def init(key, shape, dtype=jnp.float32):\n",
    "        # sample original parameters and then invert the reparametrization\n",
    "        return f(initializer(key, shape, dtype))\n",
    "    return init\n",
    "\n",
    "class ReparamDense(nn.Module):\n",
    "    # same as nn.Dense but with reparam weights\n",
    "    # reparam: Callable = lambda x: x # w = reparam(x)\n",
    "    # reparam_inv: Callable = lambda x: x # x = reparam_inv(w)\n",
    "    # bias_init: Callable = reparam_initializer(nn.initializers.normal(stddev=1e-6))\n",
    "    # kernel_init: Callable = reparam_initializer(nn.initializers.lecun_normal())\n",
    "\n",
    "    def __init__(self, features, reparam, reparam_inv, init_scale=None):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.reparam = reparam\n",
    "        self.reparam_inv = reparam_inv\n",
    "        # zero init for bias\n",
    "        if init_scale is None:\n",
    "            self.bias_init = reparam_initializer(nn.initializers.normal(stddev=1e-4), f=reparam_inv)\n",
    "            self.kernel_init = reparam_initializer(nn.initializers.lecun_normal(), f=reparam_inv)\n",
    "        else:\n",
    "            self.bias_init = reparam_initializer(nn.initializers.normal(stddev=init_scale), f=reparam_inv)\n",
    "            self.kernel_init = reparam_initializer(nn.initializers.normal(stddev=init_scale), f=reparam_inv)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        reparam_kernel = self.param('reparam_kernel', self.kernel_init, (inputs.shape[-1], self.features))\n",
    "        reparam_bias = self.param('reparam_bias', self.bias_init, (1, self.features)) # not using bias_init to avoid dividing by zero\n",
    "        # invert weights\n",
    "        kernel = jax.tree_util.tree_map(self.reparam, reparam_kernel)\n",
    "        bias = jax.tree_util.tree_map(self.reparam, reparam_bias)\n",
    "        # clamp to avoid numerical issues\n",
    "        kernel = jnp.clip(kernel, a_min=-1e6, a_max=1e6)\n",
    "        bias = jnp.clip(bias, a_min=-1e6, a_max=1e6)\n",
    "        return jnp.dot(inputs, kernel) + bias\n",
    "\n",
    "class ReparamMLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 64\n",
    "    hidden_layers: int = 1\n",
    "    reparam: Callable = lambda x: x # w = reparam(x)\n",
    "    reparam_inv: Callable = lambda x: x # x = reparam_inv(w)\n",
    "    act: Callable = nn.tanh\n",
    "    init_scale: float = None\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        layers = [ReparamDense(features=self.H, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers += [self.act, ReparamDense(features=self.H, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        layers += [self.act, ReparamDense(features=self.out_size, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        out = nn.Sequential(layers)(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_sigular_values(model, p, x):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('b...->...b', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    J = J.reshape(-1, x.shape[0]).T # (N, P)\n",
    "    # sigular values of J\n",
    "    _, S, _ = jnp.linalg.svd(J, full_matrices=False)\n",
    "    return S\n",
    "\n",
    "def get_K_matrix(model, p, x):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('b...->...b', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    J = J.reshape(-1, x.shape[0]).T # (N, P)\n",
    "    N = x.shape[0]\n",
    "    K = J.T @ J / N # (P, P)\n",
    "    return K\n",
    "\n",
    "def log_det_K_svd(model, p, x, jitter=1e-6, scale=1.0):\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    logdet_svd = jnp.sum(jnp.log(s ** 2 + jitter))\n",
    "    return logdet_svd\n",
    "\n",
    "def log_det_K(model, p, x, jitter=1e-6, scale=1.0):\n",
    "    K = get_K_matrix(model, p, x)\n",
    "    # add jitter\n",
    "    K = K + jitter * jnp.eye(K.shape[0])\n",
    "    s, log_det = jnp.linalg.slogdet(K)\n",
    "    return log_det\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    logdet_svd = 2 * jnp.sum(jnp.log(s))\n",
    "    return logdet_svd\n",
    "\n",
    "\n",
    "def log_det_diagonal_approx(model, p, x, jitter=1e-6):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('b...->...b', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    J = J.reshape(-1, x.shape[0]).T # (N, P)\n",
    "    avg_j_sq = jnp.mean(J ** 2, axis=0) # (P,)\n",
    "    logdet_diag = jnp.sum(jnp.log(avg_j_sq + jitter))\n",
    "    return logdet_diag\n",
    "\n",
    "def function_instability(model, p, x, dp):\n",
    "    # p_perturbed = p + dp, both pytrees\n",
    "    p_perturbed = jax.tree_util.tree_map(lambda x, y: x + y, p, dp)\n",
    "    # compute the difference between the outputs\n",
    "    y = model.apply(p, x)\n",
    "    y_perturbed = model.apply(p_perturbed, x)\n",
    "    diff = y - y_perturbed\n",
    "    # compute the norm of the difference\n",
    "    avg_dff_sq = jnp.mean(diff ** 2)\n",
    "    return avg_dff_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(lr, prior_scale, n_step, rng_key, loss_fn, model, x_train, y_train, x_eval_generator, optimizer, fsmap, jitter, method='exact', temp=1.0):\n",
    "    rng_key, init_params_key = jax.random.split(rng_key)\n",
    "    init_params = jax.jit(model.init)(init_params_key, jnp.ones((1, x_train.shape[1])))\n",
    "    def get_train_state(optimizer, lr, init_params, warmup_steps=100):\n",
    "        lr_sched = optax.linear_schedule(0, lr, warmup_steps, transition_begin=0)\n",
    "        if optimizer == 'adam':\n",
    "            tx = optax.chain(\n",
    "                    optax.adam(learning_rate=lr_sched),\n",
    "                )\n",
    "            \n",
    "        elif optimizer == 'sgd':\n",
    "            tx = optax.sgd(learning_rate=lr_sched, momentum=0.9)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return train_state.TrainState.create(apply_fn=model.apply, params=init_params, tx=tx)\n",
    "    \n",
    "    ts = get_train_state(optimizer, lr, init_params)\n",
    "    \n",
    "    def augmented_loss_fn(p, x_eval, dp):\n",
    "        # loss = likelihood / N = 1 / (2 * sigma^2) * ||y - f(x)||^2 / N\n",
    "        # it contains a factor 1 / N, where N = x_train.shape[0]\n",
    "        # all other terms should be divided by N as well\n",
    "        avg_nll = loss_fn(p) \n",
    "        N = x_train.shape[0]\n",
    "        # number of params\n",
    "        P = jax.tree_util.tree_leaves(p)[0].shape[0]\n",
    "        if fsmap:\n",
    "            if method == 'diag':\n",
    "                logdet = 1 / 2 * log_det_diagonal_approx(model, p, x_eval, jitter) / N\n",
    "            elif method == 'exact':\n",
    "                # svd is much more stable\n",
    "                logdet = 1 / 2 * log_det_K_svd(model, p, x_eval, jitter) / N\n",
    "            elif method == 'instability':\n",
    "                logdet = P / 2 * function_instability(model, p, x_eval, dp) / N\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            logdet = 0\n",
    "        params_flat, unravel = jax.flatten_util.ravel_pytree(p)\n",
    "        log_p_w = 1 / (2 * (prior_scale ** 2)) * jnp.sum(params_flat ** 2) / N\n",
    "        logdet =  temp * logdet + (1 - temp) * jax.lax.stop_gradient(logdet)\n",
    "        return avg_nll + log_p_w + logdet, (avg_nll, logdet)\n",
    "\n",
    "    grad_fn = jax.jit(jax.value_and_grad(lambda p, x_eval, dp: augmented_loss_fn(p, x_eval, dp), has_aux=True))\n",
    "    losses = []\n",
    "    logdets = []\n",
    "    avg_nlls = []\n",
    "    def sample_dp(p, rng_key):\n",
    "        # gaussian tree\n",
    "        dp = tree_random_normal_like(rng_key, p)\n",
    "        # norm of dp flattened\n",
    "        dp_flat, _ = jax.flatten_util.ravel_pytree(dp)\n",
    "        dp_norm = jnp.sqrt(jnp.sum(dp_flat ** 2))\n",
    "        # normalize dp\n",
    "        dp = jax.tree_util.tree_map(lambda x: x / dp_norm, dp)\n",
    "        return dp\n",
    "    @jax.jit\n",
    "    def train_step(ts, rng_key):\n",
    "        rng_key, x_eval_key = jax.random.split(rng_key)\n",
    "        rng_key, dp_key = jax.random.split(rng_key)\n",
    "        x_eval = x_eval_generator(x_eval_key)\n",
    "        dp = sample_dp(ts.params, dp_key)\n",
    "        (loss, aux), grads = grad_fn(ts.params, x_eval, dp)\n",
    "        ts = ts.apply_gradients(grads=grads)\n",
    "        return ts, loss, aux, rng_key\n",
    "    for _ in tqdm(range(int(n_step))):\n",
    "        ts, loss, aux, rng_key = train_step(ts, rng_key)\n",
    "        avg_nll, logdet = aux\n",
    "        losses.append(loss.item())\n",
    "        logdets.append(logdet.item())\n",
    "        avg_nlls.append(avg_nll.item())\n",
    "    losses = np.array(losses)\n",
    "    logdets = np.array(logdets)\n",
    "    avg_nlls = np.array(avg_nlls)\n",
    "    return ts.params, losses, avg_nlls, logdets\n",
    "\n",
    "def random_split_like_tree(rng_key, target=None, treedef=None):\n",
    "    if treedef is None:\n",
    "        treedef = jax.tree_structure(target)\n",
    "    keys = jax.random.split(rng_key, treedef.num_leaves)\n",
    "    return jax.tree_unflatten(treedef, keys)\n",
    "\n",
    "\n",
    "def tree_random_normal_like(rng_key, target):\n",
    "    keys_tree = random_split_like_tree(rng_key, target)\n",
    "    return jax.tree_map(\n",
    "        lambda l, k: jax.random.normal(k, l.shape, l.dtype),\n",
    "        target,\n",
    "        keys_tree,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, task='regression', fsmap=False, method='exact', jitter=1e-6, temp=1):\n",
    "    # model\n",
    "    model = arch(out_size=1)\n",
    "    x_eval_sample = x_eval_generator(jax.random.PRNGKey(0))\n",
    "    # count parameters\n",
    "    init_params = model.init(jax.random.PRNGKey(0), jnp.ones((1, x_train.shape[1])))\n",
    "    leaves, _ = jax.tree_util.tree_flatten(init_params)\n",
    "    n_params = sum([np.prod(p.shape) for p in leaves])\n",
    "    print(f\"Number of parameters: {n_params}\")\n",
    "\n",
    "\n",
    "    if task == 'regression':\n",
    "        train_loss_fn = lambda p: 0.5 * jnp.mean((model.apply(p, x_train) - y_train) ** 2) / noise_scale ** 2\n",
    "        test_loss_fn = lambda p: jnp.mean((model.apply(p, x_test) - y_test) ** 2)\n",
    "    elif task == 'classification':\n",
    "        train_loss_fn = lambda p: jnp.mean(jax.nn.sigmoid(model.apply(p, x_train)) * (1 - y_train) + (1 - jax.nn.sigmoid(model.apply(p, x_train))) * y_train)\n",
    "        test_loss_fn = lambda p: jnp.mean(jnp.round(jax.nn.sigmoid(model.apply(p, x_test))) != y_test)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    params, losses, avg_nlls, logdets = optimize(lr, prior_scale, n_step, rng_key, train_loss_fn, model, x_train, y_train, x_eval_generator, optimizer, fsmap, jitter, method, temp)\n",
    "    torch.save(params, f'{output_dir}/params_{seed}.pt')\n",
    "    print('Saved ps parameters at ', f'{output_dir}/params_{seed}.pt')\n",
    "    # plot and save losses\n",
    "    plt.figure()\n",
    "    \n",
    "    def make_subplots(arrays, labels):\n",
    "        n_plots = len(arrays)\n",
    "        fig, axs = plt.subplots(n_plots, 1, figsize=(10, 10))\n",
    "        for i, (a, label) in enumerate(zip(arrays, labels)):\n",
    "            q1, q2 = np.quantile(a, [0., 0.95])\n",
    "            axs[i].plot(a)\n",
    "            axs[i].set_ylim(q1, q2)\n",
    "            axs[i].set_xlabel('Step')\n",
    "            axs[i].set_ylabel(label)\n",
    "            axs[i].legend()\n",
    "        plt.show()\n",
    "    \n",
    "    make_subplots([losses, avg_nlls, logdets], ['Loss', 'Avg NLL', 'Logdet'])\n",
    "    \n",
    "    def log_posterior(model, params, x_eval_sample, jitter):\n",
    "        log_likelihood = -0.5 * jnp.sum((model.apply(params, x_train) - y_train) ** 2) / noise_scale ** 2\n",
    "        log_param_prior = -0.5 * jnp.sum(jnp.array([jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(params)])) / (prior_scale ** 2)\n",
    "        log_det = log_det_K_svd(model, params, x_eval_sample, jitter)\n",
    "        return log_likelihood + log_param_prior - 1 / 2 * log_det\n",
    "    \n",
    "    # compute function space posterior \n",
    "    log_posterior_learned = log_posterior(model, params, x_eval_sample, jitter)\n",
    "    if gt_params is not None:\n",
    "        log_posterior_gt = log_posterior(model, gt_params, x_eval_sample, jitter)\n",
    "    else:\n",
    "        log_posterior_gt = None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6), dpi=120)\n",
    "    prediction = model.apply(params, x_test)\n",
    "    ax.plot(x_test[..., 0], prediction, c='green', label=rf'Prediction $\\log p(f) = {log_posterior_learned}$' if log_posterior_learned is not None else 'Prediction')\n",
    "    # plot X_eval on the x-axis\n",
    "    ax.scatter(x_eval_sample[..., 0], np.zeros(x_eval_sample.shape[0]), c='black', label='Eval Points Sample', linestyle='None', s=3)\n",
    "    ax.set(xlabel='$x$', ylabel='$y$', ylim=[y_test.min() - 0.5, y_test.max() + 0.5], xlim=[x_test.min(), x_test.max()])\n",
    "    ax.scatter(x_train[..., 0], y_train, c='r', label='Train Data', s=10)\n",
    "    ax.plot(x_test[..., 0], y_test, c='b', label=rf'Truth $\\log p(f^*) = {log_posterior_gt}$' if log_posterior_gt is not None else 'Truth')\n",
    "    ax.grid(True)\n",
    "\n",
    "    # show noise_scale, prior_scale on plot\n",
    "    ax.text(0.05, 0.95, rf'$\\alpha = {prior_scale}$, $\\sigma = {noise_scale}$, $\\tau = {temp}$', transform=ax.transAxes, fontsize=16, verticalalignment='top')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "    train_loss = jnp.mean((model.apply(params, x_train) - y_train) ** 2)\n",
    "    test_loss = jnp.mean((model.apply(params, x_test) - y_test) ** 2)\n",
    "    result = {'train_loss': train_loss, 'test_loss': test_loss, 'n_params': n_params, 'prior_scale': prior_scale}\n",
    "    if task == 'regression':\n",
    "        result['test_loss'] = jnp.mean((model.apply(params, x_test) - y_test) ** 2)\n",
    "    elif task == 'classification':\n",
    "        result['test_loss'] = jnp.mean(jax.nn.sigmoid(model.apply(params, x_test)) * (1 - y_test) + (1 - jax.nn.sigmoid(model.apply(params, x_test))) * y_test)\n",
    "        result['test_error'] = jnp.mean(jnp.round(jax.nn.sigmoid(model.apply(params, x_test))) != y_test)\n",
    "    # Hessian of training loss\n",
    "    params_flat, unravel = jax.flatten_util.ravel_pytree(params)\n",
    "    model_apply_flat = make_flat_function(model.apply, unravel)\n",
    "    if task == 'regression':\n",
    "        loss_fn_flat = lambda p: jnp.mean((model_apply_flat(p, x_train) - y_train) ** 2)\n",
    "    elif task == 'classification':\n",
    "        loss_fn_flat = lambda p: jnp.mean(jax.nn.sigmoid(model_apply_flat(p, x_train)) * (1 - y_train) + (1 - jax.nn.sigmoid(model_apply_flat(p, x_train))) * y_train)\n",
    "    result['p_norm'] = jnp.linalg.norm(params_flat)\n",
    "    result['log_posterior_learned'] = log_posterior_learned\n",
    "    torch.save(result, f'{output_dir}/result_{seed}.pt')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_two_moons(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, fsmap=False, method='exact', jitter=1e-6, temp=1):\n",
    "    # model\n",
    "    model = arch(out_size=2)\n",
    "    x_eval_sample = x_eval_generator(jax.random.PRNGKey(0))\n",
    "    # count parameters\n",
    "    init_params = model.init(jax.random.PRNGKey(0), jnp.ones((1, x_train.shape[1])))\n",
    "    leaves, _ = jax.tree_util.tree_flatten(init_params)\n",
    "    n_params = sum([np.prod(p.shape) for p in leaves])\n",
    "    print(f\"Number of parameters: {n_params}\")\n",
    "\n",
    "    # softmax multi-task cross entropy loss (not binary)\n",
    "    y_train_one_hot = jax.nn.one_hot(y_train.reshape(-1), 2)\n",
    "    train_loss_fn = lambda p: jnp.mean(jnp.sum(-jax.nn.log_softmax(model.apply(p, x_train), axis=1) * y_train_one_hot, axis=1))\n",
    "    \n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    params, losses, avg_nlls, logdets = optimize(lr, prior_scale, n_step, rng_key, train_loss_fn, model, x_train, y_train, x_eval_generator, optimizer, fsmap, jitter, method, temp)\n",
    "    torch.save(params, f'{output_dir}/params_{seed}.pt')\n",
    "    print('Saved ps parameters at ', f'{output_dir}/params_{seed}.pt')\n",
    "    # plot and save losses\n",
    "    plt.figure()\n",
    "    \n",
    "    def make_subplots(arrays, labels):\n",
    "        n_plots = len(arrays)\n",
    "        fig, axs = plt.subplots(n_plots, 1, figsize=(10, 10))\n",
    "        for i, (a, label) in enumerate(zip(arrays, labels)):\n",
    "            q1, q2 = np.quantile(a, [0., 0.95])\n",
    "            axs[i].plot(a)\n",
    "            axs[i].set_ylim(q1, q2)\n",
    "            axs[i].set_xlabel('Step')\n",
    "            axs[i].set_ylabel(label)\n",
    "            axs[i].legend()\n",
    "        plt.show()\n",
    "    \n",
    "    make_subplots([losses, avg_nlls, logdets], ['Loss', 'Avg NLL', 'Logdet'])\n",
    "    \n",
    "    def log_posterior(model, params, x, jitter):\n",
    "        log_likelihood = jnp.sum(jnp.sum(jax.nn.log_softmax(model.apply(params, x_train), axis=1) * y_train_one_hot, axis=1))\n",
    "        log_param_prior = -0.5 * jnp.sum(jnp.array([jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(params)])) / (prior_scale ** 2)\n",
    "        log_det = log_det_K_svd(model, params, x, jitter)\n",
    "        print(f'log_likelihood: {log_likelihood}, log_param_prior: {log_param_prior}, log_det: {log_det}')\n",
    "        return log_likelihood + log_param_prior - 1 / 2 * log_det\n",
    "    \n",
    "    # compute function space posterior \n",
    "    log_posterior_learned = log_posterior(model, params, x_test, jitter)\n",
    "    if gt_params is not None:\n",
    "        log_posterior_gt = log_posterior(model, gt_params, x_test, jitter)\n",
    "    else:\n",
    "        log_posterior_gt = None\n",
    "\n",
    "    prediction = jax.nn.softmax(model.apply(params, x_test))[..., 1].reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    cbar = plt.contourf(xx, yy, prediction, levels=20, cmap=cm.coolwarm)\n",
    "    cb = plt.colorbar(cbar,)\n",
    "    cb.ax.set_ylabel(\n",
    "        \"$\\mathbb{E}[\\mathbf{y} | \\mathcal{D}; \\mathbf{x}]$\",\n",
    "        rotation=270,\n",
    "        labelpad=40,\n",
    "        size=30,\n",
    "    )\n",
    "    # cb.ax.set_ylabel('$E[y | \\mathcal{D}; x]$', labelpad=-90)\n",
    "    cb.ax.tick_params(labelsize=30)\n",
    "    plt.scatter(\n",
    "        x_train[y_train[:, 0] == 0][..., 0],\n",
    "        x_train[y_train[:, 0] == 0][..., 1],\n",
    "        color=\"cornflowerblue\",\n",
    "        edgecolors=\"black\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        x_train[y_train[:, 0] == 1][..., 0],\n",
    "        x_train[y_train[:, 0] == 1][..., 1],\n",
    "        color=\"tomato\",\n",
    "        edgecolors=\"black\",\n",
    "    )\n",
    "    plt.tick_params(labelsize=30)\n",
    "    plt.savefig(f'{output_dir}/{seed}.png')\n",
    "    # show noise_scale, prior_scale on plot\n",
    "    # plt.text(0.05, 0.9, rf'$\\alpha = {prior_scale}$, $\\sigma = {noise_scale}$, $\\tau = {temp}$', fontsize=20, verticalalignment='top')\n",
    "    plt.title(rf'$\\alpha = {prior_scale}$, $\\tau = {temp}$, $\\log p(f|D) = {log_posterior_learned:.0f}$')\n",
    "    # plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'tanh': nn.tanh,\n",
    "    'elu': nn.elu,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [16]\n",
    "depths = [2]\n",
    "acts = ['tanh']\n",
    "\n",
    "\n",
    "# SGD hyper\n",
    "optimizer = 'adam'\n",
    "lr = 1e-2\n",
    "n_step = int(1e3)\n",
    "prior_scales = [1]\n",
    "noise_scale = 0\n",
    "n_train = 100\n",
    "n_eval = 1000\n",
    "x_lims = [-5, 5]\n",
    "\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = jax.random.uniform(rng_key, (n_eval, 2), minval=x_lims[0], maxval=x_lims[1])\n",
    "    return X_eval\n",
    "\n",
    "skip_if_done = True\n",
    "fsmap = False\n",
    "method = 'exact'\n",
    "pmap_results = []\n",
    "for width in widths:\n",
    "    for depth in depths:\n",
    "        for act in acts:\n",
    "            for prior_scale in prior_scales:\n",
    "                output_dir = f'two_moons_pmap/{width}_{depth}_{act}_{optimizer}_noise{noise_scale}_prior{prior_scale}'\n",
    "                # mkdir if needed\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                for seed in range(1):\n",
    "                    arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                    result_path = f'{output_dir}/result_{seed}.pt'\n",
    "                    if os.path.exists(result_path) and skip_if_done:\n",
    "                        pmap_results.append(torch.load(result_path))\n",
    "                        print('Loaded result from ', result_path)\n",
    "                    else:\n",
    "                        gt_params = None\n",
    "                        pmap_results.append(run_two_moons(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [16]\n",
    "depths = [2]\n",
    "acts = ['tanh']\n",
    "\n",
    "\n",
    "# SGD hyper\n",
    "optimizer = 'adam'\n",
    "lr = 1e-2\n",
    "n_step = int(1e3)\n",
    "prior_scales = [1]\n",
    "noise_scale = 0\n",
    "n_eval = 400\n",
    "x_lims = [-5, 5]\n",
    "\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = jax.random.uniform(rng_key, (n_eval, 2), minval=x_lims[0], maxval=x_lims[1])\n",
    "    return X_eval\n",
    "\n",
    "skip_if_done = True\n",
    "fsmap = True\n",
    "temps = [1, 0.5, 0]\n",
    "method = 'exact'\n",
    "pmap_results = []\n",
    "for width in widths:\n",
    "    for depth in depths:\n",
    "        for act in acts:\n",
    "            for prior_scale in prior_scales:\n",
    "                for temp in temps:\n",
    "                    output_dir = f'two_moons_fmap_{method}/{width}_{depth}_{act}_{optimizer}_noise{noise_scale}_prior{prior_scale}_temp{temp}'\n",
    "                    # mkdir if needed\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    for seed in range(1):\n",
    "                        arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                        result_path = f'{output_dir}/result_{seed}.pt'\n",
    "                        if os.path.exists(result_path) and skip_if_done:\n",
    "                            pmap_results.append(torch.load(result_path))\n",
    "                            print('Loaded result from ', result_path)\n",
    "                        else:\n",
    "                            gt_params = None\n",
    "                            pmap_results.append(run_two_moons(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [16]\n",
    "depths = [2]\n",
    "acts = ['tanh']\n",
    "\n",
    "\n",
    "# SGD hyper\n",
    "optimizer = 'adam'\n",
    "lr = 1e-2\n",
    "n_step = int(1e3)\n",
    "prior_scales = [1]\n",
    "noise_scale = 0\n",
    "n_eval = 100\n",
    "x_lims = [-5, 5]\n",
    "\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = jax.random.uniform(rng_key, (n_eval, 2), minval=x_lims[0], maxval=x_lims[1])\n",
    "    return X_eval\n",
    "\n",
    "skip_if_done = True\n",
    "fsmap = True\n",
    "temps = [1, 0.5, 0]\n",
    "method = 'exact'\n",
    "pmap_results = []\n",
    "for width in widths:\n",
    "    for depth in depths:\n",
    "        for act in acts:\n",
    "            for prior_scale in prior_scales:\n",
    "                for temp in temps:\n",
    "                    output_dir = f'two_moons_fmap_{method}/{width}_{depth}_{act}_{optimizer}_noise{noise_scale}_prior{prior_scale}_temp{temp}'\n",
    "                    # mkdir if needed\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    for seed in range(1):\n",
    "                        arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                        result_path = f'{output_dir}/result_{seed}.pt'\n",
    "                        if os.path.exists(result_path) and skip_if_done:\n",
    "                            pmap_results.append(torch.load(result_path))\n",
    "                            print('Loaded result from ', result_path)\n",
    "                        else:\n",
    "                            gt_params = None\n",
    "                            pmap_results.append(run_two_moons(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [16]\n",
    "depths = [2]\n",
    "acts = ['tanh']\n",
    "\n",
    "\n",
    "# SGD hyper\n",
    "optimizer = 'adam'\n",
    "lr = 1e-2\n",
    "n_step = int(1e3)\n",
    "prior_scales = [1]\n",
    "noise_scale = 0\n",
    "n_eval = 1000\n",
    "x_lims = [-5, 5]\n",
    "\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = jax.random.uniform(rng_key, (n_eval, 2), minval=x_lims[0], maxval=x_lims[1])\n",
    "    return X_eval\n",
    "\n",
    "skip_if_done = True\n",
    "fsmap = True\n",
    "temps = [1, 0.5, 0]\n",
    "method = 'exact'\n",
    "pmap_results = []\n",
    "for width in widths:\n",
    "    for depth in depths:\n",
    "        for act in acts:\n",
    "            for prior_scale in prior_scales:\n",
    "                for temp in temps:\n",
    "                    output_dir = f'two_moons_fmap_{method}/{width}_{depth}_{act}_{optimizer}_noise{noise_scale}_prior{prior_scale}_temp{temp}'\n",
    "                    # mkdir if needed\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    for seed in range(1):\n",
    "                        arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                        result_path = f'{output_dir}/result_{seed}.pt'\n",
    "                        if os.path.exists(result_path) and skip_if_done:\n",
    "                            pmap_results.append(torch.load(result_path))\n",
    "                            print('Loaded result from ', result_path)\n",
    "                        else:\n",
    "                            gt_params = None\n",
    "                            pmap_results.append(run_two_moons(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [16]\n",
    "depths = [2]\n",
    "acts = ['tanh']\n",
    "\n",
    "\n",
    "# SGD hyper\n",
    "optimizer = 'adam'\n",
    "lr = 1e-2\n",
    "n_step = int(1e3)\n",
    "prior_scales = [1]\n",
    "noise_scale = 0\n",
    "n_eval = 400\n",
    "x_lims = [-10, 10]\n",
    "\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = jax.random.uniform(rng_key, (n_eval, 2), minval=x_lims[0], maxval=x_lims[1])\n",
    "    return X_eval\n",
    "\n",
    "skip_if_done = True\n",
    "fsmap = True\n",
    "temps = [1e2, 1e3, 1e4, 1e5]\n",
    "method = 'instability'\n",
    "pmap_results = []\n",
    "for width in widths:\n",
    "    for depth in depths:\n",
    "        for act in acts:\n",
    "            for prior_scale in prior_scales:\n",
    "                for temp in temps:\n",
    "                    output_dir = f'two_moons_fmap_{method}/{width}_{depth}_{act}_{optimizer}_noise{noise_scale}_prior{prior_scale}_temp{temp}'\n",
    "                    # mkdir if needed\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    for seed in range(1):\n",
    "                        arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                        result_path = f'{output_dir}/result_{seed}.pt'\n",
    "                        if os.path.exists(result_path) and skip_if_done:\n",
    "                            pmap_results.append(torch.load(result_path))\n",
    "                            print('Loaded result from ', result_path)\n",
    "                        else:\n",
    "                            gt_params = None\n",
    "                            pmap_results.append(run_two_moons(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [16]\n",
    "depths = [2]\n",
    "acts = ['tanh']\n",
    "\n",
    "\n",
    "# SGD hyper\n",
    "optimizer = 'adam'\n",
    "lr = 1e-2\n",
    "n_step = int(1e3)\n",
    "prior_scales = [1]\n",
    "noise_scale = 0\n",
    "n_eval = 400\n",
    "x_lims = [-10, 10]\n",
    "\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = jax.random.uniform(rng_key, (n_eval, 2), minval=x_lims[0], maxval=x_lims[1])\n",
    "    return X_eval\n",
    "\n",
    "skip_if_done = True\n",
    "fsmap = True\n",
    "temps = [1, 0.5, 0.1, 0.01]\n",
    "method = 'diag'\n",
    "pmap_results = []\n",
    "for width in widths:\n",
    "    for depth in depths:\n",
    "        for act in acts:\n",
    "            for prior_scale in prior_scales:\n",
    "                for temp in temps:\n",
    "                    output_dir = f'two_moons_fmap_{method}/{width}_{depth}_{act}_{optimizer}_noise{noise_scale}_prior{prior_scale}_temp{temp}'\n",
    "                    # mkdir if needed\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    for seed in range(1):\n",
    "                        arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                        result_path = f'{output_dir}/result_{seed}.pt'\n",
    "                        if os.path.exists(result_path) and skip_if_done:\n",
    "                            pmap_results.append(torch.load(result_path))\n",
    "                            print('Loaded result from ', result_path)\n",
    "                        else:\n",
    "                            gt_params = None\n",
    "                            pmap_results.append(run_two_moons(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fspace': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9436057e92285046d415c34e216bd357b01decd87fa7e06f42744a4b160880c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
