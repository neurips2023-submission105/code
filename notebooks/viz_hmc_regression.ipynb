{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "# os.environ['XLA_PYTHON_CLIENT_ALLOCATOR']= 'platform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import blackjax\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "# sns.set(font_scale=2, style='whitegrid')\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats(\"pdf\", \"png\")\n",
    "plt.rcParams[\"savefig.dpi\"] = 150\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams[\"figure.figsize\"] = 6, 4\n",
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"lines.linewidth\"] = 2.0\n",
    "plt.rcParams[\"lines.markersize\"] = 8\n",
    "plt.rcParams[\"legend.fontsize\"] = 14\n",
    "plt.rcParams[\"grid.linestyle\"] = \"-\"\n",
    "plt.rcParams[\"grid.linewidth\"] = 1.0\n",
    "plt.rcParams[\"legend.facecolor\"] = \"white\"\n",
    "# plt.rcParams['grid.color'] = \"grey\"\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "# plt.rcParams['font.family'] = \"normal\"\n",
    "# plt.rcParams['font.family'] = \"sans-serif\"\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "plt.rcParams[\n",
    "    \"text.latex.preamble\"\n",
    "] = \"\\\\usepackage{subdepth} \\\\usepackage{amsfonts} \\\\usepackage{type1cm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snelson(x_train, y_train, n_test=500, x_test_lim=6, standardize_x=False, standardize_y=False):\n",
    "    mask = ((x_train < 1.5) | (x_train > 3)).flatten()\n",
    "    x_train = x_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "\n",
    "    idx = np.argsort(x_train)\n",
    "    x_train = x_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "\n",
    "    if standardize_x:\n",
    "        x_train = (x_train - x_train.mean(0)) / x_train.std(0)\n",
    "    if standardize_y:\n",
    "        y_train = (y_train - y_train.mean(0)) / y_train.std(0)\n",
    "\n",
    "    x_test = np.linspace(-x_test_lim, x_test_lim, n_test)[:, None]\n",
    "\n",
    "    return x_train[:, None], y_train[:, None], x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_data = pd.read_csv('snelson.csv')\n",
    "x_train, y_train = np.array(_raw_data.x.values), np.array(_raw_data.y.values)\n",
    "x_train, y_train, x_test = snelson(x_train, y_train, standardize_x=True, standardize_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 100\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        out = nn.Sequential([\n",
    "            nn.Dense(features=self.H),\n",
    "            nn.tanh,\n",
    "            nn.Dense(features=self.H),\n",
    "            nn.tanh,\n",
    "            nn.Dense(features=self.out_size),\n",
    "        ])(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distrax\n",
    "\n",
    "\n",
    "def logprior_fn(params, prior_scale):\n",
    "    leaves, _ = jax.tree_util.tree_flatten(params)\n",
    "    flat_params = jnp.concatenate([jnp.ravel(p) for p in leaves])\n",
    "    lik = distrax.Normal(0., prior_scale).log_prob(flat_params)\n",
    "    return jnp.sum(lik)\n",
    "\n",
    "def loglikelihood_fn(params, X, Y, model, noise_scale):\n",
    "    f = model.apply(params, X)\n",
    "    lik = distrax.Normal(f, noise_scale).log_prob(Y)\n",
    "    return jnp.sum(lik)\n",
    "\n",
    "def logprob_fn(params, X, Y, model, noise_scale, prior_scale):\n",
    "    return loglikelihood_fn(params, X, Y, model, noise_scale) + logprior_fn(params, prior_scale)\n",
    "\n",
    "def l2_loss_fn(params, X, Y, model, noise_scale, prior_scale):\n",
    "    y_pred = model.apply(params, X)\n",
    "    loss = jnp.sum((Y - y_pred)**2)\n",
    "    leaves, _ = jax.tree_util.tree_flatten(params)\n",
    "    reg = jnp.sum(jnp.array([jnp.sum(jnp.ravel(p)**2) for p in leaves]))\n",
    "    lmbda = (noise_scale / prior_scale)**2\n",
    "    return loss + lmbda * reg\n",
    "\n",
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    @jax.jit\n",
    "    def one_step(state, rng_key):\n",
    "        state, _ = kernel(rng_key, state)\n",
    "        return state, state\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, states = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states\n",
    "\n",
    "def get_predictions(model, samples, X):\n",
    "    vmap = jax.vmap(model.apply, in_axes=(0, None), out_axes=0)\n",
    "    predictions = vmap(samples, X)\n",
    "    return predictions.squeeze(-1)\n",
    "\n",
    "def get_hmc_samples(rng_key, logposterior_fn, model, n_warmup, num_samples, x_test):    \n",
    "    rng_key, init_params_key = jax.random.split(rng_key)\n",
    "    init_params = jax.jit(model.init)(init_params_key, jnp.ones((1, 1)))\n",
    "    adapt = blackjax.window_adaptation(blackjax.nuts, logposterior_fn, n_warmup)\n",
    "    rng_key, warmup_key = jax.random.split(rng_key)\n",
    "    final_state, kernel, _ = adapt.run(warmup_key, init_params)\n",
    "    rng_key, inference_key = jax.random.split(rng_key)\n",
    "    states = inference_loop(inference_key, kernel, final_state, num_samples)\n",
    "    samples = states.position\n",
    "    y_pred_samples = get_predictions(model, samples, x_test)\n",
    "    return y_pred_samples\n",
    "\n",
    "def get_sgd_pred(lr, momentum, n_steps, rng_key, loss_fn, model, x_train, y_train, x_test):\n",
    "    rng_key, init_params_key = jax.random.split(rng_key)\n",
    "    init_params = jax.jit(model.init)(init_params_key, jnp.ones((1, 1)))\n",
    "    tx = optax.sgd(lr, momentum=momentum)\n",
    "    # tx = optax.adam(lr)\n",
    "    ts = train_state.TrainState.create(apply_fn=model.apply, params=init_params, tx=tx)\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    losses = []\n",
    "    for e in tqdm(range(n_steps)):\n",
    "        loss, grads = grad_fn(ts.params)\n",
    "        ts = ts.apply_gradients(grads=grads)\n",
    "        # y_pred = model.apply(ts.params, x_train)\n",
    "        # loss = jnp.mean((y_train - y_pred)**2)\n",
    "        losses.append(loss)\n",
    "    return model.apply(ts.params, x_test)[..., 0], losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(noise_scale, prior_scale, n_warmup, num_samples, lr, momentum, n_steps, plots_dir):\n",
    "    # model\n",
    "    model = MLP(out_size=1)\n",
    "    logposterior_fn = partial(logprob_fn, X=x_train, Y=y_train, model=model, noise_scale=noise_scale, prior_scale=prior_scale)\n",
    "    # HMC\n",
    "    rng_key = jax.random.PRNGKey(42)\n",
    "    y_hmc_samples = get_hmc_samples(rng_key, logposterior_fn, model, n_warmup, num_samples, x_test)\n",
    "\n",
    "    # SGD\n",
    "    rng_key = jax.random.PRNGKey(42)\n",
    "    loss_fn = partial(l2_loss_fn, X=x_train, Y=y_train, model=model, noise_scale=noise_scale, prior_scale=prior_scale)\n",
    "    y_sgd, losses = get_sgd_pred(lr, momentum, n_steps, rng_key, loss_fn, model, x_train, y_train, x_test)\n",
    "    # plot and save losses\n",
    "    plt.figure()\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(f'{plots_dir}/sgd_losses_n{noise_scale}_p{prior_scale}.png')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 6), dpi=120)\n",
    "    ax.scatter(x_train, y_train, c='r', label='Train Data', s=10)\n",
    "\n",
    "    ax.plot(x_test[..., 0], y_sgd, c='green', label='MAP (GD)')\n",
    "\n",
    "    y_hmc_mean = np.mean(y_hmc_samples, axis=0)\n",
    "    y_hmc_std = np.std(y_hmc_samples, axis=0)\n",
    "    ax.plot(x_test[..., 0], y_hmc_mean, c='black', label='Posterior mean')\n",
    "    ax.fill_between(x_test[..., 0], y_hmc_mean - 2 * y_hmc_std, y_hmc_mean + 2 * y_hmc_std, color=\"C0\", alpha=0.2)\n",
    "\n",
    "    # choose n_draws random y_hmc_samples\n",
    "    n_draws = 100\n",
    "    y_hmc_samples_viz = y_hmc_samples[jax.random.choice(rng_key, y_hmc_samples.shape[0], (n_draws,))]\n",
    "    ax.plot(x_test[..., 0], y_hmc_samples_viz.T,\n",
    "            linewidth=0.5,\n",
    "            color=\"xkcd:blue\",\n",
    "            zorder=3,\n",
    "            alpha=0.3)\n",
    "\n",
    "    ax.set(xlabel='$x$', ylabel='$y$', ylim=[-3, 3])\n",
    "    # add function draws to legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    handles.append(plt.Line2D([0], [0], color='blue', linewidth=.5))\n",
    "    labels.append('Posterior draws')\n",
    "    ax.legend(handles, labels)\n",
    "\n",
    "    ax.scatter(x_train, y_train, c='r', label='Train Data', s=10)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # show noise_scale, prior_scale on plot\n",
    "    ax.text(0.05, 0.95, f'Noise scale: {noise_scale}', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "    ax.text(0.05, 0.9, f'Prior scale: {prior_scale}', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'{plots_dir}/n{noise_scale}_p{prior_scale}.png')\n",
    "    fig.show()\n",
    "\n",
    "    # save samples and sgd prediction\n",
    "    np.save(f'{plots_dir}/n{noise_scale}_p{prior_scale}_hmc_samples.npy', y_hmc_samples)\n",
    "    np.save(f'{plots_dir}/n{noise_scale}_p{prior_scale}_sgd_pred.npy', y_sgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "noise_scales = [1, 0.33, 0.1]\n",
    "prior_scales = [0.33, 1, 3.3]\n",
    "\n",
    "# HMC hyper\n",
    "n_warmup = 500\n",
    "num_samples = 100\n",
    "\n",
    "# SGD hyper\n",
    "lr = 1e-4\n",
    "momentum = .9\n",
    "n_steps = 5000\n",
    "\n",
    "plots_dir = 'plots/1012'\n",
    "# mkdir if needed\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "for noise_scale in noise_scales:\n",
    "    for prior_scale in prior_scales:\n",
    "        print(f'Running experiment for noise_scale={noise_scale} and prior_scale={prior_scale}')\n",
    "        run_experiment(noise_scale, prior_scale, n_warmup, num_samples, lr, momentum, n_steps, plots_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fspace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9436057e92285046d415c34e216bd357b01decd87fa7e06f42744a4b160880c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
