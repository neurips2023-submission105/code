{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# os.environ['XLA_PYTHON_CLIENT_ALLOCATOR']= 'platform'\n",
    "# os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import blackjax\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import jaxopt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"pdf\", \"png\")\n",
    "plt.rcParams[\"savefig.dpi\"] = 75\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams[\"figure.figsize\"] = 6, 4\n",
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"lines.linewidth\"] = 2.0\n",
    "plt.rcParams[\"lines.markersize\"] = 8\n",
    "plt.rcParams[\"legend.fontsize\"] = 14\n",
    "plt.rcParams[\"grid.linestyle\"] = \"-\"\n",
    "plt.rcParams[\"grid.linewidth\"] = 1.0\n",
    "plt.rcParams[\"legend.facecolor\"] = \"white\"\n",
    "# plt.rcParams['grid.color'] = \"grey\"\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "# plt.rcParams['font.family'] = \"normal\"\n",
    "# plt.rcParams['font.family'] = \"sans-serif\"\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "plt.rcParams[\n",
    "    \"text.latex.preamble\"\n",
    "] = \"\\\\usepackage{subdepth} \\\\usepackage{amsfonts} \\\\usepackage{type1cm}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "\n",
    "seed = 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "### LOAD DATA\n",
    "def _one_hot(x, k):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k))\n",
    "\n",
    "_x_train, _y_train = sklearn.datasets.make_moons(\n",
    "    n_samples=200, shuffle=True, noise=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "h = 0.25\n",
    "test_lim = 3\n",
    "x_min, x_max = -5, 5\n",
    "y_min, y_max = -5, 5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "_x_test = np.vstack((xx.reshape(-1), yy.reshape(-1))).T\n",
    "\n",
    "h = 0.25\n",
    "test_lim = 7\n",
    "x_wide_min, x_wide_max = (\n",
    "    _x_train[:, 0].min() - test_lim,\n",
    "    _x_train[:, 0].max() + test_lim,\n",
    ")\n",
    "y_wide_min, y_wide_max = (\n",
    "    _x_train[:, 1].min() - test_lim,\n",
    "    _x_train[:, 1].max() + test_lim,\n",
    ")\n",
    "xx_wide, yy_wide = np.meshgrid(\n",
    "    np.arange(x_wide_min, x_wide_max, h), np.arange(y_wide_min, y_wide_max, h)\n",
    ")\n",
    "_x_test_wide = np.vstack((xx_wide.reshape(-1), yy_wide.reshape(-1))).T\n",
    "\n",
    "_y_test = np.ones(_x_test.shape[0])\n",
    "_y_test_wide = np.ones(_x_test_wide.shape[0])\n",
    "\n",
    "\n",
    "x_train = _x_train\n",
    "y_train = _y_train.reshape(-1, 1)\n",
    "\n",
    "x_test = _x_test\n",
    "y_test = _y_test.reshape(-1, 1)\n",
    "\n",
    "x_test_wide = _x_test_wide\n",
    "y_test_wide = _y_test_wide.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from typing import Callable\n",
    "\n",
    "def count_params(model, x):\n",
    "    params = model.init(jax.random.PRNGKey(0), x)\n",
    "    n = sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "    print(f'Number of parameters: {n/1e3} k')\n",
    "    \n",
    "def param_size(params):\n",
    "    return sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 64\n",
    "    hidden_layers: int = 1\n",
    "    act: Callable = nn.relu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        layers = [nn.Dense(self.H)]\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers += [self.act, nn.Dense(self.H)]\n",
    "        layers += [self.act, nn.Dense(self.out_size)]\n",
    "        return nn.Sequential(layers)(x)\n",
    "\n",
    "def reparam_initializer(initializer, f):\n",
    "    def init(key, shape, dtype=jnp.float32):\n",
    "        # sample original parameters and then invert the reparametrization\n",
    "        return f(initializer(key, shape, dtype))\n",
    "    return init\n",
    "\n",
    "class ReparamDense(nn.Module):\n",
    "    # same as nn.Dense but with reparam weights\n",
    "    # reparam: Callable = lambda x: x # w = reparam(x)\n",
    "    # reparam_inv: Callable = lambda x: x # x = reparam_inv(w)\n",
    "    # bias_init: Callable = reparam_initializer(nn.initializers.normal(stddev=1e-6))\n",
    "    # kernel_init: Callable = reparam_initializer(nn.initializers.lecun_normal())\n",
    "\n",
    "    def __init__(self, features, reparam, reparam_inv, init_scale=None):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.reparam = reparam\n",
    "        self.reparam_inv = reparam_inv\n",
    "        # zero init for bias\n",
    "        if init_scale is None:\n",
    "            self.bias_init = reparam_initializer(nn.initializers.normal(stddev=1e-4), f=reparam_inv)\n",
    "            self.kernel_init = reparam_initializer(nn.initializers.lecun_normal(), f=reparam_inv)\n",
    "        else:\n",
    "            self.bias_init = reparam_initializer(nn.initializers.normal(stddev=init_scale), f=reparam_inv)\n",
    "            self.kernel_init = reparam_initializer(nn.initializers.normal(stddev=init_scale), f=reparam_inv)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        reparam_kernel = self.param('reparam_kernel', self.kernel_init, (inputs.shape[-1], self.features))\n",
    "        reparam_bias = self.param('reparam_bias', self.bias_init, (1, self.features)) # not using bias_init to avoid dividing by zero\n",
    "        # invert weights\n",
    "        kernel = jax.tree_util.tree_map(self.reparam, reparam_kernel)\n",
    "        bias = jax.tree_util.tree_map(self.reparam, reparam_bias)\n",
    "        # clamp to avoid numerical issues\n",
    "        kernel = jnp.clip(kernel, a_min=-1e6, a_max=1e6)\n",
    "        bias = jnp.clip(bias, a_min=-1e6, a_max=1e6)\n",
    "        return jnp.dot(inputs, kernel) + bias\n",
    "\n",
    "class ReparamMLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 64\n",
    "    hidden_layers: int = 1\n",
    "    reparam: Callable = lambda x: x # w = reparam(x)\n",
    "    reparam_inv: Callable = lambda x: x # x = reparam_inv(w)\n",
    "    act: Callable = nn.tanh\n",
    "    init_scale: float = None\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        layers = [ReparamDense(features=self.H, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers += [self.act, ReparamDense(features=self.H, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        layers += [self.act, ReparamDense(features=self.out_size, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        out = nn.Sequential(layers)(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_norm(tree):\n",
    "    return jnp.sqrt(sum([jnp.sum(x**2) for x in jax.tree_leaves(tree)]))\n",
    "\n",
    "def random_split_like_tree(rng_key, target=None, treedef=None):\n",
    "    if treedef is None:\n",
    "        treedef = jax.tree_structure(target)\n",
    "    keys = jax.random.split(rng_key, treedef.num_leaves)\n",
    "    return jax.tree_unflatten(treedef, keys)\n",
    "\n",
    "@jax.jit\n",
    "def tree_random_normal_like(rng_key, target):\n",
    "    keys_tree = random_split_like_tree(rng_key, target)\n",
    "    return jax.tree_map(\n",
    "        lambda l, k: jax.random.normal(k, l.shape, l.dtype),\n",
    "        target,\n",
    "        keys_tree,\n",
    "    )\n",
    "\n",
    "def jacobian_sigular_values(model, p, x):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x) # (b, o, p)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('bo...->...bo', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    P = param_size(p)\n",
    "    J = J.reshape(P, -1).T # (B*O, P)\n",
    "    # sigular values of J\n",
    "    _, S, _ = jnp.linalg.svd(J, full_matrices=False)\n",
    "    return S\n",
    "\n",
    "def get_K_matrix(model, p, x):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('b...->...b', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    J = J.reshape(-1, x.shape[0]).T # (N, P)\n",
    "    N = x.shape[0]\n",
    "    K = J.T @ J / N # (P, P)\n",
    "    return K\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,3,4,5))\n",
    "def log_det_g_svd(model, p, x, jitter=1e-6, return_eig=False, shift_by_jitter=True):\n",
    "    P = param_size(p)\n",
    "    zeros = jnp.zeros(P)\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    # P eigenvalues, correctly handling the case where P > N\n",
    "    s = zeros.at[:s.shape[0]].set(s)\n",
    "    logdet_svd = jnp.sum(jnp.log(s ** 2 + jitter))\n",
    "    if shift_by_jitter:\n",
    "        logdet_svd = logdet_svd - P * jnp.log(jitter)\n",
    "    if return_eig:\n",
    "        return logdet_svd, s ** 2\n",
    "    return logdet_svd\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,3,4))\n",
    "def log_det_g_svd_first_order(model, p, x, jitter=1e-6, return_eig=False):\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    logdet_svd = jnp.sum(jnp.log(jitter) + (s ** 2) / jitter)\n",
    "    P = param_size(p)\n",
    "    logdet_svd = logdet_svd - P * jnp.log(jitter)\n",
    "    if return_eig:\n",
    "        return logdet_svd, s ** 2\n",
    "    return logdet_svd\n",
    "\n",
    "def log_det_K(model, p, x, jitter=1e-6, scale=1.0):\n",
    "    K = get_K_matrix(model, p, x)\n",
    "    # add jitter\n",
    "    K = K + jitter * jnp.eye(K.shape[0])\n",
    "    s, log_det = jnp.linalg.slogdet(K)\n",
    "    return log_det\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    logdet_svd = 2 * jnp.sum(jnp.log(s))\n",
    "    return logdet_svd\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,4))\n",
    "def trace_estimator(model, p, x, dp, sigma=0.1):\n",
    "    # dp: N(0, I)\n",
    "    # p_perturbed = p + dp, both pytrees\n",
    "    # scale dp by sigma\n",
    "    dp = jax.tree_util.tree_map(lambda x: x * sigma, dp)\n",
    "    p_perturbed = jax.tree_util.tree_map(lambda x, y: x + y, p, dp)\n",
    "    # compute the difference between the outputs\n",
    "    y = model.apply(p, x)\n",
    "    y_perturbed = model.apply(p_perturbed, x)\n",
    "    diff = y - y_perturbed\n",
    "    # compute the avg sq norm of the difference\n",
    "    avg_dff_sq_norm = jnp.mean((diff ** 2).sum(axis=-1))\n",
    "    return avg_dff_sq_norm / (sigma ** 2)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,3,4,5,6))\n",
    "def log_det_trace_estimator(model, p, x, jitter, sigma=0.01, n_samples=1, normalize=False):\n",
    "    # sample n_samples dp from N(0, I) and compute the trace estimator\n",
    "    traces = []\n",
    "    P = param_size(p)\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    for _ in range(n_samples):\n",
    "        rng_key, rng_key_sample = jax.random.split(rng_key)\n",
    "        dp = tree_random_normal_like(rng_key_sample, p)\n",
    "        if normalize:\n",
    "            scale = (P ** 0.5) / tree_norm(dp)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        # scale dp to have squared norm P * sigma^2\n",
    "        dp = jax.tree_util.tree_map(lambda x: x * scale, dp)\n",
    "        trace = trace_estimator(model, p, x, dp, sigma=sigma)\n",
    "        traces.append(trace)\n",
    "    trace = jnp.array(traces).mean()\n",
    "    return trace / jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'tanh': nn.tanh,\n",
    "    'elu': nn.elu,\n",
    "    'cos': jnp.cos,\n",
    "    'sin': jnp.sin,\n",
    "    'softplus': nn.softplus,\n",
    "    'relu': nn.relu,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(style=\"whitegrid\", font_scale=2.0, rc={\"lines.linewidth\": 3.0})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "def subtract_min(x):\n",
    "    return x\n",
    "    # return x - np.min(x)\n",
    "\n",
    "def normalize(v):\n",
    "    v_norm2 = jnp.sum(jnp.array([jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(v)]))\n",
    "    v = jax.tree_util.tree_map(lambda x: x / jnp.sqrt(v_norm2), v)    \n",
    "    return v\n",
    "\n",
    "def log_posterior(model, params, x_train, y_train, x_context, jitter, log_det_fn):\n",
    "    y_train_one_hot = jax.nn.one_hot(y_train.reshape(-1), 2)\n",
    "    log_likelihood = jnp.sum(jnp.sum(jax.nn.log_softmax(model.apply(params, x_train), axis=1) * y_train_one_hot, axis=1))\n",
    "    log_param_prior = -0.5 * jnp.sum(jnp.array([jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(params)])) / (prior_scale ** 2)\n",
    "    log_det = log_det_fn(model, params, x_context, jitter)\n",
    "    # print(f'log_likelihood: {log_likelihood}, log_param_prior: {log_param_prior}, log_det: {log_det}')\n",
    "    return log_likelihood, log_param_prior, log_det\n",
    "\n",
    "def scan_posterior(model, x_context, jitter, steps, max_alpha=1, log_det_fn=log_det_g_svd):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    init_params = model.init(rng_key, x_train)\n",
    "    # split key\n",
    "    rng_key, rng_key_sample_1, rng_key_sample_2, rng_key_sample_3 = jax.random.split(rng_key, 4)\n",
    "    # gaussian random param\n",
    "    v1 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_1, x.shape), init_params))\n",
    "    v2 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_2, x.shape), init_params))\n",
    "    v3 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_3, x.shape), init_params))\n",
    "    # plot log posterior on the ray spanned by \n",
    "    alpha = jnp.concatenate([-jnp.logspace(jnp.log10(max_alpha), -4, steps), jnp.logspace(-4, jnp.log10(max_alpha), steps)])\n",
    "    alpha = alpha[alpha != 0]\n",
    "\n",
    "    log_likelihoods, log_param_priors, log_dets = [], [], []\n",
    "    log_posterior_fn = jax.jit(lambda p: log_posterior(model, p, x_train, y_train, x_context, jitter, log_det_fn))\n",
    "    for a in tqdm(alpha):\n",
    "        for v in [v1, v2, v3]:\n",
    "            params = jax.tree_util.tree_map(lambda x: x * a, v)\n",
    "            log_likelihood, log_param_prior, log_det = log_posterior_fn(params)\n",
    "            log_likelihoods.append(log_likelihood)\n",
    "            log_param_priors.append(log_param_prior)\n",
    "            log_dets.append(log_det)\n",
    "\n",
    "    log_likelihoods = jnp.array(log_likelihoods).reshape(-1, 3)\n",
    "    log_param_priors = jnp.array(log_param_priors).reshape(-1, 3)\n",
    "    log_dets = jnp.array(log_dets).reshape(-1, 3)\n",
    "    return alpha, log_likelihoods, log_param_priors, log_dets\n",
    "\n",
    "def plot_scan(alpha, log_likelihoods, log_param_priors, log_dets):\n",
    "    # plot three quantities, as dots not lines\n",
    "    plt.figure(dpi=100, figsize=(10, 6))\n",
    "    for i in range(3):\n",
    "        plt.plot(alpha, -log_likelihoods[:, i], label=r'NLL', color=f'C{i}', linestyle='--')\n",
    "        plt.plot(alpha, subtract_min(-log_param_priors[:, i]), label=r'$-\\Delta\\log p_w$', color=f'C{i}', linestyle=':')\n",
    "        plt.plot(alpha[alpha<0], 1/2 * subtract_min(log_dets[alpha<0, i]), label=rf'$\\Delta1/2\\log\\det$', color=f'C{i}', linestyle='-')\n",
    "        plt.plot(alpha[alpha>0], 1/2 * subtract_min(log_dets[alpha>0, i]), color=f'C{i}', linestyle='-')\n",
    "    # put legend outside\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('divergence.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_posterior_2d(model, x_context, jitter, steps, max_alpha=1, log_det_fn=log_det_g_svd):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    init_params = model.init(rng_key, x_train)\n",
    "    # split key\n",
    "    rng_key, rng_key_sample_1, rng_key_sample_2, rng_key_sample_3 = jax.random.split(rng_key, 4)\n",
    "    # gaussian random param\n",
    "    v1 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_1, x.shape), init_params))\n",
    "    v2 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_2, x.shape), init_params))\n",
    "    # plot log posterior on the ray spanned by v1 and v2\n",
    "    alpha = jnp.concatenate([-jnp.logspace(jnp.log10(max_alpha), -4, steps), jnp.logspace(-4, jnp.log10(max_alpha), steps)])\n",
    "    alpha = alpha[alpha != 0]\n",
    "    # 2d coordinates\n",
    "    u, v = jnp.meshgrid(alpha, alpha)\n",
    "    log_likelihoods, log_param_priors, log_dets = [], [], []\n",
    "    log_posterior_fn = jax.jit(lambda p: log_posterior(model, p, x_train, y_train, x_context, jitter, log_det_fn))\n",
    "    for a, b in tqdm(zip(u.flatten(), v.flatten())):\n",
    "        av1 = jax.tree_util.tree_map(lambda x: x * a, v1)\n",
    "        bv2 = jax.tree_util.tree_map(lambda x: x * b, v2)\n",
    "        params = jax.tree_util.tree_map(lambda x, y: x + y, av1, bv2)\n",
    "        log_likelihood, log_param_prior, log_det = log_posterior_fn(params)\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "        log_param_priors.append(log_param_prior)\n",
    "        log_dets.append(log_det)\n",
    "    log_likelihoods = jnp.array(log_likelihoods)\n",
    "    log_param_priors = jnp.array(log_param_priors)\n",
    "    log_dets = jnp.array(log_dets)\n",
    "    return u, v, log_likelihoods, log_param_priors, log_dets\n",
    "    \n",
    "def plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets):\n",
    "    # three subplots\n",
    "    fig, axs = plt.subplots(1, 3, dpi=100, figsize=(30, 10))\n",
    "    # plot delta log det\n",
    "    axs[0].contourf(u, v, subtract_min(log_dets).reshape(u.shape), levels=20)\n",
    "    axs[0].set_title(r'$\\Delta\\log\\det$')\n",
    "    axs[0].set_xlabel(r'$\\alpha$')\n",
    "    axs[0].set_ylabel(r'$\\beta$')\n",
    "    # make image square\n",
    "    axs[0].set_aspect('equal')\n",
    "    # colorbar\n",
    "    cbar = fig.colorbar(axs[0].contourf(u, v, subtract_min(log_dets).reshape(u.shape), levels=20), ax=axs[0])\n",
    "    # plot NLL\n",
    "    axs[1].contourf(u, v, -log_likelihoods.reshape(u.shape), levels=20)\n",
    "    axs[1].set_title(r'NLL')\n",
    "    axs[1].set_xlabel(r'$\\alpha$')\n",
    "    axs[1].set_ylabel(r'$\\beta$')\n",
    "    axs[1].set_aspect('equal')\n",
    "    # colorbar\n",
    "    cbar = fig.colorbar(axs[1].contourf(u, v, -log_likelihoods.reshape(u.shape), levels=20), ax=axs[1])\n",
    "    # plot delta -log p_w\n",
    "    axs[2].contourf(u, v, subtract_min(-log_param_priors).reshape(u.shape), levels=20)\n",
    "    axs[2].set_title(r'$-\\Delta\\log p_w$')\n",
    "    axs[2].set_xlabel(r'$\\alpha$')\n",
    "    axs[2].set_ylabel(r'$\\beta$')\n",
    "    axs[2].set_aspect('equal')\n",
    "    # colorbar\n",
    "    cbar = fig.colorbar(axs[2].contourf(u, v, subtract_min(-log_param_priors).reshape(u.shape), levels=20), ax=axs[2])\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets):\n",
    "    # same but plot in 3d\n",
    "    fig = plt.figure(figsize=(30, 10))\n",
    "    ax = fig.add_subplot(131, projection='3d')\n",
    "    ax.plot_surface(u, v, subtract_min(log_dets).reshape(u.shape), cmap='viridis', edgecolor='none')\n",
    "    ax.set_title(r'$\\Delta\\log\\det$')\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel(r'$\\beta$')\n",
    "    ax.set_zlabel(r'$\\Delta\\log\\det$')\n",
    "    ax = fig.add_subplot(132, projection='3d')\n",
    "    ax.plot_surface(u, v, -log_likelihoods.reshape(u.shape), cmap='viridis', edgecolor='none')\n",
    "    ax.set_title(r'NLL')\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel(r'$\\beta$')\n",
    "    ax.set_zlabel(r'NLL')\n",
    "    ax = fig.add_subplot(133, projection='3d')\n",
    "    ax.plot_surface(u, v, subtract_min(-log_param_priors).reshape(u.shape), cmap='viridis', edgecolor='none')\n",
    "    ax.set_title(r'$-\\Delta\\log p_w$')\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel(r'$\\beta$')\n",
    "    ax.set_zlabel(r'$-\\Delta\\log p_w$')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing objectives ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=2.0, rc={\"lines.linewidth\": 3.0})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "def plot_compare(x, y, c, xlabel, ylabel, clabel, title, fit=True, log_axes=False):\n",
    "    # a nice square plot to compare x, y\n",
    "    # color the points by min_eig and plot a colorbar\n",
    "\n",
    "    # set figure size to be square\n",
    "    plt.figure(figsize=(8, 8), dpi=100)\n",
    "    # draw diagonal line\n",
    "    max_logdet = max(max(x), max(y))\n",
    "    # create scatter plot\n",
    "    plt.scatter(x, y, c=c, cmap='viridis')\n",
    "    if fit:\n",
    "        # fit a line and obatin goodness of fit\n",
    "        x_fit = x if not log_axes else np.log(x)\n",
    "        y_fit = y if not log_axes else np.log(y)\n",
    "\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        model.fit(x_fit.reshape(-1, 1), y_fit.reshape(-1, 1))\n",
    "        slope = float(model.coef_[0])\n",
    "        def predict(x):\n",
    "            if log_axes:\n",
    "                return np.exp(slope * np.log(x))\n",
    "            else:\n",
    "                return slope * x\n",
    "        # R^2 in fit coordinates\n",
    "        r2 = float(r2_score(y_fit, slope * x_fit))\n",
    "        plt.plot([min(x), max(x)], [predict(min(x)), predict(max(x))], 'k--', label=rf'$a = {slope:.1f}, r^2 = {r2**2:.2f}$')\n",
    "\n",
    "    # colorbar labelled as min_eig\n",
    "    plt.colorbar(label=clabel)\n",
    "    # x = y line\n",
    "    plt.plot([0, max_logdet], [0, max_logdet], 'k--', color='gray')\n",
    "\n",
    "    # label the axes\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    if log_axes:\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "    # set aspect ratio to 1 to make it square\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 4\n",
    "act = 'tanh'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "p = model.init(jax.random.PRNGKey(0), x_train[0][None, :])\n",
    "print(f'Number of parameters: {param_size(p)}, norm: {tree_norm(p).item():.1f}')\n",
    "P = param_size(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "x_context = x_test\n",
    "# trace estimator\n",
    "n_samples = 10\n",
    "sigma = 0.01 # noise scale\n",
    "normalize = False\n",
    "\n",
    "max_scale = 10\n",
    "for num_eval_points in [1000, 300, 100, 30]:\n",
    "    # shuffle with seed 0\n",
    "    eval_idx = np.arange(len(x_test))\n",
    "    for sigma in [0.001, 0.01, 0.1]:\n",
    "        for jitter in [1000]:\n",
    "            logdet_svds = []\n",
    "            logdet_first_order = []\n",
    "            logdet_traces = []\n",
    "            eig_over_jitter = []\n",
    "            max_eig_over_jitter = []\n",
    "            condition = []\n",
    "            rng_key = jax.random.PRNGKey(0)\n",
    "            for i in tqdm(range(trials)):\n",
    "                rng = np.random.default_rng(i)\n",
    "                rng.shuffle(eval_idx)\n",
    "                rng_key, rng_key_sample = jax.random.split(rng_key)\n",
    "                params = model.init(rng_key_sample, x_train[0][None, :])\n",
    "                # uniform between 0 and max_scale\n",
    "                rng_key, rng_key_scale = jax.random.split(rng_key)\n",
    "                scale = jax.random.uniform(rng_key_scale, shape=(), minval=0, maxval=max_scale)\n",
    "                params = jax.tree_map(lambda x: x * scale, params)\n",
    "                logdet_svd, eig = log_det_g_svd(model, params, x_context, jitter, return_eig=True)\n",
    "                logdet_svds.append(logdet_svd)\n",
    "                eig_over_jitter.append(eig.mean().item() / jitter)\n",
    "                max_eig_over_jitter.append(eig.max().item() / jitter)\n",
    "                condition.append(eig.max().item() / (eig.min().item() + 1e-32))\n",
    "                logdet_traces.append(log_det_trace_estimator(model, params, x_context[eval_idx[:num_eval_points]], jitter, sigma, n_samples, normalize).item())\n",
    "                logdet_first_order.append(log_det_g_svd_first_order(model, params, x_context, jitter))\n",
    "\n",
    "            logdet_traces = np.array(logdet_traces)\n",
    "            logdet_svds = np.array(logdet_svds)\n",
    "            logdet_first_order = np.array(logdet_first_order)\n",
    "            condition = np.array(condition)\n",
    "            \n",
    "\n",
    "            plot_compare(logdet_svds, logdet_traces, eig_over_jitter, 'log det', 'Laplacian estimate', r'$\\langle \\lambda / \\epsilon \\rangle$', rf'$\\epsilon = {jitter}$', log_axes=True)\n",
    "            plt.title(rf'$\\sigma = {sigma}, \\epsilon = {jitter}, N = {num_eval_points}$')\n",
    "            plt.savefig(f'objectives/trace_approx_eps{jitter}_std{sigma}_n{num_eval_points}_{act}.pdf')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "x_context = x_test\n",
    "\n",
    "# laplacian estimator\n",
    "n_samples = 10\n",
    "sigma = 0.01 # noise scale\n",
    "normalize = False\n",
    "\n",
    "max_scale = 10\n",
    "\n",
    "for jitter in [1000, 1, 1e-3, 1e-6]:\n",
    "    for num_eval_points in [1600, 800, 100]: # max = 1600\n",
    "        # shuffle with seed 0\n",
    "        eval_idx = np.arange(len(x_test))\n",
    "        for sigma in [0.001, 0.01, 0.1] if jitter == 1000 else [0.001]:\n",
    "            logdets = [] # full eval set\n",
    "            logdets_n = [] # subsampled eval set\n",
    "            true_laplacians = [] # true laplacian based on eigenvalues\n",
    "            laplacian_estimates = [] # estimated laplacian with subsampled eval set\n",
    "            eig_over_jitter = []\n",
    "            max_eig_over_jitter = []\n",
    "            condition = []\n",
    "            rng_key = jax.random.PRNGKey(0)\n",
    "            for i in tqdm(range(trials)):\n",
    "                rng = np.random.default_rng(i)\n",
    "                rng.shuffle(eval_idx)\n",
    "                rng_key, rng_key_sample = jax.random.split(rng_key)\n",
    "                params = model.init(rng_key_sample, x_train[0][None, :])\n",
    "                # uniform between 0 and max_scale\n",
    "                rng_key, rng_key_scale = jax.random.split(rng_key)\n",
    "                scale = jax.random.uniform(rng_key_scale, shape=(), minval=0, maxval=max_scale)\n",
    "                params = jax.tree_map(lambda x: x * scale, params)\n",
    "                logdet_full, eig = log_det_g_svd(model, params, x_context, jitter, return_eig=True, shift_by_jitter=True)\n",
    "                logdets.append(logdet_full)\n",
    "                logdet_n = log_det_g_svd(model, params, x_context[eval_idx[:num_eval_points]], jitter, return_eig=False, shift_by_jitter=True)\n",
    "                logdets_n.append(logdet_n)\n",
    "                eig_over_jitter.append(eig.mean().item() / jitter)\n",
    "                max_eig_over_jitter.append(eig.max().item() / jitter)\n",
    "                condition.append(eig.max().item() / (eig.min().item() + 1e-32))\n",
    "                laplacian_estimates.append(log_det_trace_estimator(model, params, x_context[eval_idx[:num_eval_points]], jitter, sigma, n_samples, normalize).item())\n",
    "                true_laplacians.append(log_det_g_svd_first_order(model, params, x_context, jitter)) # jitter has no effect on this\n",
    "\n",
    "            laplacian_estimates = np.array(laplacian_estimates)\n",
    "            logdets = np.array(logdets)\n",
    "            logdets_n = np.array(logdets_n)\n",
    "            true_laplacians = np.array(true_laplacians)\n",
    "            condition = np.array(condition)\n",
    "            \n",
    "\n",
    "            plot_compare(logdets, laplacian_estimates, eig_over_jitter, rf'$\\log\\det\\left(g + \\epsilon I\\right)$', r'$\\hat{\\Delta} d$', clabel=r'$\\langle \\lambda / \\epsilon \\rangle$', title=rf'$\\sigma = {sigma}, \\epsilon = {jitter}, N = {num_eval_points}$', log_axes=True)\n",
    "            plt.savefig(f'objectives/logdet_laplace_eps{jitter}_std{sigma}_n{num_eval_points}_{act}.pdf')\n",
    "            plt.show()\n",
    "            plot_compare(logdets, logdets_n, eig_over_jitter, rf'$\\log\\det\\left(g + \\epsilon I\\right)$', r'$\\log\\det\\left(\\hat{g} + \\epsilon I\\right)$', clabel=r'$\\langle \\lambda / \\epsilon \\rangle$', title=rf'$\\epsilon = {jitter}, N = {num_eval_points}$', log_axes=True)\n",
    "            plt.savefig(f'objectives/logdet_logdet_n_eps{jitter}_std{sigma}_n{num_eval_points}_{act}.pdf')\n",
    "            plt.show()\n",
    "            plot_compare(true_laplacians, laplacian_estimates, None, r'$\\Delta d$', r'$\\hat{\\Delta} d$', clabel=None, title=rf'$\\sigma = {sigma}, N = {num_eval_points}$', log_axes=True)\n",
    "            plt.savefig(f'objectives/truelaplace_laplace_eps{jitter}_std{sigma}_n{num_eval_points}_{act}.pdf')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "x_context = x_test\n",
    "\n",
    "# laplacian estimator\n",
    "n_samples = 10\n",
    "sigma = 0.01 # noise scale\n",
    "normalize = False\n",
    "\n",
    "max_scale = 10\n",
    "\n",
    "for jitter in [1000, 1e-6]:\n",
    "    for scale_context in [0.3, 3, 10]:\n",
    "        for sigma in [0.001]:\n",
    "            logdets = [] # full eval set\n",
    "            logdets_n = [] # subsampled eval set\n",
    "            true_laplacians = [] # true laplacian based on eigenvalues\n",
    "            laplacian_estimates = [] # estimated laplacian with subsampled eval set\n",
    "            eig_over_jitter = []\n",
    "            max_eig_over_jitter = []\n",
    "            condition = []\n",
    "            rng_key = jax.random.PRNGKey(0)\n",
    "            for i in tqdm(range(trials)):\n",
    "                rng_key, rng_key_sample = jax.random.split(rng_key)\n",
    "                params = model.init(rng_key_sample, x_train[0][None, :])\n",
    "                # uniform between 0 and max_scale\n",
    "                rng_key, rng_key_scale = jax.random.split(rng_key)\n",
    "                scale = jax.random.uniform(rng_key_scale, shape=(), minval=0, maxval=max_scale)\n",
    "                params = jax.tree_map(lambda x: x * scale, params)\n",
    "                logdet_full, eig = log_det_g_svd(model, params, x_context, jitter, return_eig=True, shift_by_jitter=True)\n",
    "                logdets.append(logdet_full)\n",
    "                logdet_n = log_det_g_svd(model, params, x_context * scale_context, jitter, return_eig=False, shift_by_jitter=True)\n",
    "                logdets_n.append(logdet_n)\n",
    "                eig_over_jitter.append(eig.mean().item() / jitter)\n",
    "                max_eig_over_jitter.append(eig.max().item() / jitter)\n",
    "                condition.append(eig.max().item() / (eig.min().item() + 1e-32))\n",
    "                laplacian_estimates.append(log_det_trace_estimator(model, params, x_context * scale_context, jitter, sigma, n_samples, normalize).item())\n",
    "                true_laplacians.append(log_det_g_svd_first_order(model, params, x_context, jitter)) # jitter has no effect on this\n",
    "\n",
    "            laplacian_estimates = np.array(laplacian_estimates)\n",
    "            logdets = np.array(logdets)\n",
    "            logdets_n = np.array(logdets_n)\n",
    "            true_laplacians = np.array(true_laplacians)\n",
    "            condition = np.array(condition)\n",
    "            \n",
    "            plot_compare(logdets, logdets_n, eig_over_jitter, rf'$\\log\\det\\left(g + \\epsilon I\\right)$', r'$\\log\\det\\left(\\hat{g} + \\epsilon I\\right)$', clabel=r'$\\langle \\lambda / \\epsilon \\rangle$', title=rf'$\\epsilon = {jitter}, \\gamma={scale_context}$', log_axes=True, fit=False)\n",
    "            plt.savefig(f'objectives/logdet_logdet_n_eps{jitter}_std{sigma}_ctxs{scale_context}_{act}.pdf')\n",
    "            plt.show()\n",
    "            plot_compare(true_laplacians, laplacian_estimates, None, r'$\\Delta d$', r'$\\hat{\\Delta} d$', clabel=None, title=rf'$\\sigma = {sigma}, \\gamma={scale_context}$', log_axes=True, fit=False)\n",
    "            plt.savefig(f'objectives/truelaplace_laplace_eps{jitter}_std{sigma}_ctxs{scale_context}_{act}.pdf')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare(logdet_svds, logdet_traces, eig_over_jitter, 'log det', 'Laplacian estimate', r'$\\langle \\lambda / \\epsilon \\rangle$', rf'$\\epsilon = {jitter}$', log_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare(logdet_svds, logdet_first_order, eig_over_jitter, 'log det', 'log det first order', r'$\\langle \\lambda / \\epsilon \\rangle$', rf'$\\epsilon = {jitter}$', log_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare(logdet_first_order, logdet_traces, np.log10(condition), 'log det first order', 'log det trace', r'$\\log \\kappa$', rf'$\\epsilon = {jitter}$',  log_axes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fspace': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9436057e92285046d415c34e216bd357b01decd87fa7e06f42744a4b160880c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
