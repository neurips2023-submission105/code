{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "# os.environ['XLA_PYTHON_CLIENT_ALLOCATOR']= 'platform'\n",
    "# os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import blackjax\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import jaxopt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"pdf\", \"png\")\n",
    "plt.rcParams[\"savefig.dpi\"] = 75\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams[\"figure.figsize\"] = 6, 4\n",
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"lines.linewidth\"] = 2.0\n",
    "plt.rcParams[\"lines.markersize\"] = 8\n",
    "plt.rcParams[\"legend.fontsize\"] = 14\n",
    "plt.rcParams[\"grid.linestyle\"] = \"-\"\n",
    "plt.rcParams[\"grid.linewidth\"] = 1.0\n",
    "plt.rcParams[\"legend.facecolor\"] = \"white\"\n",
    "# plt.rcParams['grid.color'] = \"grey\"\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "# plt.rcParams['font.family'] = \"normal\"\n",
    "# plt.rcParams['font.family'] = \"sans-serif\"\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "plt.rcParams[\n",
    "    \"text.latex.preamble\"\n",
    "] = \"\\\\usepackage{subdepth} \\\\usepackage{amsfonts} \\\\usepackage{type1cm}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "\n",
    "seed = 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "### LOAD DATA\n",
    "def _one_hot(x, k):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k))\n",
    "\n",
    "_x_train, _y_train = sklearn.datasets.make_moons(\n",
    "    n_samples=200, shuffle=True, noise=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "h = 0.25\n",
    "test_lim = 3\n",
    "x_min, x_max = -5, 5\n",
    "y_min, y_max = -5, 5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "_x_test = np.vstack((xx.reshape(-1), yy.reshape(-1))).T\n",
    "\n",
    "h = 0.25\n",
    "test_lim = 7\n",
    "x_wide_min, x_wide_max = (\n",
    "    _x_train[:, 0].min() - test_lim,\n",
    "    _x_train[:, 0].max() + test_lim,\n",
    ")\n",
    "y_wide_min, y_wide_max = (\n",
    "    _x_train[:, 1].min() - test_lim,\n",
    "    _x_train[:, 1].max() + test_lim,\n",
    ")\n",
    "xx_wide, yy_wide = np.meshgrid(\n",
    "    np.arange(x_wide_min, x_wide_max, h), np.arange(y_wide_min, y_wide_max, h)\n",
    ")\n",
    "_x_test_wide = np.vstack((xx_wide.reshape(-1), yy_wide.reshape(-1))).T\n",
    "\n",
    "_y_test = np.ones(_x_test.shape[0])\n",
    "_y_test_wide = np.ones(_x_test_wide.shape[0])\n",
    "\n",
    "\n",
    "x_train = _x_train\n",
    "y_train = _y_train.reshape(-1, 1)\n",
    "\n",
    "x_test = _x_test\n",
    "y_test = _y_test.reshape(-1, 1)\n",
    "\n",
    "x_test_wide = _x_test_wide\n",
    "y_test_wide = _y_test_wide.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from typing import Callable\n",
    "\n",
    "def count_params(model, x):\n",
    "    params = model.init(jax.random.PRNGKey(0), x)\n",
    "    n = sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "    print(f'Number of parameters: {n/1e3} k')\n",
    "    \n",
    "def param_size(params):\n",
    "    return sum([np.prod(p.shape) for p in jax.tree_util.tree_leaves(params)])\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 64\n",
    "    hidden_layers: int = 1\n",
    "    act: Callable = nn.relu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        layers = [nn.Dense(self.H)]\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers += [self.act, nn.Dense(self.H)]\n",
    "        layers += [self.act, nn.Dense(self.out_size)]\n",
    "        return nn.Sequential(layers)(x)\n",
    "\n",
    "def reparam_initializer(initializer, f):\n",
    "    def init(key, shape, dtype=jnp.float32):\n",
    "        # sample original parameters and then invert the reparametrization\n",
    "        return f(initializer(key, shape, dtype))\n",
    "    return init\n",
    "\n",
    "class ReparamDense(nn.Module):\n",
    "    # same as nn.Dense but with reparam weights\n",
    "    # reparam: Callable = lambda x: x # w = reparam(x)\n",
    "    # reparam_inv: Callable = lambda x: x # x = reparam_inv(w)\n",
    "    # bias_init: Callable = reparam_initializer(nn.initializers.normal(stddev=1e-6))\n",
    "    # kernel_init: Callable = reparam_initializer(nn.initializers.lecun_normal())\n",
    "\n",
    "    def __init__(self, features, reparam, reparam_inv, init_scale=None):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.reparam = reparam\n",
    "        self.reparam_inv = reparam_inv\n",
    "        # zero init for bias\n",
    "        if init_scale is None:\n",
    "            self.bias_init = reparam_initializer(nn.initializers.normal(stddev=1e-4), f=reparam_inv)\n",
    "            self.kernel_init = reparam_initializer(nn.initializers.lecun_normal(), f=reparam_inv)\n",
    "        else:\n",
    "            self.bias_init = reparam_initializer(nn.initializers.normal(stddev=init_scale), f=reparam_inv)\n",
    "            self.kernel_init = reparam_initializer(nn.initializers.normal(stddev=init_scale), f=reparam_inv)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        reparam_kernel = self.param('reparam_kernel', self.kernel_init, (inputs.shape[-1], self.features))\n",
    "        reparam_bias = self.param('reparam_bias', self.bias_init, (1, self.features)) # not using bias_init to avoid dividing by zero\n",
    "        # invert weights\n",
    "        kernel = jax.tree_util.tree_map(self.reparam, reparam_kernel)\n",
    "        bias = jax.tree_util.tree_map(self.reparam, reparam_bias)\n",
    "        # clamp to avoid numerical issues\n",
    "        kernel = jnp.clip(kernel, a_min=-1e6, a_max=1e6)\n",
    "        bias = jnp.clip(bias, a_min=-1e6, a_max=1e6)\n",
    "        return jnp.dot(inputs, kernel) + bias\n",
    "\n",
    "class ReparamMLP(nn.Module):\n",
    "    out_size: int\n",
    "    H: int = 64\n",
    "    hidden_layers: int = 1\n",
    "    reparam: Callable = lambda x: x # w = reparam(x)\n",
    "    reparam_inv: Callable = lambda x: x # x = reparam_inv(w)\n",
    "    act: Callable = nn.tanh\n",
    "    init_scale: float = None\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        layers = [ReparamDense(features=self.H, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            layers += [self.act, ReparamDense(features=self.H, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        layers += [self.act, ReparamDense(features=self.out_size, reparam=self.reparam, reparam_inv=self.reparam_inv, init_scale=self.init_scale)]\n",
    "        out = nn.Sequential(layers)(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_norm(tree):\n",
    "    return jnp.sqrt(sum([jnp.sum(x**2) for x in jax.tree_leaves(tree)]))\n",
    "\n",
    "def random_split_like_tree(rng_key, target=None, treedef=None):\n",
    "    if treedef is None:\n",
    "        treedef = jax.tree_structure(target)\n",
    "    keys = jax.random.split(rng_key, treedef.num_leaves)\n",
    "    return jax.tree_unflatten(treedef, keys)\n",
    "\n",
    "@jax.jit\n",
    "def tree_random_normal_like(rng_key, target):\n",
    "    keys_tree = random_split_like_tree(rng_key, target)\n",
    "    return jax.tree_map(\n",
    "        lambda l, k: jax.random.normal(k, l.shape, l.dtype),\n",
    "        target,\n",
    "        keys_tree,\n",
    "    )\n",
    "\n",
    "def jacobian_sigular_values(model, p, x):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x) # (b, o, p)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('bo...->...bo', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    P = param_size(p)\n",
    "    J = J.reshape(P, -1).T # (B*O, P)\n",
    "    # sigular values of J\n",
    "    _, S, _ = jnp.linalg.svd(J, full_matrices=False)\n",
    "    return S\n",
    "\n",
    "def get_K_matrix(model, p, x):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('b...->...b', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    J = J.reshape(-1, x.shape[0]).T # (N, P)\n",
    "    N = x.shape[0]\n",
    "    K = J.T @ J / N # (P, P)\n",
    "    return K\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,3,4))\n",
    "def log_det_K_svd(model, p, x, jitter=1e-6, return_eig=False):\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    logdet_svd = jnp.sum(jnp.log(s ** 2 + jitter))\n",
    "    P = param_size(p)\n",
    "    logdet_svd = logdet_svd - P * jnp.log(jitter)\n",
    "    if return_eig:\n",
    "        return logdet_svd, s ** 2\n",
    "    return logdet_svd\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,3,4))\n",
    "def log_det_K_svd_first_order(model, p, x, jitter=1e-6, return_eig=False):\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    logdet_svd = jnp.sum(jnp.log(jitter) + (s ** 2) / jitter)\n",
    "    P = param_size(p)\n",
    "    logdet_svd = logdet_svd - P * jnp.log(jitter)\n",
    "    if return_eig:\n",
    "        return logdet_svd, s ** 2\n",
    "    return logdet_svd\n",
    "\n",
    "def log_det_K(model, p, x, jitter=1e-6, scale=1.0):\n",
    "    K = get_K_matrix(model, p, x)\n",
    "    # add jitter\n",
    "    K = K + jitter * jnp.eye(K.shape[0])\n",
    "    s, log_det = jnp.linalg.slogdet(K)\n",
    "    return log_det\n",
    "    s = jacobian_sigular_values(model, p, x) / (x.shape[0] ** 0.5)\n",
    "    logdet_svd = 2 * jnp.sum(jnp.log(s))\n",
    "    return logdet_svd\n",
    "\n",
    "\n",
    "def log_det_diagonal_approx(model, p, x, jitter=1e-6):\n",
    "    jac_rev = jax.jacrev(lambda p, x: model.apply(p, x))\n",
    "    jac_vmap = jax.vmap(jac_rev, in_axes=(None, 0))\n",
    "    # j = jac_rev(p, x)\n",
    "    j = jac_vmap(p, x)\n",
    "    # move the batch axis to last\n",
    "    j = jax.tree_util.tree_map(lambda x: jnp.einsum('b...->...b', x), j)\n",
    "    # flatten j\n",
    "    J, _ = jax.flatten_util.ravel_pytree(j)\n",
    "    J = J.reshape(-1, x.shape[0]).T # (N, P)\n",
    "    avg_j_sq = jnp.mean(J ** 2, axis=0) # (P,)\n",
    "    logdet_diag = jnp.sum(jnp.log(avg_j_sq + jitter))\n",
    "    return logdet_diag\n",
    "\n",
    "def function_instability(model, p, x, dp):\n",
    "    # p_perturbed = p + dp, both pytrees\n",
    "    p_perturbed = jax.tree_util.tree_map(lambda x, y: x + y, p, dp)\n",
    "    # compute the difference between the outputs\n",
    "    y = model.apply(p, x)\n",
    "    y_perturbed = model.apply(p_perturbed, x)\n",
    "    diff = y - y_perturbed\n",
    "    # compute the norm of the difference\n",
    "    avg_dff_sq = jnp.mean(diff ** 2)\n",
    "    return avg_dff_sq\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,4))\n",
    "def trace_estimator(model, p, x, dp, sigma=0.1):\n",
    "    # dp: N(0, I)\n",
    "    # p_perturbed = p + dp, both pytrees\n",
    "    # scale dp by sigma\n",
    "    dp = jax.tree_util.tree_map(lambda x: x * sigma, dp)\n",
    "    p_perturbed = jax.tree_util.tree_map(lambda x, y: x + y, p, dp)\n",
    "    # compute the difference between the outputs\n",
    "    y = model.apply(p, x)\n",
    "    y_perturbed = model.apply(p_perturbed, x)\n",
    "    diff = y - y_perturbed\n",
    "    # compute the avg sq norm of the difference\n",
    "    avg_dff_sq_norm = jnp.mean((diff ** 2).sum(axis=-1))\n",
    "    return avg_dff_sq_norm / (sigma ** 2)\n",
    "\n",
    "def log_det_trace_estimator(model, p, x, jitter, sigma, n_samples=10, normalize=False):\n",
    "    # sample n_samples dp from N(0, I) and compute the trace estimator\n",
    "    traces = []\n",
    "    P = param_size(p)\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    for _ in range(n_samples):\n",
    "        rng_key, rng_key_sample = jax.random.split(rng_key)\n",
    "        dp = tree_random_normal_like(rng_key_sample, p)\n",
    "        if normalize:\n",
    "            scale = (P ** 0.5) / tree_norm(dp)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        # scale dp to have squared norm P * sigma^2\n",
    "        dp = jax.tree_util.tree_map(lambda x: x * scale, dp)\n",
    "        trace = trace_estimator(model, p, x, dp, sigma=sigma)\n",
    "        traces.append(trace)\n",
    "    trace = jnp.array(traces).mean()\n",
    "    return trace / jitter\n",
    "\n",
    "# def log_det_trace_estimator(model, p, x, jitter, sigma, n_samples=10):\n",
    "#     # sample n_samples dp from N(0, I) and compute the trace estimator\n",
    "#     dp_list = []\n",
    "#     for i in range(n_samples):\n",
    "#         rng_key = jax.random.PRNGKey(i)\n",
    "#         dp_list.append(tree_random_normal_like(rng_key, p))\n",
    "#     dps = jax.tree_map(lambda *x: jnp.stack(x, axis=0), *dp_list)\n",
    "#     # vamp the trace estimator\n",
    "#     traces = jax.vmap(trace_estimator, in_axes=(None, None, None, 0, None))(model, p, x, dps, sigma)\n",
    "#     trace = traces.mean()\n",
    "#     return trace / jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(lr, prior_scale, n_step, rng_key, loss_fn, model, x_train, y_train, x_eval_generator, optimizer, fsmap, jitter, method='exact', temp=1.0):\n",
    "    rng_key, init_params_key = jax.random.split(rng_key)\n",
    "    init_params = jax.jit(model.init)(init_params_key, jnp.ones((1, x_train.shape[1])))\n",
    "    def get_train_state(optimizer, lr, init_params, warmup_steps=100):\n",
    "        lr_sched = optax.linear_schedule(0, lr, warmup_steps, transition_begin=0)\n",
    "        if optimizer == 'adam':\n",
    "            tx = optax.chain(\n",
    "                    optax.adam(learning_rate=lr_sched),\n",
    "                )\n",
    "            \n",
    "        elif optimizer == 'sgd':\n",
    "            tx = optax.sgd(learning_rate=lr_sched, momentum=0.9)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return train_state.TrainState.create(apply_fn=model.apply, params=init_params, tx=tx)\n",
    "    \n",
    "    ts = get_train_state(optimizer, lr, init_params)\n",
    "    \n",
    "    def augmented_loss_fn(p, x_eval, dp):\n",
    "        # loss = likelihood / N = 1 / (2 * sigma^2) * ||y - f(x)||^2 / N\n",
    "        # it contains a factor 1 / N, where N = x_train.shape[0]\n",
    "        # all other terms should be divided by N as well\n",
    "        avg_nll = loss_fn(p) \n",
    "        N = x_train.shape[0]\n",
    "        # number of params\n",
    "        P = jax.tree_util.tree_leaves(p)[0].shape[0]\n",
    "        if fsmap:\n",
    "            if method == 'diag':\n",
    "                logdet = 1 / 2 * log_det_diagonal_approx(model, p, x_eval, jitter) / N\n",
    "            elif method == 'exact':\n",
    "                # svd is much more stable\n",
    "                logdet = 1 / 2 * log_det_K_svd(model, p, x_eval, jitter) / N\n",
    "            elif method == 'trace':\n",
    "                logdet = 1 / 2 * ((1/jitter) * trace_estimator(model, p, x_eval, dp)) / N\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            logdet = 0\n",
    "        params_flat, unravel = jax.flatten_util.ravel_pytree(p)\n",
    "        log_p_w = 1 / (2 * (prior_scale ** 2)) * jnp.sum(params_flat ** 2) / N\n",
    "        logdet =  temp * logdet + (1 - temp) * jax.lax.stop_gradient(logdet)\n",
    "        return avg_nll + log_p_w + logdet, (avg_nll, logdet)\n",
    "\n",
    "    grad_fn = jax.jit(jax.value_and_grad(lambda p, x_eval, dp: augmented_loss_fn(p, x_eval, dp), has_aux=True))\n",
    "    losses = []\n",
    "    logdets = []\n",
    "    avg_nlls = []\n",
    "    def sample_dp(p, rng_key):\n",
    "        # gaussian tree\n",
    "        dp = tree_random_normal_like(rng_key, p)\n",
    "        # # norm of dp flattened\n",
    "        # dp_flat, _ = jax.flatten_util.ravel_pytree(dp)\n",
    "        # dp_norm = jnp.sqrt(jnp.sum(dp_flat ** 2))\n",
    "        # # normalize dp\n",
    "        # dp = jax.tree_util.tree_map(lambda x: x / dp_norm, dp)\n",
    "        return dp\n",
    "    @jax.jit\n",
    "    def train_step(ts, rng_key):\n",
    "        rng_key, x_eval_key = jax.random.split(rng_key)\n",
    "        rng_key, dp_key = jax.random.split(rng_key)\n",
    "        x_eval = x_eval_generator(x_eval_key)\n",
    "        dp = sample_dp(ts.params, dp_key)\n",
    "        (loss, aux), grads = grad_fn(ts.params, x_eval, dp)\n",
    "        ts = ts.apply_gradients(grads=grads)\n",
    "        return ts, loss, aux, rng_key\n",
    "    for _ in tqdm(range(int(n_step))):\n",
    "        ts, loss, aux, rng_key = train_step(ts, rng_key)\n",
    "        avg_nll, logdet = aux\n",
    "        losses.append(loss.item())\n",
    "        logdets.append(logdet.item())\n",
    "        avg_nlls.append(avg_nll.item())\n",
    "    losses = np.array(losses)\n",
    "    logdets = np.array(logdets)\n",
    "    avg_nlls = np.array(avg_nlls)\n",
    "    return ts.params, losses, avg_nlls, logdets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_two_moons(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, fsmap=False, method='exact', jitter=1e-6, temp=1):\n",
    "    # model\n",
    "    model = arch(out_size=2)\n",
    "    x_eval_sample = x_eval_generator(jax.random.PRNGKey(0))\n",
    "    # count parameters\n",
    "    init_params = model.init(jax.random.PRNGKey(0), jnp.ones((1, x_train.shape[1])))\n",
    "    leaves, _ = jax.tree_util.tree_flatten(init_params)\n",
    "    n_params = sum([np.prod(p.shape) for p in leaves])\n",
    "    print(f\"Number of parameters: {n_params}\")\n",
    "\n",
    "    # softmax multi-task cross entropy loss (not binary)\n",
    "    y_train_one_hot = jax.nn.one_hot(y_train.reshape(-1), 2)\n",
    "    train_loss_fn = lambda p: jnp.mean(jnp.sum(-jax.nn.log_softmax(model.apply(p, x_train), axis=1) * y_train_one_hot, axis=1))\n",
    "    \n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    params, losses, avg_nlls, logdets = optimize(lr, prior_scale, n_step, rng_key, train_loss_fn, model, x_train, y_train, x_eval_generator, optimizer, fsmap, jitter, method, temp)\n",
    "    torch.save(params, f'{output_dir}/params_{seed}.pt')\n",
    "    print('Saved ps parameters at ', f'{output_dir}/params_{seed}.pt')\n",
    "    # plot and save losses\n",
    "    plt.figure()\n",
    "    \n",
    "    def make_subplots(arrays, labels):\n",
    "        n_plots = len(arrays)\n",
    "        fig, axs = plt.subplots(n_plots, 1, figsize=(10, 10))\n",
    "        for i, (a, label) in enumerate(zip(arrays, labels)):\n",
    "            q1, q2 = np.quantile(a, [0., 0.95])\n",
    "            axs[i].plot(a)\n",
    "            axs[i].set_ylim(q1, q2)\n",
    "            axs[i].set_xlabel('Step')\n",
    "            axs[i].set_ylabel(label)\n",
    "            axs[i].legend()\n",
    "        plt.show()\n",
    "    \n",
    "    make_subplots([losses, avg_nlls, logdets], ['Loss', 'Avg NLL', 'Logdet'])\n",
    "    \n",
    "    def log_posterior(model, params, x, jitter):\n",
    "        log_likelihood = jnp.sum(jnp.sum(jax.nn.log_softmax(model.apply(params, x_train), axis=1) * y_train_one_hot, axis=1))\n",
    "        log_param_prior = -0.5 * jnp.sum(jnp.array([jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(params)])) / (prior_scale ** 2)\n",
    "        log_det = log_det_K_svd(model, params, x, jitter)\n",
    "        print(f'log_likelihood: {log_likelihood}, log_param_prior: {log_param_prior}, log_det: {log_det}')\n",
    "        return log_likelihood + log_param_prior - 1 / 2 * log_det\n",
    "    \n",
    "    # compute function space posterior \n",
    "    log_posterior_learned = log_posterior(model, params, x_test, jitter)\n",
    "    if gt_params is not None:\n",
    "        log_posterior_gt = log_posterior(model, gt_params, x_test, jitter)\n",
    "    else:\n",
    "        log_posterior_gt = None\n",
    "\n",
    "    prediction = jax.nn.softmax(model.apply(params, x_test))[..., 1].reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    cbar = plt.contourf(xx, yy, prediction, levels=20, cmap=cm.coolwarm)\n",
    "    cb = plt.colorbar(cbar,)\n",
    "    cb.ax.set_ylabel(\n",
    "        \"$\\mathbb{E}[\\mathbf{y} | \\mathcal{D}; \\mathbf{x}]$\",\n",
    "        rotation=270,\n",
    "        labelpad=40,\n",
    "        size=30,\n",
    "    )\n",
    "    # cb.ax.set_ylabel('$E[y | \\mathcal{D}; x]$', labelpad=-90)\n",
    "    cb.ax.tick_params(labelsize=30)\n",
    "    plt.scatter(\n",
    "        x_train[y_train[:, 0] == 0][..., 0],\n",
    "        x_train[y_train[:, 0] == 0][..., 1],\n",
    "        color=\"cornflowerblue\",\n",
    "        edgecolors=\"black\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        x_train[y_train[:, 0] == 1][..., 0],\n",
    "        x_train[y_train[:, 0] == 1][..., 1],\n",
    "        color=\"tomato\",\n",
    "        edgecolors=\"black\",\n",
    "    )\n",
    "    plt.tick_params(labelsize=30)\n",
    "    plt.savefig(f'{output_dir}/{seed}.pdf')\n",
    "    # show noise_scale, prior_scale on plot\n",
    "    # plt.text(0.05, 0.9, rf'$\\alpha = {prior_scale}$, $\\sigma = {noise_scale}$, $\\tau = {temp}$', fontsize=20, verticalalignment='top')\n",
    "    # plt.title(rf'$\\alpha = {prior_scale}$, $\\tau = {temp}$, $\\log p(f|D) = {log_posterior_learned:.0f}$')\n",
    "    # plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'tanh': nn.tanh,\n",
    "    'elu': nn.elu,\n",
    "    'cos': jnp.cos,\n",
    "    'sin': jnp.sin,\n",
    "    'softplus': nn.softplus,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSMAP\n",
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [16]\n",
    "depths = [2]\n",
    "acts = ['softplus']\n",
    "\n",
    "\n",
    "optimizer = 'adam'\n",
    "lr = 1e-3\n",
    "n_step = int(2e3)\n",
    "prior_scales = [1]\n",
    "noise_scale = 0\n",
    "n_eval = 400\n",
    "x_lims = [-5, 5]\n",
    "\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = x_test\n",
    "    return X_eval\n",
    "\n",
    "skip_if_done = True\n",
    "fsmap = False\n",
    "temps = [1]\n",
    "method = 'exact'\n",
    "jitter = 1e-32\n",
    "\n",
    "pmap_results = []\n",
    "for width in widths:\n",
    "    for depth in depths:\n",
    "        for act in acts:\n",
    "            for prior_scale in prior_scales:\n",
    "                for temp in temps:\n",
    "                    output_dir = f'psmap'\n",
    "                    # mkdir if needed\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    for seed in range(1):\n",
    "                        arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                        result_path = f'{output_dir}/result_{seed}.pt'\n",
    "                        if os.path.exists(result_path) and skip_if_done:\n",
    "                            pmap_results.append(torch.load(result_path))\n",
    "                            print('Loaded result from ', result_path)\n",
    "                        else:\n",
    "                            gt_params = None\n",
    "                            pmap_results.append(run_two_moons(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=temp, jitter=jitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reparam = lambda x: x\n",
    "reparam_inv = lambda w: w\n",
    "\n",
    "widths = [16]\n",
    "depths = [2]\n",
    "acts = ['tanh']\n",
    "\n",
    "\n",
    "optimizer = 'adam'\n",
    "lr = 1e-3\n",
    "n_step = int(2e3)\n",
    "prior_scales = [1]\n",
    "noise_scale = 0\n",
    "# n_eval = 400\n",
    "# x_lims = [-5, 5]\n",
    "\n",
    "def x_eval_generator(rng_key):\n",
    "    X_eval = x_test\n",
    "    return X_eval\n",
    "\n",
    "skip_if_done = True\n",
    "fsmap = True\n",
    "temps = [1]\n",
    "method = 'exact'\n",
    "# jitter = 1e-32\n",
    "\n",
    "pmap_results = []\n",
    "for jitter in [1e-32]: #1e-6, 1e-3, 1e-1, 1, 10\n",
    "    for width in widths:\n",
    "        for depth in depths:\n",
    "            for act in acts:\n",
    "                for prior_scale in prior_scales:\n",
    "                    for temp in temps:\n",
    "                        output_dir = f'pathology'\n",
    "                        # mkdir if needed\n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "                        for seed in range(1):\n",
    "                            arch = partial(ReparamMLP, reparam=reparam, reparam_inv=reparam_inv, H=width, hidden_layers=depth, act=activations[act])\n",
    "                            result_path = f'{output_dir}/result_{seed}.pt'\n",
    "                            if os.path.exists(result_path) and skip_if_done:\n",
    "                                pmap_results.append(torch.load(result_path))\n",
    "                                print('Loaded result from ', result_path)\n",
    "                            else:\n",
    "                                gt_params = None\n",
    "                                pmap_results.append(run_two_moons(arch, noise_scale, prior_scale, x_train, y_train, x_test, y_test, gt_params, x_eval_generator, n_step, lr, optimizer, output_dir, seed, fsmap=fsmap, method=method, temp=temp, jitter=jitter))\n",
    "                                plt.show()\n",
    "                                plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_pred.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = arch(out_size=2)\n",
    "x_eval_sample = x_eval_generator(jax.random.PRNGKey(0))\n",
    "# count parameters\n",
    "init_params = model.init(jax.random.PRNGKey(0), jnp.ones((1, x_train.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.load('pathology/params_0.pt')\n",
    "# reduce to a flat vector\n",
    "p_flat = jax.tree_util.tree_reduce(lambda x, y: jnp.concatenate([x.reshape(-1), y.reshape(-1)]), p)\n",
    "p_init_flat = jax.tree_util.tree_reduce(lambda x, y: jnp.concatenate([x.reshape(-1), y.reshape(-1)]), init_params)\n",
    "import seaborn as sns\n",
    "sns.histplot(p_flat)\n",
    "sns.histplot(p_init_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(style=\"whitegrid\", font_scale=2.0, rc={\"lines.linewidth\": 3.0})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "def subtract_min(x):\n",
    "    return x\n",
    "    # return x - np.min(x)\n",
    "\n",
    "def normalize(v):\n",
    "    v_norm2 = jnp.sum(jnp.array([jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(v)]))\n",
    "    v = jax.tree_util.tree_map(lambda x: x / jnp.sqrt(v_norm2), v)    \n",
    "    return v\n",
    "\n",
    "def log_posterior(model, params, x_train, y_train, x_context, jitter, log_det_fn):\n",
    "    y_train_one_hot = jax.nn.one_hot(y_train.reshape(-1), 2)\n",
    "    log_likelihood = jnp.sum(jnp.sum(jax.nn.log_softmax(model.apply(params, x_train), axis=1) * y_train_one_hot, axis=1))\n",
    "    log_param_prior = -0.5 * jnp.sum(jnp.array([jnp.sum(p ** 2) for p in jax.tree_util.tree_leaves(params)])) / (prior_scale ** 2)\n",
    "    log_det = log_det_fn(model, params, x_context, jitter)\n",
    "    # print(f'log_likelihood: {log_likelihood}, log_param_prior: {log_param_prior}, log_det: {log_det}')\n",
    "    return log_likelihood, log_param_prior, log_det\n",
    "\n",
    "def scan_posterior(model, x_context, jitter, steps, max_alpha=1, log_det_fn=log_det_K_svd):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    init_params = model.init(rng_key, x_train)\n",
    "    # split key\n",
    "    rng_key, rng_key_sample_1, rng_key_sample_2, rng_key_sample_3 = jax.random.split(rng_key, 4)\n",
    "    # gaussian random param\n",
    "    v1 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_1, x.shape), init_params))\n",
    "    v2 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_2, x.shape), init_params))\n",
    "    v3 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_3, x.shape), init_params))\n",
    "    # plot log posterior on the ray spanned by \n",
    "    alpha = jnp.concatenate([-jnp.logspace(jnp.log10(max_alpha), -4, steps), jnp.logspace(-4, jnp.log10(max_alpha), steps)])\n",
    "    alpha = alpha[alpha != 0]\n",
    "\n",
    "    log_likelihoods, log_param_priors, log_dets = [], [], []\n",
    "    log_posterior_fn = jax.jit(lambda p: log_posterior(model, p, x_train, y_train, x_context, jitter, log_det_fn))\n",
    "    for a in tqdm(alpha):\n",
    "        for v in [v1, v2, v3]:\n",
    "            params = jax.tree_util.tree_map(lambda x: x * a, v)\n",
    "            log_likelihood, log_param_prior, log_det = log_posterior_fn(params)\n",
    "            log_likelihoods.append(log_likelihood)\n",
    "            log_param_priors.append(log_param_prior)\n",
    "            log_dets.append(log_det)\n",
    "\n",
    "    log_likelihoods = jnp.array(log_likelihoods).reshape(-1, 3)\n",
    "    log_param_priors = jnp.array(log_param_priors).reshape(-1, 3)\n",
    "    log_dets = jnp.array(log_dets).reshape(-1, 3)\n",
    "    return alpha, log_likelihoods, log_param_priors, log_dets\n",
    "\n",
    "def plot_scan(alpha, log_likelihoods, log_param_priors, log_dets):\n",
    "    # plot three quantities, as dots not lines\n",
    "    plt.figure(dpi=100, figsize=(10, 6))\n",
    "    for i in range(3):\n",
    "        plt.plot(alpha, -log_likelihoods[:, i], label=r'NLL', color=f'C{i}', linestyle='--')\n",
    "        plt.plot(alpha, subtract_min(-log_param_priors[:, i]), label=r'$-\\Delta\\log p_w$', color=f'C{i}', linestyle=':')\n",
    "        plt.plot(alpha[alpha<0], 1/2 * subtract_min(log_dets[alpha<0, i]), label=rf'$\\Delta1/2\\log\\det$', color=f'C{i}', linestyle='-')\n",
    "        plt.plot(alpha[alpha>0], 1/2 * subtract_min(log_dets[alpha>0, i]), color=f'C{i}', linestyle='-')\n",
    "    # put legend outside\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.xlabel(r'$\\alpha$')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('divergence.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_posterior_2d(model, x_context, jitter, steps, max_alpha=1, log_det_fn=log_det_K_svd):\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    init_params = model.init(rng_key, x_train)\n",
    "    # split key\n",
    "    rng_key, rng_key_sample_1, rng_key_sample_2, rng_key_sample_3 = jax.random.split(rng_key, 4)\n",
    "    # gaussian random param\n",
    "    v1 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_1, x.shape), init_params))\n",
    "    v2 = normalize(jax.tree_util.tree_map(lambda x: jax.random.normal(rng_key_sample_2, x.shape), init_params))\n",
    "    # plot log posterior on the ray spanned by v1 and v2\n",
    "    alpha = jnp.concatenate([-jnp.logspace(jnp.log10(max_alpha), -4, steps), jnp.logspace(-4, jnp.log10(max_alpha), steps)])\n",
    "    alpha = alpha[alpha != 0]\n",
    "    # 2d coordinates\n",
    "    u, v = jnp.meshgrid(alpha, alpha)\n",
    "    log_likelihoods, log_param_priors, log_dets = [], [], []\n",
    "    log_posterior_fn = jax.jit(lambda p: log_posterior(model, p, x_train, y_train, x_context, jitter, log_det_fn))\n",
    "    for a, b in tqdm(zip(u.flatten(), v.flatten())):\n",
    "        av1 = jax.tree_util.tree_map(lambda x: x * a, v1)\n",
    "        bv2 = jax.tree_util.tree_map(lambda x: x * b, v2)\n",
    "        params = jax.tree_util.tree_map(lambda x, y: x + y, av1, bv2)\n",
    "        log_likelihood, log_param_prior, log_det = log_posterior_fn(params)\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "        log_param_priors.append(log_param_prior)\n",
    "        log_dets.append(log_det)\n",
    "    log_likelihoods = jnp.array(log_likelihoods)\n",
    "    log_param_priors = jnp.array(log_param_priors)\n",
    "    log_dets = jnp.array(log_dets)\n",
    "    return u, v, log_likelihoods, log_param_priors, log_dets\n",
    "    \n",
    "def plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets):\n",
    "    # three subplots\n",
    "    fig, axs = plt.subplots(1, 3, dpi=100, figsize=(30, 10))\n",
    "    # plot delta log det\n",
    "    axs[0].contourf(u, v, subtract_min(log_dets).reshape(u.shape), levels=20)\n",
    "    axs[0].set_title(r'$\\Delta\\log\\det$')\n",
    "    axs[0].set_xlabel(r'$\\alpha$')\n",
    "    axs[0].set_ylabel(r'$\\beta$')\n",
    "    # make image square\n",
    "    axs[0].set_aspect('equal')\n",
    "    # colorbar\n",
    "    cbar = fig.colorbar(axs[0].contourf(u, v, subtract_min(log_dets).reshape(u.shape), levels=20), ax=axs[0])\n",
    "    # plot NLL\n",
    "    axs[1].contourf(u, v, -log_likelihoods.reshape(u.shape), levels=20)\n",
    "    axs[1].set_title(r'NLL')\n",
    "    axs[1].set_xlabel(r'$\\alpha$')\n",
    "    axs[1].set_ylabel(r'$\\beta$')\n",
    "    axs[1].set_aspect('equal')\n",
    "    # colorbar\n",
    "    cbar = fig.colorbar(axs[1].contourf(u, v, -log_likelihoods.reshape(u.shape), levels=20), ax=axs[1])\n",
    "    # plot delta -log p_w\n",
    "    axs[2].contourf(u, v, subtract_min(-log_param_priors).reshape(u.shape), levels=20)\n",
    "    axs[2].set_title(r'$-\\Delta\\log p_w$')\n",
    "    axs[2].set_xlabel(r'$\\alpha$')\n",
    "    axs[2].set_ylabel(r'$\\beta$')\n",
    "    axs[2].set_aspect('equal')\n",
    "    # colorbar\n",
    "    cbar = fig.colorbar(axs[2].contourf(u, v, subtract_min(-log_param_priors).reshape(u.shape), levels=20), ax=axs[2])\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets):\n",
    "    # same but plot in 3d\n",
    "    fig = plt.figure(figsize=(30, 10))\n",
    "    ax = fig.add_subplot(131, projection='3d')\n",
    "    ax.plot_surface(u, v, subtract_min(log_dets).reshape(u.shape), cmap='viridis', edgecolor='none')\n",
    "    ax.set_title(r'$\\Delta\\log\\det$')\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel(r'$\\beta$')\n",
    "    ax.set_zlabel(r'$\\Delta\\log\\det$')\n",
    "    ax = fig.add_subplot(132, projection='3d')\n",
    "    ax.plot_surface(u, v, -log_likelihoods.reshape(u.shape), cmap='viridis', edgecolor='none')\n",
    "    ax.set_title(r'NLL')\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel(r'$\\beta$')\n",
    "    ax.set_zlabel(r'NLL')\n",
    "    ax = fig.add_subplot(133, projection='3d')\n",
    "    ax.plot_surface(u, v, subtract_min(-log_param_priors).reshape(u.shape), cmap='viridis', edgecolor='none')\n",
    "    ax.set_title(r'$-\\Delta\\log p_w$')\n",
    "    ax.set_xlabel(r'$\\alpha$')\n",
    "    ax.set_ylabel(r'$\\beta$')\n",
    "    ax.set_zlabel(r'$-\\Delta\\log p_w$')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'tanh'\n",
    "prior_scale = 1\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "count_params(model, x_train)\n",
    "x_context = x_test\n",
    "\n",
    "for jitter in [1000]:\n",
    "    alpha, log_likelihoods, log_param_priors, log_dets = scan_posterior(model, x_context, jitter, steps=10, max_alpha=1)\n",
    "    plot_scan(alpha, 0*log_likelihoods, 0*log_param_priors, log_dets)\n",
    "    plt.show()\n",
    "    # plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}.png')\n",
    "    # plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'tanh'\n",
    "prior_scale = 1\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "count_params(model, x_train)\n",
    "x_context = x_test\n",
    "\n",
    "log_det_fn = lambda model, p, x, jitter: log_det_trace_estimator(model, p, x, jitter, sigma=0.01, n_samples=10)\n",
    "for jitter in [1000]:\n",
    "    alpha, log_likelihoods, log_param_priors, log_dets = scan_posterior(model, x_context, jitter, steps=10, max_alpha=1, log_det_fn=log_det_fn)\n",
    "    plot_scan(alpha, 0*log_likelihoods, 0*log_param_priors, log_dets)\n",
    "    plt.show()\n",
    "    # plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}.png')\n",
    "    # plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'tanh'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "for jitter in [1e-1]:\n",
    "    u, v, log_likelihoods, log_param_priors, log_dets = scan_posterior_2d(model, x_context, jitter, steps=30, max_alpha=100)\n",
    "    torch.save((u, v, log_likelihoods, log_param_priors, log_dets), f'pathology/{act}_l{depth}h{width}j{jitter}.pt')\n",
    "    plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_contour.png')\n",
    "    plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_surface.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'tanh'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "log_det_fn = lambda model, p, x, jitter: log_det_trace_estimator(model, p, x, jitter, sigma=0.1, n_samples=10)\n",
    "for jitter in [1e-1]:\n",
    "    u, v, log_likelihoods, log_param_priors, log_dets = scan_posterior_2d(model, x_context, jitter, steps=30, max_alpha=100, log_det_fn=log_det_fn)\n",
    "    torch.save((u, v, log_likelihoods, log_param_priors, log_dets), f'pathology/{act}_l{depth}h{width}j{jitter}.pt')\n",
    "    plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_contour_tr.png')\n",
    "    plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_surface_tr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 4\n",
    "act = 'elu'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "for jitter in [1e-32, 1e-6, 1e-3]:\n",
    "    u, v, log_likelihoods, log_param_priors, log_dets = scan_posterior_2d(model, x_context, jitter, steps=30, max_alpha=100)\n",
    "    torch.save((u, v, log_likelihoods, log_param_priors, log_dets), f'pathology/{act}_l{depth}h{width}j{jitter}.pt')\n",
    "    plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_contour.png')\n",
    "    plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "    plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_surface.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'elu'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "jitter = 1e-32\n",
    "u, v, log_likelihoods, log_param_priors, log_dets = scan_posterior_2d(model, x_context, jitter, steps=30, max_alpha=1000)\n",
    "torch.save((u, v, log_likelihoods, log_param_priors, log_dets), f'pathology/{act}_l{depth}h{width}j{jitter}_wide.pt')\n",
    "plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_contour_wide.png')\n",
    "plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_surface_wide.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 4\n",
    "act = 'elu'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "jitter = 1e-32\n",
    "u, v, log_likelihoods, log_param_priors, log_dets = scan_posterior_2d(model, x_context, jitter, steps=30, max_alpha=1000)\n",
    "torch.save((u, v, log_likelihoods, log_param_priors, log_dets), f'pathology/{act}_l{depth}h{width}j{jitter}_wide.pt')\n",
    "plot_scan_2d(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_contour_wide.png')\n",
    "plot_scan_2d_surface(u, v, log_likelihoods, log_param_priors, log_dets)\n",
    "plt.savefig(f'pathology/{act}_l{depth}h{width}j{jitter}_surface_wide.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing objectives ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "depth = 2\n",
    "act = 'tanh'\n",
    "arch = partial(ReparamMLP, reparam=lambda x: x, reparam_inv=lambda x: x, H=width, hidden_layers=depth, act=activations[act])\n",
    "model = arch(out_size=2)\n",
    "\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "init_params = model.init(rng_key, x_train)\n",
    "# log_det_K_svd_fn = jax.jit(lambda params, x_context, jitter: log_det_K_svd(model, params, x_context, jitter, return_eig=True), static_argnums=(3,))\n",
    "# log_det_trace_estimator_fn = jax.jit(lambda params, x_context, jitter, sigma, n_samples: log_det_trace_estimator(model, params, x_context, jitter, sigma, n_samples), static_argnums=(3,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = 50\n",
    "x_context = x_test\n",
    "jitter = 1000\n",
    "# trace estimator\n",
    "n_samples = 100\n",
    "sigma = 0.001 # noise scale\n",
    "\n",
    "logdet_svds = []\n",
    "logdet_traces = []\n",
    "max_eigs = []\n",
    "for _ in tqdm(range(points)):\n",
    "    rng_key, rng_key_sample = jax.random.split(rng_key)\n",
    "    params = model.init(rng_key_sample, x_train[0][None, :])\n",
    "    logdet_svd, eig = log_det_K_svd(model, params, x_context, jitter, return_eig=True)\n",
    "    logdet_svds.append(logdet_svd)\n",
    "    max_eigs.append(eig.max().item())\n",
    "    logdet_traces.append(log_det_trace_estimator(model, params, x_context, jitter, sigma, n_samples).item())\n",
    "\n",
    "logdet_traces = np.array(logdet_traces)\n",
    "logdet_svds = np.array(logdet_svds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = (logdet_traces / logdet_svds)\n",
    "_ = plt.hist(ratio, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(style=\"whitegrid\", font_scale=2.0, rc={\"lines.linewidth\": 3.0})\n",
    "sns.set_palette(\"Set1\")\n",
    "\n",
    "# a nice square plot to compare logdet_svds, logdet_traces\n",
    "# color the points by min_eig and plot a colorbar\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(logdet_svds, logdet_traces, c=max_eigs, cmap='viridis')\n",
    "\n",
    "plt.colorbar()\n",
    "# # a diagonal line spanning the range of the data\n",
    "min_logdet = min(min(logdet_svds), min(logdet_traces))\n",
    "max_logdet = max(max(logdet_svds), max(logdet_traces))\n",
    "plt.plot([0, max_logdet], [0, max_logdet], 'k--')\n",
    "\n",
    "# label the axes\n",
    "plt.xlabel('logdet svd')\n",
    "plt.ylabel('logdet trace')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fspace': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9436057e92285046d415c34e216bd357b01decd87fa7e06f42744a4b160880c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
